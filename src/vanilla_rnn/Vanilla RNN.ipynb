{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_channels = 22 #from project guidelines\n",
    "test_count = 50 #from project guideline, 238 for train-validation and 50 for test\n",
    "validation_count = 38 # 38 points in validation set and remaining 200 points in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from includes import *\n",
    "\n",
    "#################### \"includes\" imports:\n",
    "#\n",
    "#   from read_data import *\n",
    "#   \n",
    "#   import torch\n",
    "#   from torch.autograd import Variable\n",
    "#   import torch.nn as nn\n",
    "#   import torch.optim as optim\n",
    "#\n",
    "#   dtype = torch.cuda.FloatTensor # torch.FloatTensor\n",
    "#\n",
    "#   all_files = [h5py.File(m, 'r') for m in mat_names]\n",
    "#   all_ims = [f['image'] for f in all_files]\n",
    "#   all_types = [f['type'] for f in all_files]\n",
    "#\n",
    "####################\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n"
     ]
    }
   ],
   "source": [
    "image_mat = all_ims[0]\n",
    "image_shape = image_mat.shape # 288 (48x6) trials across 25 electrodes for 1000 time points (250Hz*4s)\n",
    "print image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./project_datasets/A01T_slice.mat', './project_datasets/A02T_slice.mat', './project_datasets/A03T_slice.mat', './project_datasets/A04T_slice.mat', './project_datasets/A05T_slice.mat', './project_datasets/A06T_slice.mat', './project_datasets/A07T_slice.mat', './project_datasets/A08T_slice.mat', './project_datasets/A09T_slice.mat']\n"
     ]
    }
   ],
   "source": [
    "print(mat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting seed\n",
    "np.random.seed(seed=1337)\n",
    "test_picked = np.random.choice(image_shape[0], test_count, replace=False)\n",
    "train_val_picked = np.setdiff1d(np.arange(image_shape[0]), test_picked)\n",
    "val_picked = train_val_picked[:validation_count]\n",
    "train_picked = train_val_picked[validation_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mat = all_types[0]\n",
    "type_shape = type_mat.shape\n",
    "type_set = list(set(type_mat[0,:]))\n",
    "\n",
    "# all the 0's occur after 288, and are meaningless I think\n",
    "# so the image_mat, which has shape (288, 25, 1000) should correspond\n",
    "# to the first 288 entries of type_mat, so\n",
    "# for a single subject, training data should be image_mat, with 288 samples, each sample has shape (25, 1000)\n",
    "# and our target label matrix should be type_mat[:288] (or 287?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_data_X = []\n",
    "training_data_X = []\n",
    "validation_data_X = []\n",
    "test_data_X = []\n",
    "\n",
    "trainval_data_Y = []\n",
    "training_data_Y = []\n",
    "validation_data_Y = []\n",
    "test_data_Y = []\n",
    "\n",
    "for i in range(len(mat_names)):\n",
    "    matfile = h5py.File(mat_names[i], 'r')\n",
    "    \n",
    "    trainval_data_X.append(matfile['image'][sorted(train_val_picked),:EEG_channels,:]) #(238, 22, 1000) x 9\n",
    "    training_data_X.append(matfile['image'][sorted(train_picked),:EEG_channels,:]) #(200, 22, 1000) x 9\n",
    "    validation_data_X.append(matfile['image'][sorted(val_picked),:EEG_channels,:]) #(38, 22, 1000) x 9\n",
    "    test_data_X.append(matfile['image'][sorted(test_picked),:EEG_channels,:]) #(50, 22, 1000) x 9\n",
    "    \n",
    "    trainval_data_Y.append(matfile['type'][0,sorted(train_val_picked)] - type_set[1]) #(238, ) x 9\n",
    "    training_data_Y.append(matfile['type'][0,sorted(train_picked)] - type_set[1]) #(200, ) x 9\n",
    "    validation_data_Y.append(matfile['type'][0,sorted(val_picked)] - type_set[1]) #(38, ) x 9\n",
    "    test_data_Y.append(matfile['type'][0,sorted(test_picked)] - type_set[1]) #(50, ) x 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mat_names)):\n",
    "    assert(set(training_data_Y[i]) == set(validation_data_Y[i])) #check whether we have data for all labels \n",
    "    assert(set(training_data_Y[i]) == set(test_data_Y[i])) #check whether we have data for all labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "rnn_trainval_data_X = np.concatenate(trainval_data_X, axis=0) #(2142, 22, 1000)\n",
    "rnn_training_data_X = np.concatenate(training_data_X, axis=0) #(1800, 22, 1000)\n",
    "rnn_validation_data_X = np.concatenate(validation_data_X, axis=0) #(342, 22, 1000)\n",
    "rnn_test_data_X = np.concatenate(test_data_X, axis=0) #(450, 22, 1000)\n",
    "\n",
    "rnn_trainval_data_Y = np.concatenate(trainval_data_Y, axis=0) #(2142, )\n",
    "rnn_training_data_Y = np.concatenate(training_data_Y, axis=0) #(1800, )\n",
    "rnn_validation_data_Y = np.concatenate(validation_data_Y, axis=0) #(342, )\n",
    "rnn_test_data_Y = np.concatenate(test_data_Y, axis=0) #(450,)\n",
    "\n",
    "def remove_nan_rows_A(A, b):\n",
    "    if (np.isnan(A).any() or np.isnan(b).any()):\n",
    "        mask = ~np.isnan(np.sum(A,axis=(1,2))) & ~np.isnan(b[:])\n",
    "        A = A[mask, :, :]\n",
    "        b = b[mask]\n",
    "        \n",
    "    assert A.shape[0] == b.shape[0]\n",
    "    return A, b\n",
    "\n",
    "rnn_trainval_data_X, rnn_trainval_data_Y = remove_nan_rows_A(rnn_trainval_data_X,\n",
    "                                                             rnn_trainval_data_Y)\n",
    "rnn_training_data_X, rnn_training_data_Y = remove_nan_rows_A(rnn_training_data_X, \n",
    "                                                             rnn_training_data_Y)\n",
    "rnn_validation_data_X, rnn_validation_data_Y = remove_nan_rows_A(rnn_validation_data_X,\n",
    "                                         rnn_validation_data_Y)\n",
    "rnn_test_data_X, rnn_test_data_Y = remove_nan_rows_A(rnn_test_data_X,\n",
    "                                   rnn_test_data_Y)\n",
    "\n",
    "\n",
    "# repeating the Y labels for the rnn\n",
    "N_trainval, E, T = rnn_trainval_data_X.shape\n",
    "N_training, _, _ = rnn_trainval_data_X.shape\n",
    "N_validation, _, _ = rnn_test_data_X.shape\n",
    "N_test, _, _ = rnn_test_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(training_data_Y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "training_data_shape = training_data_X[0].shape\n",
    "print(training_data_shape) #(200, 22, 1000) while test data shape is (50, 22, 1000) and validation data is (38, 22,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115, 22, 1000)\n",
      "(2115,)\n"
     ]
    }
   ],
   "source": [
    "print rnn_trainval_data_X.shape\n",
    "print rnn_trainval_data_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence as ppseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1775, 22, 1000])\n",
      "torch.Size([1775])\n"
     ]
    }
   ],
   "source": [
    "tensor_x_train = torch.Tensor(rnn_training_data_X)\n",
    "tensor_y_train = torch.LongTensor(rnn_training_data_Y)\n",
    "\n",
    "print tensor_x_train.shape\n",
    "print tensor_y_train.shape\n",
    "\n",
    "tensor_x_val = torch.Tensor(rnn_validation_data_X)\n",
    "tensor_y_val = torch.LongTensor(rnn_validation_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(tensor_x_train,tensor_y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(tensor_x_val,tensor_y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VanillaRNN import VanillaRNN\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "net = VanillaRNN(recurrent_hidden_size=100, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1775000, 4])\n",
      "torch.Size([1775, 4])\n"
     ]
    }
   ],
   "source": [
    "net.train(True)\n",
    "out, hidden = net.forward(tensor_x_train)\n",
    "print(out.shape)\n",
    "net.train(False)\n",
    "out, hidden = net.forward(tensor_x_train)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Iter [1/17] Loss: 1.4513\n",
      "Epoch [1/20], Iter [2/17] Loss: 1.4510\n",
      "Epoch [1/20], Iter [3/17] Loss: 1.4518\n",
      "Epoch [1/20], Iter [4/17] Loss: 1.4474\n",
      "Epoch [1/20], Iter [5/17] Loss: 1.4498\n",
      "Epoch [1/20], Iter [6/17] Loss: 1.4465\n",
      "Epoch [1/20], Iter [7/17] Loss: 1.4451\n",
      "Epoch [1/20], Iter [8/17] Loss: 1.4481\n",
      "Epoch [1/20], Iter [9/17] Loss: 1.4491\n",
      "Epoch [1/20], Iter [10/17] Loss: 1.4433\n",
      "Epoch [1/20], Iter [11/17] Loss: 1.4476\n",
      "Epoch [1/20], Iter [12/17] Loss: 1.4463\n",
      "Epoch [1/20], Iter [13/17] Loss: 1.4438\n",
      "Epoch [1/20], Iter [14/17] Loss: 1.4441\n",
      "Epoch [1/20], Iter [15/17] Loss: 1.4456\n",
      "Epoch [1/20], Iter [16/17] Loss: 1.4411\n",
      "Epoch [1/20], Iter [17/17] Loss: 1.4468\n",
      "Epoch [1/20], Iter [18/17] Loss: 1.4445\n",
      "Train Accuracy: 24 %\n",
      "validation Accuracy: 20 %\n",
      "Epoch [2/20], Iter [1/17] Loss: 1.4422\n",
      "Epoch [2/20], Iter [2/17] Loss: 1.4393\n",
      "Epoch [2/20], Iter [3/17] Loss: 1.4412\n",
      "Epoch [2/20], Iter [4/17] Loss: 1.4429\n",
      "Epoch [2/20], Iter [5/17] Loss: 1.4399\n",
      "Epoch [2/20], Iter [6/17] Loss: 1.4371\n",
      "Epoch [2/20], Iter [7/17] Loss: 1.4381\n",
      "Epoch [2/20], Iter [8/17] Loss: 1.4391\n",
      "Epoch [2/20], Iter [9/17] Loss: 1.4409\n",
      "Epoch [2/20], Iter [10/17] Loss: 1.4404\n",
      "Epoch [2/20], Iter [11/17] Loss: 1.4401\n",
      "Epoch [2/20], Iter [12/17] Loss: 1.4393\n",
      "Epoch [2/20], Iter [13/17] Loss: 1.4385\n",
      "Epoch [2/20], Iter [14/17] Loss: 1.4370\n",
      "Epoch [2/20], Iter [15/17] Loss: 1.4361\n",
      "Epoch [2/20], Iter [16/17] Loss: 1.4339\n",
      "Epoch [2/20], Iter [17/17] Loss: 1.4339\n",
      "Epoch [2/20], Iter [18/17] Loss: 1.4377\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 20 %\n",
      "Epoch [3/20], Iter [1/17] Loss: 1.4365\n",
      "Epoch [3/20], Iter [2/17] Loss: 1.4360\n",
      "Epoch [3/20], Iter [3/17] Loss: 1.4346\n",
      "Epoch [3/20], Iter [4/17] Loss: 1.4342\n",
      "Epoch [3/20], Iter [5/17] Loss: 1.4311\n",
      "Epoch [3/20], Iter [6/17] Loss: 1.4325\n",
      "Epoch [3/20], Iter [7/17] Loss: 1.4319\n",
      "Epoch [3/20], Iter [8/17] Loss: 1.4316\n",
      "Epoch [3/20], Iter [9/17] Loss: 1.4323\n",
      "Epoch [3/20], Iter [10/17] Loss: 1.4322\n",
      "Epoch [3/20], Iter [11/17] Loss: 1.4312\n",
      "Epoch [3/20], Iter [12/17] Loss: 1.4292\n",
      "Epoch [3/20], Iter [13/17] Loss: 1.4317\n",
      "Epoch [3/20], Iter [14/17] Loss: 1.4321\n",
      "Epoch [3/20], Iter [15/17] Loss: 1.4300\n",
      "Epoch [3/20], Iter [16/17] Loss: 1.4298\n",
      "Epoch [3/20], Iter [17/17] Loss: 1.4302\n",
      "Epoch [3/20], Iter [18/17] Loss: 1.4296\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 21 %\n",
      "Epoch [4/20], Iter [1/17] Loss: 1.4274\n",
      "Epoch [4/20], Iter [2/17] Loss: 1.4290\n",
      "Epoch [4/20], Iter [3/17] Loss: 1.4261\n",
      "Epoch [4/20], Iter [4/17] Loss: 1.4272\n",
      "Epoch [4/20], Iter [5/17] Loss: 1.4253\n",
      "Epoch [4/20], Iter [6/17] Loss: 1.4252\n",
      "Epoch [4/20], Iter [7/17] Loss: 1.4288\n",
      "Epoch [4/20], Iter [8/17] Loss: 1.4273\n",
      "Epoch [4/20], Iter [9/17] Loss: 1.4295\n",
      "Epoch [4/20], Iter [10/17] Loss: 1.4264\n",
      "Epoch [4/20], Iter [11/17] Loss: 1.4252\n",
      "Epoch [4/20], Iter [12/17] Loss: 1.4276\n",
      "Epoch [4/20], Iter [13/17] Loss: 1.4256\n",
      "Epoch [4/20], Iter [14/17] Loss: 1.4270\n",
      "Epoch [4/20], Iter [15/17] Loss: 1.4236\n",
      "Epoch [4/20], Iter [16/17] Loss: 1.4252\n",
      "Epoch [4/20], Iter [17/17] Loss: 1.4253\n",
      "Epoch [4/20], Iter [18/17] Loss: 1.4252\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 21 %\n",
      "Epoch [5/20], Iter [1/17] Loss: 1.4258\n",
      "Epoch [5/20], Iter [2/17] Loss: 1.4233\n",
      "Epoch [5/20], Iter [3/17] Loss: 1.4236\n",
      "Epoch [5/20], Iter [4/17] Loss: 1.4262\n",
      "Epoch [5/20], Iter [5/17] Loss: 1.4231\n",
      "Epoch [5/20], Iter [6/17] Loss: 1.4232\n",
      "Epoch [5/20], Iter [7/17] Loss: 1.4223\n",
      "Epoch [5/20], Iter [8/17] Loss: 1.4247\n",
      "Epoch [5/20], Iter [9/17] Loss: 1.4219\n",
      "Epoch [5/20], Iter [10/17] Loss: 1.4224\n",
      "Epoch [5/20], Iter [11/17] Loss: 1.4236\n",
      "Epoch [5/20], Iter [12/17] Loss: 1.4236\n",
      "Epoch [5/20], Iter [13/17] Loss: 1.4213\n",
      "Epoch [5/20], Iter [14/17] Loss: 1.4211\n",
      "Epoch [5/20], Iter [15/17] Loss: 1.4204\n",
      "Epoch [5/20], Iter [16/17] Loss: 1.4207\n",
      "Epoch [5/20], Iter [17/17] Loss: 1.4204\n",
      "Epoch [5/20], Iter [18/17] Loss: 1.4201\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 20 %\n",
      "Epoch [6/20], Iter [1/17] Loss: 1.4201\n",
      "Epoch [6/20], Iter [2/17] Loss: 1.4216\n",
      "Epoch [6/20], Iter [3/17] Loss: 1.4220\n",
      "Epoch [6/20], Iter [4/17] Loss: 1.4166\n",
      "Epoch [6/20], Iter [5/17] Loss: 1.4205\n",
      "Epoch [6/20], Iter [6/17] Loss: 1.4200\n",
      "Epoch [6/20], Iter [7/17] Loss: 1.4179\n",
      "Epoch [6/20], Iter [8/17] Loss: 1.4184\n",
      "Epoch [6/20], Iter [9/17] Loss: 1.4163\n",
      "Epoch [6/20], Iter [10/17] Loss: 1.4178\n",
      "Epoch [6/20], Iter [11/17] Loss: 1.4184\n",
      "Epoch [6/20], Iter [12/17] Loss: 1.4168\n",
      "Epoch [6/20], Iter [13/17] Loss: 1.4171\n",
      "Epoch [6/20], Iter [14/17] Loss: 1.4180\n",
      "Epoch [6/20], Iter [15/17] Loss: 1.4172\n",
      "Epoch [6/20], Iter [16/17] Loss: 1.4141\n",
      "Epoch [6/20], Iter [17/17] Loss: 1.4157\n",
      "Epoch [6/20], Iter [18/17] Loss: 1.4162\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 19 %\n",
      "Epoch [7/20], Iter [1/17] Loss: 1.4138\n",
      "Epoch [7/20], Iter [2/17] Loss: 1.4152\n",
      "Epoch [7/20], Iter [3/17] Loss: 1.4169\n",
      "Epoch [7/20], Iter [4/17] Loss: 1.4160\n",
      "Epoch [7/20], Iter [5/17] Loss: 1.4154\n",
      "Epoch [7/20], Iter [6/17] Loss: 1.4184\n",
      "Epoch [7/20], Iter [7/17] Loss: 1.4133\n",
      "Epoch [7/20], Iter [8/17] Loss: 1.4154\n",
      "Epoch [7/20], Iter [9/17] Loss: 1.4183\n",
      "Epoch [7/20], Iter [10/17] Loss: 1.4162\n",
      "Epoch [7/20], Iter [11/17] Loss: 1.4154\n",
      "Epoch [7/20], Iter [12/17] Loss: 1.4165\n",
      "Epoch [7/20], Iter [13/17] Loss: 1.4134\n",
      "Epoch [7/20], Iter [14/17] Loss: 1.4134\n",
      "Epoch [7/20], Iter [15/17] Loss: 1.4164\n",
      "Epoch [7/20], Iter [16/17] Loss: 1.4120\n",
      "Epoch [7/20], Iter [17/17] Loss: 1.4142\n",
      "Epoch [7/20], Iter [18/17] Loss: 1.4142\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 18 %\n",
      "Epoch [8/20], Iter [1/17] Loss: 1.4141\n",
      "Epoch [8/20], Iter [2/17] Loss: 1.4102\n",
      "Epoch [8/20], Iter [3/17] Loss: 1.4168\n",
      "Epoch [8/20], Iter [4/17] Loss: 1.4124\n",
      "Epoch [8/20], Iter [5/17] Loss: 1.4159\n",
      "Epoch [8/20], Iter [6/17] Loss: 1.4137\n",
      "Epoch [8/20], Iter [7/17] Loss: 1.4110\n",
      "Epoch [8/20], Iter [8/17] Loss: 1.4135\n",
      "Epoch [8/20], Iter [9/17] Loss: 1.4128\n",
      "Epoch [8/20], Iter [10/17] Loss: 1.4114\n",
      "Epoch [8/20], Iter [11/17] Loss: 1.4102\n",
      "Epoch [8/20], Iter [12/17] Loss: 1.4120\n",
      "Epoch [8/20], Iter [13/17] Loss: 1.4091\n",
      "Epoch [8/20], Iter [14/17] Loss: 1.4084\n",
      "Epoch [8/20], Iter [15/17] Loss: 1.4130\n",
      "Epoch [8/20], Iter [16/17] Loss: 1.4122\n",
      "Epoch [8/20], Iter [17/17] Loss: 1.4131\n",
      "Epoch [8/20], Iter [18/17] Loss: 1.4112\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 19 %\n",
      "Epoch [9/20], Iter [1/17] Loss: 1.4122\n",
      "Epoch [9/20], Iter [2/17] Loss: 1.4074\n",
      "Epoch [9/20], Iter [3/17] Loss: 1.4138\n",
      "Epoch [9/20], Iter [4/17] Loss: 1.4093\n",
      "Epoch [9/20], Iter [5/17] Loss: 1.4083\n",
      "Epoch [9/20], Iter [6/17] Loss: 1.4093\n",
      "Epoch [9/20], Iter [7/17] Loss: 1.4097\n",
      "Epoch [9/20], Iter [8/17] Loss: 1.4093\n",
      "Epoch [9/20], Iter [9/17] Loss: 1.4102\n",
      "Epoch [9/20], Iter [10/17] Loss: 1.4058\n",
      "Epoch [9/20], Iter [11/17] Loss: 1.4107\n",
      "Epoch [9/20], Iter [12/17] Loss: 1.4138\n",
      "Epoch [9/20], Iter [13/17] Loss: 1.4108\n",
      "Epoch [9/20], Iter [14/17] Loss: 1.4089\n",
      "Epoch [9/20], Iter [15/17] Loss: 1.4095\n",
      "Epoch [9/20], Iter [16/17] Loss: 1.4090\n",
      "Epoch [9/20], Iter [17/17] Loss: 1.4082\n",
      "Epoch [9/20], Iter [18/17] Loss: 1.4131\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 19 %\n",
      "Epoch [10/20], Iter [1/17] Loss: 1.4097\n",
      "Epoch [10/20], Iter [2/17] Loss: 1.4103\n",
      "Epoch [10/20], Iter [3/17] Loss: 1.4083\n",
      "Epoch [10/20], Iter [4/17] Loss: 1.4092\n",
      "Epoch [10/20], Iter [5/17] Loss: 1.4086\n",
      "Epoch [10/20], Iter [6/17] Loss: 1.4068\n",
      "Epoch [10/20], Iter [7/17] Loss: 1.4062\n",
      "Epoch [10/20], Iter [8/17] Loss: 1.4037\n",
      "Epoch [10/20], Iter [9/17] Loss: 1.4099\n",
      "Epoch [10/20], Iter [10/17] Loss: 1.4076\n",
      "Epoch [10/20], Iter [11/17] Loss: 1.4095\n",
      "Epoch [10/20], Iter [12/17] Loss: 1.4088\n",
      "Epoch [10/20], Iter [13/17] Loss: 1.4074\n",
      "Epoch [10/20], Iter [14/17] Loss: 1.4077\n",
      "Epoch [10/20], Iter [15/17] Loss: 1.4094\n",
      "Epoch [10/20], Iter [16/17] Loss: 1.4112\n",
      "Epoch [10/20], Iter [17/17] Loss: 1.4056\n",
      "Epoch [10/20], Iter [18/17] Loss: 1.4078\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 19 %\n",
      "Epoch [11/20], Iter [1/17] Loss: 1.4052\n",
      "Epoch [11/20], Iter [2/17] Loss: 1.4053\n",
      "Epoch [11/20], Iter [3/17] Loss: 1.4072\n",
      "Epoch [11/20], Iter [4/17] Loss: 1.4080\n",
      "Epoch [11/20], Iter [5/17] Loss: 1.4075\n",
      "Epoch [11/20], Iter [6/17] Loss: 1.4090\n",
      "Epoch [11/20], Iter [7/17] Loss: 1.4073\n",
      "Epoch [11/20], Iter [8/17] Loss: 1.4096\n",
      "Epoch [11/20], Iter [9/17] Loss: 1.4059\n",
      "Epoch [11/20], Iter [10/17] Loss: 1.4090\n",
      "Epoch [11/20], Iter [11/17] Loss: 1.4096\n",
      "Epoch [11/20], Iter [12/17] Loss: 1.4059\n",
      "Epoch [11/20], Iter [13/17] Loss: 1.4062\n",
      "Epoch [11/20], Iter [14/17] Loss: 1.4022\n",
      "Epoch [11/20], Iter [15/17] Loss: 1.4058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Iter [16/17] Loss: 1.4070\n",
      "Epoch [11/20], Iter [17/17] Loss: 1.4052\n",
      "Epoch [11/20], Iter [18/17] Loss: 1.4083\n",
      "Train Accuracy: 25 %\n",
      "validation Accuracy: 19 %\n",
      "Epoch [12/20], Iter [1/17] Loss: 1.4023\n",
      "Epoch [12/20], Iter [2/17] Loss: 1.4043\n",
      "Epoch [12/20], Iter [3/17] Loss: 1.4106\n",
      "Epoch [12/20], Iter [4/17] Loss: 1.4056\n",
      "Epoch [12/20], Iter [5/17] Loss: 1.4049\n",
      "Epoch [12/20], Iter [6/17] Loss: 1.4057\n",
      "Epoch [12/20], Iter [7/17] Loss: 1.4065\n",
      "Epoch [12/20], Iter [8/17] Loss: 1.4050\n",
      "Epoch [12/20], Iter [9/17] Loss: 1.4077\n",
      "Epoch [12/20], Iter [10/17] Loss: 1.4050\n",
      "Epoch [12/20], Iter [11/17] Loss: 1.4039\n",
      "Epoch [12/20], Iter [12/17] Loss: 1.4074\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "loss_history = []\n",
    "          \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    ######################## TRAINING\n",
    "        \n",
    "    net.train(True)\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #     for i in range(cnn_training_data_X.shape[0]):\n",
    "        images = Variable(images, requires_grad=True) #unsqueeze used to make a 4d tensor because \n",
    "        #     print images.shape\n",
    "        labels = labels.repeat(T)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = net(images)\n",
    "        \n",
    "        #reshaped_outputs = outputs.view(-1, NUM_CLASSES)\n",
    "        reshaped_labels = labels.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, reshaped_labels)\n",
    "        loss_history.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "               %(epoch+1, num_epochs, i+1, rnn_training_data_X.shape[0]*1.0/batch_size, loss.data[0]))\n",
    "    \n",
    "    ######################## PER EPOCH EVALUATION\n",
    "    net.train(False)\n",
    "    \n",
    "    images = Variable(torch.Tensor(rnn_training_data_X))\n",
    "    test_labels = torch.LongTensor(rnn_training_data_Y)\n",
    "    outputs, hidden = net(images)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += test_labels.size(0)\n",
    "    correct += (predicted == test_labels).double().sum()\n",
    "    print('Train Accuracy: %d %%' % (100 * correct / total))\n",
    "    train_acc.append((100 * correct / total))\n",
    "\n",
    "    net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    images = Variable(torch.Tensor(rnn_validation_data_X))\n",
    "    test_labels = torch.LongTensor(rnn_validation_data_Y)\n",
    "    outputs, hidden = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += test_labels.size(0)\n",
    "    correct += (predicted == test_labels).double().sum()\n",
    "    print('validation Accuracy: %d %%' % (100.0 * correct / total))\n",
    "    val_acc.append((100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
