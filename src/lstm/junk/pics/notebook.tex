
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Vanilla RNN}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{n}{EEG\PYZus{}channels} \PY{o}{=} \PY{l+m+mi}{22} \PY{c+c1}{\PYZsh{}from project guidelines}
        \PY{n}{test\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{50} \PY{c+c1}{\PYZsh{}from project guideline, 238 for train\PYZhy{}validation and 50 for test}
        \PY{n}{validation\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{38} \PY{c+c1}{\PYZsh{} 38 points in validation set and remaining 200 points in test set}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{includes} \PY{k+kn}{import} \PY{o}{*}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} \PYZdq{}includes\PYZdq{} imports:}
        \PY{c+c1}{\PYZsh{}}
        \PY{c+c1}{\PYZsh{}   from read\PYZus{}data import *}
        \PY{c+c1}{\PYZsh{}   }
        \PY{c+c1}{\PYZsh{}   import torch}
        \PY{c+c1}{\PYZsh{}   from torch.autograd import Variable}
        \PY{c+c1}{\PYZsh{}   import torch.nn as nn}
        \PY{c+c1}{\PYZsh{}   import torch.optim as optim}
        \PY{c+c1}{\PYZsh{}}
        \PY{c+c1}{\PYZsh{}   dtype = torch.cuda.FloatTensor \PYZsh{} torch.FloatTensor}
        \PY{c+c1}{\PYZsh{}}
        \PY{c+c1}{\PYZsh{}   all\PYZus{}files = [h5py.File(m, \PYZsq{}r\PYZsq{}) for m in mat\PYZus{}names]}
        \PY{c+c1}{\PYZsh{}   all\PYZus{}ims = [f[\PYZsq{}image\PYZsq{}] for f in all\PYZus{}files]}
        \PY{c+c1}{\PYZsh{}   all\PYZus{}types = [f[\PYZsq{}type\PYZsq{}] for f in all\PYZus{}files]}
        \PY{c+c1}{\PYZsh{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} autoreload
        \PY{o}{\PYZpc{}}\PY{k}{autoreload} 2
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{image\PYZus{}mat} \PY{o}{=} \PY{n}{all\PYZus{}ims}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{image\PYZus{}shape} \PY{o}{=} \PY{n}{image\PYZus{}mat}\PY{o}{.}\PY{n}{shape} \PY{c+c1}{\PYZsh{} 288 (48x6) trials across 25 electrodes for 1000 time points (250Hz*4s)}
        \PY{k}{print} \PY{n}{image\PYZus{}shape}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(288, 25, 1000)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{print}\PY{p}{(}\PY{n}{mat\PYZus{}names}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['./project\_datasets/A01T\_slice.mat', './project\_datasets/A02T\_slice.mat', './project\_datasets/A03T\_slice.mat', './project\_datasets/A04T\_slice.mat', './project\_datasets/A05T\_slice.mat', './project\_datasets/A06T\_slice.mat', './project\_datasets/A07T\_slice.mat', './project\_datasets/A08T\_slice.mat', './project\_datasets/A09T\_slice.mat']

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{}setting seed}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1337}\PY{p}{)}
        \PY{n}{test\PYZus{}picked} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{image\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}count}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
        \PY{n}{train\PYZus{}val\PYZus{}picked} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{setdiff1d}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{image\PYZus{}shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{test\PYZus{}picked}\PY{p}{)}
        \PY{n}{val\PYZus{}picked} \PY{o}{=} \PY{n}{train\PYZus{}val\PYZus{}picked}\PY{p}{[}\PY{p}{:}\PY{n}{validation\PYZus{}count}\PY{p}{]}
        \PY{n}{train\PYZus{}picked} \PY{o}{=} \PY{n}{train\PYZus{}val\PYZus{}picked}\PY{p}{[}\PY{n}{validation\PYZus{}count}\PY{p}{:}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{type\PYZus{}mat} \PY{o}{=} \PY{n}{all\PYZus{}types}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{type\PYZus{}shape} \PY{o}{=} \PY{n}{type\PYZus{}mat}\PY{o}{.}\PY{n}{shape}
        \PY{n}{type\PYZus{}set} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{type\PYZus{}mat}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} all the 0\PYZsq{}s occur after 288, and are meaningless I think}
        \PY{c+c1}{\PYZsh{} so the image\PYZus{}mat, which has shape (288, 25, 1000) should correspond}
        \PY{c+c1}{\PYZsh{} to the first 288 entries of type\PYZus{}mat, so}
        \PY{c+c1}{\PYZsh{} for a single subject, training data should be image\PYZus{}mat, with 288 samples, each sample has shape (25, 1000)}
        \PY{c+c1}{\PYZsh{} and our target label matrix should be type\PYZus{}mat[:288] (or 287?)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{trainval\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{training\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{validation\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{test\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
        \PY{n}{trainval\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{training\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{validation\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n}{test\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{mat\PYZus{}names}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{n}{matfile} \PY{o}{=} \PY{n}{h5py}\PY{o}{.}\PY{n}{File}\PY{p}{(}\PY{n}{mat\PYZus{}names}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            
            \PY{n}{trainval\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{train\PYZus{}val\PYZus{}picked}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{n}{EEG\PYZus{}channels}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(238, 22, 1000) x 9}
            \PY{n}{training\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{train\PYZus{}picked}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{n}{EEG\PYZus{}channels}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(200, 22, 1000) x 9}
            \PY{n}{validation\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{val\PYZus{}picked}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{n}{EEG\PYZus{}channels}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(38, 22, 1000) x 9}
            \PY{n}{test\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{test\PYZus{}picked}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{n}{EEG\PYZus{}channels}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(50, 22, 1000) x 9}
            
            \PY{n}{trainval\PYZus{}data\PYZus{}Y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{train\PYZus{}val\PYZus{}picked}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{type\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(238, ) x 9}
            \PY{n}{training\PYZus{}data\PYZus{}Y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{train\PYZus{}picked}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{type\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(200, ) x 9}
            \PY{n}{validation\PYZus{}data\PYZus{}Y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{val\PYZus{}picked}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{type\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(38, ) x 9}
            \PY{n}{test\PYZus{}data\PYZus{}Y}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{matfile}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{type}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{test\PYZus{}picked}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{type\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}(50, ) x 9}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{mat\PYZus{}names}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{k}{assert}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{training\PYZus{}data\PYZus{}Y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{o}{==} \PY{n+nb}{set}\PY{p}{(}\PY{n}{validation\PYZus{}data\PYZus{}Y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}check whether we have data for all labels }
            \PY{k}{assert}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{training\PYZus{}data\PYZus{}Y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{o}{==} \PY{n+nb}{set}\PY{p}{(}\PY{n}{test\PYZus{}data\PYZus{}Y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}check whether we have data for all labels }
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{functools} \PY{k+kn}{import} \PY{n+nb}{reduce}
        
        \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{trainval\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(2142, 22, 1000)}
        \PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{training\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(1800, 22, 1000)}
        \PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{validation\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(342, 22, 1000)}
        \PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{test\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(450, 22, 1000)}
        
        \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{trainval\PYZus{}data\PYZus{}Y}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(2142, )}
        \PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{training\PYZus{}data\PYZus{}Y}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(1800, )}
        \PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{validation\PYZus{}data\PYZus{}Y}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(342, )}
        \PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{n}{test\PYZus{}data\PYZus{}Y}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{}(450,)}
        
        \PY{k}{def} \PY{n+nf}{remove\PYZus{}nan\PYZus{}rows\PYZus{}A}\PY{p}{(}\PY{n}{A}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{debug}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{p}{(}\PY{n}{debug}\PY{p}{)}\PY{p}{:}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{before nans: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{A}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{k}{if} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{isnan}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{p}{)} \PY{o+ow}{or} \PY{n}{np}\PY{o}{.}\PY{n}{isnan}\PY{p}{(}\PY{n}{b}\PY{p}{)}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{mask} \PY{o}{=} \PY{o}{\PYZti{}}\PY{n}{np}\PY{o}{.}\PY{n}{isnan}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{A}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZam{}} \PY{o}{\PYZti{}}\PY{n}{np}\PY{o}{.}\PY{n}{isnan}\PY{p}{(}\PY{n}{b}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{p}{)}
                \PY{n}{A} \PY{o}{=} \PY{n}{A}\PY{p}{[}\PY{n}{mask}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]}
                \PY{n}{b} \PY{o}{=} \PY{n}{b}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
            
            \PY{k}{if} \PY{p}{(}\PY{n}{debug}\PY{p}{)}\PY{p}{:}
                \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{before nans: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{A}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{k}{assert} \PY{n}{A}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{==} \PY{n}{b}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{k}{return} \PY{n}{A}\PY{p}{,} \PY{n}{b}
        
        \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{remove\PYZus{}nan\PYZus{}rows\PYZus{}A}\PY{p}{(}\PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}X}\PY{p}{,}
                                                                     \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}Y}\PY{p}{)}
        \PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{remove\PYZus{}nan\PYZus{}rows\PYZus{}A}\PY{p}{(}\PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}X}\PY{p}{,} 
                                                                     \PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}Y}\PY{p}{)}
        \PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{remove\PYZus{}nan\PYZus{}rows\PYZus{}A}\PY{p}{(}\PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}X}\PY{p}{,}
                                                 \PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}Y}\PY{p}{)}
        \PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}X}\PY{p}{,} \PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}Y} \PY{o}{=} \PY{n}{remove\PYZus{}nan\PYZus{}rows\PYZus{}A}\PY{p}{(}\PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}X}\PY{p}{,}
                                           \PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}Y}\PY{p}{)}
        
        \PY{n}{N\PYZus{}trainval}\PY{p}{,} \PY{n}{E}\PY{p}{,} \PY{n}{T} \PY{o}{=} \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{shape}
        \PY{n}{N\PYZus{}training}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{shape}
        \PY{n}{N\PYZus{}validation}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{shape}
        \PY{n}{N\PYZus{}test}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{rnn\PYZus{}test\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
before nans: (2142, 22, 1000)
before nans: (2115, 22, 1000)
before nans: (1800, 22, 1000)
before nans: (1775, 22, 1000)
before nans: (342, 22, 1000)
before nans: (340, 22, 1000)
before nans: (450, 22, 1000)
before nans: (443, 22, 1000)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{print}\PY{p}{(}\PY{n}{training\PYZus{}data\PYZus{}Y}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(200,)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{training\PYZus{}data\PYZus{}shape} \PY{o}{=} \PY{n}{training\PYZus{}data\PYZus{}X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}
         \PY{k}{print}\PY{p}{(}\PY{n}{training\PYZus{}data\PYZus{}shape}\PY{p}{)} \PY{c+c1}{\PYZsh{}(200, 22, 1000) while test data shape is (50, 22, 1000) and validation data is (38, 22,1000)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(200, 22, 1000)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{print} \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{shape}
         \PY{k}{print} \PY{n}{rnn\PYZus{}trainval\PYZus{}data\PYZus{}Y}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(2115, 22, 1000)
(2115,)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k+kn}{from} \PY{n+nn}{torch.nn.utils.rnn} \PY{k+kn}{import} \PY{n}{pack\PYZus{}padded\PYZus{}sequence} \PY{k}{as} \PY{n}{ppseq}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{tensor\PYZus{}x\PYZus{}train} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{Tensor}\PY{p}{(}\PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}X}\PY{p}{)}
         \PY{n}{tensor\PYZus{}y\PYZus{}train} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}Y}\PY{p}{)}
         
         \PY{k}{print} \PY{n}{tensor\PYZus{}x\PYZus{}train}\PY{o}{.}\PY{n}{shape}
         \PY{k}{print} \PY{n}{tensor\PYZus{}y\PYZus{}train}\PY{o}{.}\PY{n}{shape}
         
         \PY{n}{tensor\PYZus{}x\PYZus{}val} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{Tensor}\PY{p}{(}\PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}X}\PY{p}{)}
         \PY{n}{tensor\PYZus{}y\PYZus{}val} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}Y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 22, 1000])
torch.Size([1775])

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k+kn}{import} \PY{n+nn}{torch.utils.data}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}189}]:} \PY{c+c1}{\PYZsh{} Dataset Hyper Parameters}
          \PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{l+m+mi}{30}
          \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{100}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}190}]:} \PY{n}{train\PYZus{}dataset} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{TensorDataset}\PY{p}{(}\PY{n}{tensor\PYZus{}x\PYZus{}train}\PY{p}{,}\PY{n}{tensor\PYZus{}y\PYZus{}train}\PY{p}{)}
          \PY{n}{val\PYZus{}dataset} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{TensorDataset}\PY{p}{(}\PY{n}{tensor\PYZus{}x\PYZus{}val}\PY{p}{,}\PY{n}{tensor\PYZus{}y\PYZus{}val}\PY{p}{)}
          
          \PY{n}{train\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{dataset}\PY{o}{=}\PY{n}{train\PYZus{}dataset}\PY{p}{,}
                                                     \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} 
                                                     \PY{n}{shuffle}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
          
          \PY{n}{val\PYZus{}loader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{dataset}\PY{o}{=}\PY{n}{val\PYZus{}dataset}\PY{p}{,}
                                                    \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} 
                                                    \PY{n}{shuffle}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}204}]:} \PY{c+c1}{\PYZsh{} Optimizer Hyper Parameters}
          \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{5e\PYZhy{}4}
          \PY{n}{reg\PYZus{}hidden\PYZus{}weight} \PY{o}{=} \PY{l+m+mf}{1.0}
          \PY{n}{reg\PYZus{}weight\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.00}
          \PY{n}{NUM\PYZus{}CLASSES} \PY{o}{=} \PY{l+m+mi}{4}
          
          \PY{c+c1}{\PYZsh{} Debug Parameters}
          \PY{n}{VERBOSE} \PY{o}{=} \PY{n+nb+bp}{False}
          \PY{n}{PRINT\PYZus{}GRADS} \PY{o}{=} \PY{n+nb+bp}{False}
          
          \PY{k+kn}{from} \PY{n+nn}{VanillaRNN} \PY{k+kn}{import} \PY{n}{VanillaRNN}
          
          \PY{n}{net} \PY{o}{=} \PY{n}{VanillaRNN}\PY{p}{(}
                          \PY{n}{conv\PYZus{}layers} \PY{o}{=} \PY{n+nb+bp}{True}\PY{p}{,}
                          \PY{n}{initial\PYZus{}hidden\PYZus{}layer\PYZus{}sizes} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,}\PY{c+c1}{\PYZsh{}[100],\PYZsh{}[100],}
                          \PY{n}{recurrent\PYZus{}hidden\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{50}\PY{p}{,}
                          \PY{n}{recurrent\PYZus{}use\PYZus{}bias} \PY{o}{=} \PY{n+nb+bp}{True}\PY{p}{,}
                          \PY{n}{recurrent\PYZus{}layer\PYZus{}num} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,}
                          \PY{c+c1}{\PYZsh{}recurrent\PYZus{}dropout = 0.5,}
                          \PY{n}{final\PYZus{}hidden\PYZus{}layer\PYZus{}sizes} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{,}\PY{c+c1}{\PYZsh{}[100, 10],}
                          \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{n}{NUM\PYZus{}CLASSES}\PY{p}{,}
                          \PY{n}{verbose} \PY{o}{=} \PY{n}{VERBOSE}\PY{p}{)}
          
          \PY{n}{FCC\PYZus{}SANITY} \PY{o}{=} \PY{n+nb+bp}{False}
          
          \PY{c+c1}{\PYZsh{}from FC import FC}
          \PY{c+c1}{\PYZsh{}net = FC()}
          \PY{c+c1}{\PYZsh{}FCC\PYZus{}SANITY = True}
          
          \PY{n}{net}\PY{o}{.}\PY{n}{initialize\PYZus{}weights}\PY{p}{(}\PY{p}{)}
          
          \PY{n}{criterion} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{CrossEntropyLoss}\PY{p}{(}\PY{p}{)}
          \PY{c+c1}{\PYZsh{}optimizer = torch.optim.RMSprop(net.parameters(), lr=learning\PYZus{}rate)}
          
          \PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{weight\PYZus{}decay}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}from itertools import chain}
          
          \PY{c+c1}{\PYZsh{}part1 = chain(net.norm\PYZus{}layer.parameters(), net.layer1.parameters(), net.initial\PYZus{}layer.parameters())}
          \PY{c+c1}{\PYZsh{}optimizer1 = torch.optim.Adam(part1, lr=learning\PYZus{}rate, weight\PYZus{}decay=0.01)}
          
          \PY{c+c1}{\PYZsh{}part2 = chain(net.rnn\PYZus{}layer.parameters(), net.output\PYZus{}layer.parameters())}
          \PY{c+c1}{\PYZsh{}optimizer2 = torch.optim.Adam(part2, lr=learning\PYZus{}rate, weight\PYZus{}decay=0.01)}
          
          \PY{c+c1}{\PYZsh{}optimizer = torch.optim.SGD(net.parameters(), lr=learning\PYZus{}rate, momentum=0.9, nesterov=True)}
          
          
          \PY{k}{for} \PY{n}{s} \PY{o+ow}{in} \PY{n}{net}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{:}
              \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{s}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
              
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([22])
torch.Size([22])
torch.Size([16, 22, 5])
torch.Size([16])
torch.Size([32, 16, 4])
torch.Size([32])
torch.Size([16, 32, 4])
torch.Size([16])
torch.Size([16, 16, 2])
torch.Size([16])
torch.Size([16])
torch.Size([16])
torch.Size([50, 16])
torch.Size([50, 50])
torch.Size([50])
torch.Size([50])
torch.Size([50, 50])
torch.Size([50, 50])
torch.Size([50])
torch.Size([50])
torch.Size([50, 50])
torch.Size([50, 50])
torch.Size([50])
torch.Size([50])
torch.Size([150])
torch.Size([150])
torch.Size([4, 150])
torch.Size([4])

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}192}]:} \PY{k+kn}{import} \PY{n+nn}{gc}
          \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
          
          \PY{n}{net}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n+nb+bp}{True}\PY{p}{)}
          
          \PY{k}{if} \PY{p}{(}\PY{n}{FCC\PYZus{}SANITY}\PY{p}{)}\PY{p}{:}
              \PY{n}{out} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{tensor\PYZus{}x\PYZus{}train}\PY{p}{)}
          \PY{k}{else}\PY{p}{:}
              \PY{n}{out}\PY{p}{,} \PY{n}{hidden} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{tensor\PYZus{}x\PYZus{}train}\PY{p}{)}
          \PY{k}{print}\PY{p}{(}\PY{n}{out}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
          
          \PY{n}{net}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n+nb+bp}{False}\PY{p}{)}
          
          \PY{k}{if} \PY{p}{(}\PY{n}{FCC\PYZus{}SANITY}\PY{p}{)}\PY{p}{:}
              \PY{n}{out} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{tensor\PYZus{}x\PYZus{}train}\PY{p}{)}
          \PY{k}{else}\PY{p}{:}
              \PY{n}{out}\PY{p}{,} \PY{n}{hidden} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{tensor\PYZus{}x\PYZus{}train}\PY{p}{)}
          \PY{k}{print}\PY{p}{(}\PY{n}{out}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
              
          \PY{k}{print}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{rnn\PYZus{}out}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
torch.Size([1775, 150])

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}205}]:} \PY{k+kn}{import} \PY{n+nn}{gc}
          \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
          
          \PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{l+m+mi}{1111}\PY{p}{)}
          
          \PY{n}{train\PYZus{}acc} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{val\PYZus{}acc} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                    
          \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
          
              \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} TRAINING}
                  
              \PY{n}{net}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n+nb+bp}{True}\PY{p}{)}
              
              \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{images}\PY{p}{,} \PY{n}{labels}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{train\PYZus{}loader}\PY{p}{)}\PY{p}{:}
                  \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
                  \PY{n}{images} \PY{o}{=} \PY{n}{Variable}\PY{p}{(}\PY{n}{images}\PY{p}{,} \PY{n}{requires\PYZus{}grad}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{}unsqueeze used to make a 4d tensor because }
                  
                  \PY{k}{if} \PY{p}{(}\PY{n}{VERBOSE}\PY{p}{)}\PY{p}{:}
                      \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{initial label shape: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{labels}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                      
                  \PY{n}{labels} \PY{o}{=} \PY{n}{Variable}\PY{p}{(}\PY{n}{labels}\PY{p}{)}
          
                  \PY{k}{if} \PY{p}{(}\PY{n}{VERBOSE}\PY{p}{)}\PY{p}{:}
                      \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iteration label set \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{labels}\PY{o}{.}\PY{n}{data}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{} Forward + Backward + Optimize}
                  \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
                  
                  \PY{k}{if} \PY{p}{(}\PY{n}{FCC\PYZus{}SANITY}\PY{p}{)}\PY{p}{:}
                      \PY{n}{outputs} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{images}\PY{p}{)}
                  \PY{k}{else}\PY{p}{:}    
                      \PY{n}{outputs}\PY{p}{,} \PY{n}{hidden} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{images}\PY{p}{)}
          
                  \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
              
                  \PY{k}{if} \PY{p}{(}\PY{n}{VERBOSE}\PY{p}{)}\PY{p}{:}
                      \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training predicted set \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{predicted}\PY{o}{.}\PY{n}{data}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                      \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training output shape: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                      \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training labels shape: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{labels}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                  
                  \PY{n}{loss1} \PY{o}{=} \PY{n}{criterion}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{n}{labels}\PY{p}{)}
                  \PY{n}{loss1}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{n}{retain\PYZus{}graph}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{}\PYZsh{} hidden state vanishing gradient regularizer}
                  \PY{n}{hidden\PYZus{}loss}\PY{p}{,} \PY{n}{size\PYZus{}loss} \PY{o}{=} \PY{n}{net}\PY{o}{.}\PY{n}{loss\PYZus{}regularizer}\PY{p}{(}\PY{n}{use\PYZus{}loss1}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,}\PY{n}{use\PYZus{}loss2}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{}\PYZsh{} general weight l2 regularizer()}
                  
                  \PY{n}{loss} \PY{o}{=} \PY{n}{loss1} \PY{o}{+} \PY{n}{reg\PYZus{}hidden\PYZus{}weight}\PY{o}{*}\PY{n}{hidden\PYZus{}loss} \PY{o}{+} \PY{n}{reg\PYZus{}weight\PYZus{}size}\PY{o}{*}\PY{n}{size\PYZus{}loss}
          
                  \PY{c+c1}{\PYZsh{}loss = criterion(outputs, labels)}
          
                  \PY{n}{loss\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
                  \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                  
                  \PY{n}{nn}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{clip\PYZus{}grad\PYZus{}norm}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
                  \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{}if (i \PYZpc{} 2 == 0):}
                  \PY{c+c1}{\PYZsh{}    optimizer1.step()}
                  \PY{c+c1}{\PYZsh{}else:}
                  \PY{c+c1}{\PYZsh{}    optimizer2.step()}
                  
                  \PY{k}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch [}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{], Iter [}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{] Loss: [\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{} }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+s1}{ = }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{ * }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{ * }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+s1}{ \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}]}\PY{l+s+s1}{\PYZsq{}} 
                         \PY{o}{\PYZpc{}}\PY{p}{(}\PY{n}{epoch}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{p}{,} \PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{l+m+mf}{1.0}\PY{o}{/}\PY{n}{batch\PYZus{}size}\PY{p}{,}
                           \PY{n}{loss}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{loss1}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                           \PY{n}{reg\PYZus{}hidden\PYZus{}weight}\PY{p}{,} \PY{n}{hidden\PYZus{}loss}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
                           \PY{n}{reg\PYZus{}weight\PYZus{}size}\PY{p}{,} \PY{n}{size\PYZus{}loss}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} PER EPOCH EVALUATION}
              \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}        
              \PY{n}{bad\PYZus{}grads} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{k}{for} \PY{n}{n}\PY{p}{,}\PY{n}{p} \PY{o+ow}{in} \PY{n}{net}\PY{o}{.}\PY{n}{named\PYZus{}parameters}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                  \PY{n}{grad\PYZus{}max} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{p}\PY{o}{.}\PY{n}{grad}\PY{p}{)}
                  \PY{n}{abs\PYZus{}grad\PYZus{}mean} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{p}\PY{o}{.}\PY{n}{grad}\PY{p}{)}\PY{p}{)}
                  \PY{k}{if} \PY{p}{(}\PY{n}{PRINT\PYZus{}GRADS}\PY{p}{)}\PY{p}{:}
                      \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===========}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{gradient:\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{)}
                      \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{max: \PYZob{}\PYZcb{}, mean: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{grad\PYZus{}max}\PY{p}{,} \PY{n}{abs\PYZus{}grad\PYZus{}mean}\PY{p}{)}\PY{p}{)}
                      
                  \PY{n}{stats} \PY{o}{=} \PY{p}{[}\PY{n}{n}\PY{p}{,} \PY{n}{grad\PYZus{}max}\PY{p}{,} \PY{n}{abs\PYZus{}grad\PYZus{}mean}\PY{p}{]}
                  
                  \PY{k}{if} \PY{p}{(}\PY{n}{abs\PYZus{}grad\PYZus{}mean} \PY{o}{\PYZlt{}} \PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{)}\PY{p}{:}
                      \PY{n}{stats}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SMALL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      \PY{n}{bad\PYZus{}grads}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{stats}\PY{p}{)}
                  \PY{k}{elif} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZlt{}} \PY{n}{abs\PYZus{}grad\PYZus{}mean}\PY{p}{)}\PY{p}{:}
                      \PY{n}{stats}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BIG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                      \PY{n}{bad\PYZus{}grads}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{stats}\PY{p}{)}
                      
              \PY{k}{for} \PY{n}{n}\PY{p}{,} \PY{n}{grad\PYZus{}max}\PY{p}{,} \PY{n}{abs\PYZus{}grad\PYZus{}mean}\PY{p}{,} \PY{n}{descr} \PY{o+ow}{in} \PY{n}{bad\PYZus{}grads}\PY{p}{:}
                  \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{===========}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{ \PYZob{}\PYZcb{} gradient:\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{descr}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{)}
                  \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{max: \PYZob{}\PYZcb{}, mean: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{grad\PYZus{}max}\PY{p}{,} \PY{n}{abs\PYZus{}grad\PYZus{}mean}\PY{p}{)}\PY{p}{)}  
                  
              \PY{k}{if} \PY{p}{(}\PY{n}{VERBOSE}\PY{p}{)}\PY{p}{:}
                  \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weight\PYZus{}hh\PYZus{}l0:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{k}{print}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{rnn\PYZus{}layer}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weight\PYZus{}hh\PYZus{}l0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          
              
              \PY{n}{net}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n+nb+bp}{False}\PY{p}{)}
              
              \PY{n}{images} \PY{o}{=} \PY{n}{Variable}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{Tensor}\PY{p}{(}\PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}X}\PY{p}{)}\PY{p}{)}
              \PY{n}{test\PYZus{}labels} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{n}{rnn\PYZus{}training\PYZus{}data\PYZus{}Y}\PY{p}{)}
              
              \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
              \PY{k}{if} \PY{p}{(}\PY{n}{FCC\PYZus{}SANITY}\PY{p}{)}\PY{p}{:}
                  \PY{n}{outputs} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{images}\PY{p}{)}
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{outputs}\PY{p}{,} \PY{n}{hidden} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{images}\PY{p}{)}
              
              \PY{k}{print}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
              
              \PY{n}{correct} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{n}{total} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
              \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n}{test\PYZus{}labels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
              \PY{n}{correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{predicted} \PY{o}{==} \PY{n}{test\PYZus{}labels}\PY{p}{)}\PY{o}{.}\PY{n}{double}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
              \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Accuracy: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+m+mi}{100} \PY{o}{*} \PY{n+nb}{int}\PY{p}{(}\PY{n}{correct}\PY{p}{)} \PY{o}{/} \PY{n+nb}{float}\PY{p}{(}\PY{n}{total}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              \PY{n}{train\PYZus{}acc}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{100} \PY{o}{*} \PY{n}{correct} \PY{o}{/} \PY{n}{total}\PY{p}{)}\PY{p}{)}
          
              \PY{n}{net}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Change model to \PYZsq{}eval\PYZsq{} mode (BN uses moving mean/var).}
              \PY{n}{correct} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{n}{total} \PY{o}{=} \PY{l+m+mi}{0}
              \PY{n}{images} \PY{o}{=} \PY{n}{Variable}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{Tensor}\PY{p}{(}\PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}X}\PY{p}{)}\PY{p}{)}
              \PY{n}{test\PYZus{}labels} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{n}{rnn\PYZus{}validation\PYZus{}data\PYZus{}Y}\PY{p}{)}
              
              \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
              \PY{k}{if} \PY{p}{(}\PY{n}{FCC\PYZus{}SANITY}\PY{p}{)}\PY{p}{:}
                  \PY{n}{outputs} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{images}\PY{p}{)}
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{outputs}\PY{p}{,} \PY{n}{hidden} \PY{o}{=} \PY{n}{net}\PY{p}{(}\PY{n}{images}\PY{p}{)}
              
              \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
              
              \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{predicted set \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{predicted}\PY{o}{.}\PY{n}{data}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label set \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{test\PYZus{}labels}\PY{o}{.}\PY{n}{data}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              
              \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n}{test\PYZus{}labels}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
              \PY{n}{correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{predicted} \PY{o}{==} \PY{n}{test\PYZus{}labels}\PY{p}{)}\PY{o}{.}\PY{n}{double}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
              \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation Accuracy: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+m+mf}{100.0} \PY{o}{*} \PY{n+nb}{int}\PY{p}{(}\PY{n}{correct}\PY{p}{)} \PY{o}{/} \PY{n+nb}{float}\PY{p}{(}\PY{n}{total}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              \PY{n}{val\PYZus{}acc}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{100} \PY{o}{*} \PY{n}{correct} \PY{o}{/} \PY{n}{total}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/usr/local/lib/python2.7/dist-packages/ipykernel\_launcher.py:68: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Epoch [1/30], Iter [1/17] Loss: [----- 1.4447 = 1.4447 + 1.00 * 0.0000 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [2/17] Loss: [----- 1.3888 = 1.3888 + 1.00 * 0.0000 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [3/17] Loss: [----- 1.4606 = 1.4365 + 1.00 * 0.0240 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [4/17] Loss: [----- 1.4123 = 1.4117 + 1.00 * 0.0006 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [5/17] Loss: [----- 1.4603 = 1.4542 + 1.00 * 0.0060 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [6/17] Loss: [----- 1.3773 = 1.3658 + 1.00 * 0.0115 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [7/17] Loss: [----- 1.3883 = 1.3641 + 1.00 * 0.0242 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [8/17] Loss: [----- 1.5416 = 1.3846 + 1.00 * 0.1571 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [9/17] Loss: [----- 1.4784 = 1.3705 + 1.00 * 0.1079 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [10/17] Loss: [----- 1.7033 = 1.4328 + 1.00 * 0.2705 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [11/17] Loss: [----- 1.4276 = 1.4106 + 1.00 * 0.0170 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [12/17] Loss: [----- 1.4431 = 1.4265 + 1.00 * 0.0166 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [13/17] Loss: [----- 1.5547 = 1.3790 + 1.00 * 0.1757 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [14/17] Loss: [----- 1.5695 = 1.4294 + 1.00 * 0.1401 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [15/17] Loss: [----- 1.4028 = 1.3876 + 1.00 * 0.0152 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [16/17] Loss: [----- 1.6598 = 1.4217 + 1.00 * 0.2381 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [17/17] Loss: [----- 1.5879 = 1.4229 + 1.00 * 0.1649 + 0.00 * 0.0000 -----]
Epoch [1/30], Iter [18/17] Loss: [----- 1.4726 = 1.3838 + 1.00 * 0.0887 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 3.2271650241e-08, mean: 1.02025472515e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 26 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 3]
label set [0 1 2 3]
validation Accuracy: 25 \%
Epoch [2/30], Iter [1/17] Loss: [----- 1.3600 = 1.3552 + 1.00 * 0.0047 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [2/17] Loss: [----- 1.3971 = 1.3881 + 1.00 * 0.0090 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [3/17] Loss: [----- 1.4451 = 1.4023 + 1.00 * 0.0427 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [4/17] Loss: [----- 1.4559 = 1.3789 + 1.00 * 0.0770 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [5/17] Loss: [----- 1.3836 = 1.3454 + 1.00 * 0.0382 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [6/17] Loss: [----- 1.3809 = 1.3690 + 1.00 * 0.0119 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [7/17] Loss: [----- 1.3943 = 1.3742 + 1.00 * 0.0201 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [8/17] Loss: [----- 1.3831 = 1.3492 + 1.00 * 0.0339 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [9/17] Loss: [----- 1.4894 = 1.4098 + 1.00 * 0.0796 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [10/17] Loss: [----- 1.3915 = 1.3775 + 1.00 * 0.0140 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [11/17] Loss: [----- 1.5488 = 1.3906 + 1.00 * 0.1582 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [12/17] Loss: [----- 1.4045 = 1.3737 + 1.00 * 0.0308 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [13/17] Loss: [----- 1.3838 = 1.3715 + 1.00 * 0.0123 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [14/17] Loss: [----- 1.4303 = 1.4257 + 1.00 * 0.0046 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [15/17] Loss: [----- 1.3883 = 1.3554 + 1.00 * 0.0330 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [16/17] Loss: [----- 1.3660 = 1.3650 + 1.00 * 0.0009 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [17/17] Loss: [----- 1.3942 = 1.3875 + 1.00 * 0.0067 + 0.00 * 0.0000 -----]
Epoch [2/30], Iter [18/17] Loss: [----- 1.4191 = 1.4139 + 1.00 * 0.0053 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 3.65078449249e-07, mean: 1.03493221104e-07

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_8.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_9.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 27 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_11.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_12.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 24 \%
Epoch [3/30], Iter [1/17] Loss: [----- 1.3658 = 1.3399 + 1.00 * 0.0258 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [2/17] Loss: [----- 1.3884 = 1.3770 + 1.00 * 0.0114 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [3/17] Loss: [----- 1.3665 = 1.3610 + 1.00 * 0.0055 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [4/17] Loss: [----- 1.3797 = 1.3724 + 1.00 * 0.0072 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [5/17] Loss: [----- 1.3805 = 1.3777 + 1.00 * 0.0028 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [6/17] Loss: [----- 1.3881 = 1.3857 + 1.00 * 0.0024 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [7/17] Loss: [----- 1.3661 = 1.3633 + 1.00 * 0.0028 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [8/17] Loss: [----- 1.3717 = 1.3674 + 1.00 * 0.0044 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [9/17] Loss: [----- 1.3782 = 1.3411 + 1.00 * 0.0372 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [10/17] Loss: [----- 1.5751 = 1.3448 + 1.00 * 0.2303 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [11/17] Loss: [----- 1.3971 = 1.3757 + 1.00 * 0.0214 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [12/17] Loss: [----- 1.3885 = 1.3608 + 1.00 * 0.0277 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [13/17] Loss: [----- 1.4349 = 1.3619 + 1.00 * 0.0730 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [14/17] Loss: [----- 1.4588 = 1.3748 + 1.00 * 0.0840 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [15/17] Loss: [----- 1.3924 = 1.3771 + 1.00 * 0.0152 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [16/17] Loss: [----- 1.3977 = 1.3807 + 1.00 * 0.0170 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [17/17] Loss: [----- 1.3716 = 1.3571 + 1.00 * 0.0145 + 0.00 * 0.0000 -----]
Epoch [3/30], Iter [18/17] Loss: [----- 1.3590 = 1.3463 + 1.00 * 0.0128 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.3910029395e-07, mean: 4.20198809081e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_14.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_15.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 31 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_17.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_18.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 26 \%
Epoch [4/30], Iter [1/17] Loss: [----- 1.3327 = 1.3317 + 1.00 * 0.0010 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [2/17] Loss: [----- 1.9420 = 1.3347 + 1.00 * 0.6073 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [3/17] Loss: [----- 1.3876 = 1.3379 + 1.00 * 0.0497 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [4/17] Loss: [----- 1.6837 = 1.3503 + 1.00 * 0.3334 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [5/17] Loss: [----- 1.3800 = 1.3606 + 1.00 * 0.0194 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [6/17] Loss: [----- 1.4731 = 1.3821 + 1.00 * 0.0910 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [7/17] Loss: [----- 1.3847 = 1.3820 + 1.00 * 0.0026 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [8/17] Loss: [----- 1.5527 = 1.3493 + 1.00 * 0.2034 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [9/17] Loss: [----- 1.4870 = 1.3497 + 1.00 * 0.1373 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [10/17] Loss: [----- 1.4873 = 1.3583 + 1.00 * 0.1290 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [11/17] Loss: [----- 1.3657 = 1.3554 + 1.00 * 0.0103 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [12/17] Loss: [----- 1.4588 = 1.3783 + 1.00 * 0.0804 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [13/17] Loss: [----- 1.7157 = 1.3765 + 1.00 * 0.3392 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [14/17] Loss: [----- 1.5356 = 1.3818 + 1.00 * 0.1539 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [15/17] Loss: [----- 1.3621 = 1.3471 + 1.00 * 0.0150 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [16/17] Loss: [----- 1.3439 = 1.3371 + 1.00 * 0.0068 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [17/17] Loss: [----- 1.3776 = 1.3677 + 1.00 * 0.0098 + 0.00 * 0.0000 -----]
Epoch [4/30], Iter [18/17] Loss: [----- 1.4232 = 1.3712 + 1.00 * 0.0519 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 4.95161316394e-08, mean: 1.4351940969e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_20.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_21.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 30 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_23.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_24.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 25 \%
Epoch [5/30], Iter [1/17] Loss: [----- 1.3973 = 1.3243 + 1.00 * 0.0730 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [2/17] Loss: [----- 1.3647 = 1.3527 + 1.00 * 0.0120 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [3/17] Loss: [----- 1.3676 = 1.3372 + 1.00 * 0.0303 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [4/17] Loss: [----- 1.3629 = 1.3295 + 1.00 * 0.0334 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [5/17] Loss: [----- 1.3991 = 1.3542 + 1.00 * 0.0448 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [6/17] Loss: [----- 1.3528 = 1.3353 + 1.00 * 0.0174 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [7/17] Loss: [----- 1.7157 = 1.3550 + 1.00 * 0.3607 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [8/17] Loss: [----- 1.4029 = 1.3486 + 1.00 * 0.0542 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [9/17] Loss: [----- 1.3938 = 1.3581 + 1.00 * 0.0357 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [10/17] Loss: [----- 1.3266 = 1.3239 + 1.00 * 0.0027 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [11/17] Loss: [----- 1.4315 = 1.3303 + 1.00 * 0.1012 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [12/17] Loss: [----- 1.4201 = 1.3902 + 1.00 * 0.0299 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [13/17] Loss: [----- 1.3634 = 1.3507 + 1.00 * 0.0128 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [14/17] Loss: [----- 1.3698 = 1.3362 + 1.00 * 0.0336 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [15/17] Loss: [----- 1.3980 = 1.3433 + 1.00 * 0.0548 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [16/17] Loss: [----- 1.5541 = 1.3599 + 1.00 * 0.1942 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [17/17] Loss: [----- 1.3770 = 1.3580 + 1.00 * 0.0190 + 0.00 * 0.0000 -----]
Epoch [5/30], Iter [18/17] Loss: [----- 2.0085 = 1.3067 + 1.00 * 0.7018 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:norm\_layer.bias
---------
max: 0.000136381480843, mean: 6.59913057461e-05
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.26287904578e-08, mean: 4.50874626523e-09
===========
 SMALL gradient:output\_layer.preoutput\_ batchnorm.bias
---------
max: 0.000264455826255, mean: 8.33801459521e-05

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_26.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_27.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 33 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_29.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_30.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 26 \%
Epoch [6/30], Iter [1/17] Loss: [----- 1.5647 = 1.3221 + 1.00 * 0.2425 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [2/17] Loss: [----- 1.5616 = 1.3370 + 1.00 * 0.2246 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [3/17] Loss: [----- 1.3772 = 1.3237 + 1.00 * 0.0535 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [4/17] Loss: [----- 1.4479 = 1.2907 + 1.00 * 0.1572 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [5/17] Loss: [----- 1.7549 = 1.2968 + 1.00 * 0.4580 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [6/17] Loss: [----- 1.4323 = 1.3372 + 1.00 * 0.0951 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [7/17] Loss: [----- 1.4305 = 1.3265 + 1.00 * 0.1040 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [8/17] Loss: [----- 1.3544 = 1.3421 + 1.00 * 0.0123 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [9/17] Loss: [----- 1.9680 = 1.3340 + 1.00 * 0.6340 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [10/17] Loss: [----- 1.3826 = 1.2808 + 1.00 * 0.1019 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [11/17] Loss: [----- 1.3456 = 1.3388 + 1.00 * 0.0068 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [12/17] Loss: [----- 1.3570 = 1.3342 + 1.00 * 0.0229 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [13/17] Loss: [----- 1.3970 = 1.3636 + 1.00 * 0.0333 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [14/17] Loss: [----- 1.4238 = 1.3282 + 1.00 * 0.0956 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [15/17] Loss: [----- 1.3931 = 1.3350 + 1.00 * 0.0581 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [16/17] Loss: [----- 1.6301 = 1.3172 + 1.00 * 0.3129 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [17/17] Loss: [----- 1.5844 = 1.3187 + 1.00 * 0.2657 + 0.00 * 0.0000 -----]
Epoch [6/30], Iter [18/17] Loss: [----- 1.4684 = 1.3381 + 1.00 * 0.1303 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.68290767988e-08, mean: 1.11357172372e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_32.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_33.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 32 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_35.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_36.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 26 \%
Epoch [7/30], Iter [1/17] Loss: [----- 1.4188 = 1.3382 + 1.00 * 0.0806 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [2/17] Loss: [----- 1.4011 = 1.3694 + 1.00 * 0.0318 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [3/17] Loss: [----- 1.3758 = 1.3020 + 1.00 * 0.0738 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [4/17] Loss: [----- 1.3303 = 1.3136 + 1.00 * 0.0167 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [5/17] Loss: [----- 1.3271 = 1.3154 + 1.00 * 0.0117 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [6/17] Loss: [----- 1.3499 = 1.3012 + 1.00 * 0.0487 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [7/17] Loss: [----- 1.3379 = 1.3103 + 1.00 * 0.0275 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [8/17] Loss: [----- 1.4128 = 1.3961 + 1.00 * 0.0166 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [9/17] Loss: [----- 1.3536 = 1.3142 + 1.00 * 0.0394 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [10/17] Loss: [----- 1.3396 = 1.3205 + 1.00 * 0.0191 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [11/17] Loss: [----- 1.4554 = 1.3185 + 1.00 * 0.1369 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [12/17] Loss: [----- 1.3674 = 1.3402 + 1.00 * 0.0272 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [13/17] Loss: [----- 1.3240 = 1.3211 + 1.00 * 0.0029 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [14/17] Loss: [----- 1.3652 = 1.3450 + 1.00 * 0.0202 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [15/17] Loss: [----- 1.9773 = 1.2904 + 1.00 * 0.6869 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [16/17] Loss: [----- 1.4780 = 1.3247 + 1.00 * 0.1533 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [17/17] Loss: [----- 1.3872 = 1.3206 + 1.00 * 0.0666 + 0.00 * 0.0000 -----]
Epoch [7/30], Iter [18/17] Loss: [----- 1.3092 = 1.2956 + 1.00 * 0.0136 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 4.87062493448e-07, mean: 1.10315291124e-07

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_38.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_39.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 37 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_41.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_42.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 29 \%
Epoch [8/30], Iter [1/17] Loss: [----- 1.3875 = 1.3267 + 1.00 * 0.0607 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [2/17] Loss: [----- 1.3521 = 1.3228 + 1.00 * 0.0294 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [3/17] Loss: [----- 1.3938 = 1.2619 + 1.00 * 0.1319 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [4/17] Loss: [----- 1.3350 = 1.2671 + 1.00 * 0.0679 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [5/17] Loss: [----- 1.6806 = 1.3043 + 1.00 * 0.3763 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [6/17] Loss: [----- 1.6398 = 1.3074 + 1.00 * 0.3324 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [7/17] Loss: [----- 1.3815 = 1.3440 + 1.00 * 0.0375 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [8/17] Loss: [----- 1.4103 = 1.2651 + 1.00 * 0.1452 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [9/17] Loss: [----- 1.4151 = 1.3177 + 1.00 * 0.0974 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [10/17] Loss: [----- 1.4847 = 1.3430 + 1.00 * 0.1417 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [11/17] Loss: [----- 1.3672 = 1.3240 + 1.00 * 0.0432 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [12/17] Loss: [----- 1.3893 = 1.3577 + 1.00 * 0.0316 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [13/17] Loss: [----- 1.5434 = 1.3270 + 1.00 * 0.2164 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [14/17] Loss: [----- 1.5032 = 1.3448 + 1.00 * 0.1584 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [15/17] Loss: [----- 1.3843 = 1.2965 + 1.00 * 0.0878 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [16/17] Loss: [----- 1.5094 = 1.3193 + 1.00 * 0.1900 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [17/17] Loss: [----- 1.4272 = 1.3108 + 1.00 * 0.1164 + 0.00 * 0.0000 -----]
Epoch [8/30], Iter [18/17] Loss: [----- 1.4989 = 1.3259 + 1.00 * 0.1730 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 3.58844758352e-08, mean: 1.37494362562e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_44.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_45.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 36 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_47.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_48.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 30 \%
Epoch [9/30], Iter [1/17] Loss: [----- 1.4780 = 1.2810 + 1.00 * 0.1970 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [2/17] Loss: [----- 1.3324 = 1.3185 + 1.00 * 0.0139 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [3/17] Loss: [----- 1.3919 = 1.3147 + 1.00 * 0.0771 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [4/17] Loss: [----- 1.4298 = 1.2861 + 1.00 * 0.1438 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [5/17] Loss: [----- 1.4008 = 1.2988 + 1.00 * 0.1020 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [6/17] Loss: [----- 1.6972 = 1.3203 + 1.00 * 0.3770 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [7/17] Loss: [----- 1.3858 = 1.3091 + 1.00 * 0.0767 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [8/17] Loss: [----- 1.4939 = 1.2927 + 1.00 * 0.2012 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [9/17] Loss: [----- 1.3543 = 1.3106 + 1.00 * 0.0437 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [10/17] Loss: [----- 1.3556 = 1.3070 + 1.00 * 0.0486 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [11/17] Loss: [----- 1.3569 = 1.3144 + 1.00 * 0.0425 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [12/17] Loss: [----- 1.3648 = 1.2770 + 1.00 * 0.0878 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [13/17] Loss: [----- 1.6617 = 1.2905 + 1.00 * 0.3712 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [14/17] Loss: [----- 1.3249 = 1.3090 + 1.00 * 0.0158 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [15/17] Loss: [----- 1.4479 = 1.3090 + 1.00 * 0.1389 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [16/17] Loss: [----- 1.3645 = 1.3451 + 1.00 * 0.0194 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [17/17] Loss: [----- 1.5432 = 1.2809 + 1.00 * 0.2623 + 0.00 * 0.0000 -----]
Epoch [9/30], Iter [18/17] Loss: [----- 1.3055 = 1.2968 + 1.00 * 0.0087 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.27234315528e-07, mean: 7.38757321983e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_50.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_51.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 37 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_53.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_54.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 31 \%
Epoch [10/30], Iter [1/17] Loss: [----- 1.3882 = 1.3186 + 1.00 * 0.0696 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [2/17] Loss: [----- 1.2690 = 1.2653 + 1.00 * 0.0037 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [3/17] Loss: [----- 1.3441 = 1.2941 + 1.00 * 0.0500 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [4/17] Loss: [----- 1.3285 = 1.2328 + 1.00 * 0.0957 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [5/17] Loss: [----- 1.3349 = 1.2613 + 1.00 * 0.0736 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [6/17] Loss: [----- 1.4644 = 1.3335 + 1.00 * 0.1309 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [7/17] Loss: [----- 1.3880 = 1.2938 + 1.00 * 0.0942 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [8/17] Loss: [----- 1.3354 = 1.3248 + 1.00 * 0.0107 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [9/17] Loss: [----- 1.9038 = 1.3051 + 1.00 * 0.5988 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [10/17] Loss: [----- 1.6066 = 1.3053 + 1.00 * 0.3013 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [11/17] Loss: [----- 2.1351 = 1.3038 + 1.00 * 0.8313 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [12/17] Loss: [----- 1.7066 = 1.3019 + 1.00 * 0.4046 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [13/17] Loss: [----- 2.7718 = 1.3231 + 1.00 * 1.4487 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [14/17] Loss: [----- 1.2765 = 1.2722 + 1.00 * 0.0043 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [15/17] Loss: [----- 1.4846 = 1.2942 + 1.00 * 0.1904 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [16/17] Loss: [----- 1.8108 = 1.3111 + 1.00 * 0.4997 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [17/17] Loss: [----- 1.4715 = 1.2708 + 1.00 * 0.2006 + 0.00 * 0.0000 -----]
Epoch [10/30], Iter [18/17] Loss: [----- 2.4412 = 1.2983 + 1.00 * 1.1429 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 6.16025772615e-08, mean: 1.56675028506e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_56.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_57.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 36 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_59.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_60.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 31 \%
Epoch [11/30], Iter [1/17] Loss: [----- 1.5293 = 1.2885 + 1.00 * 0.2408 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [2/17] Loss: [----- 1.3500 = 1.2965 + 1.00 * 0.0535 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [3/17] Loss: [----- 1.3801 = 1.3317 + 1.00 * 0.0484 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [4/17] Loss: [----- 1.3337 = 1.2772 + 1.00 * 0.0564 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [5/17] Loss: [----- 1.5215 = 1.2694 + 1.00 * 0.2520 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [6/17] Loss: [----- 1.3387 = 1.2762 + 1.00 * 0.0624 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [7/17] Loss: [----- 1.3028 = 1.2501 + 1.00 * 0.0527 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [8/17] Loss: [----- 1.5038 = 1.2860 + 1.00 * 0.2177 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [9/17] Loss: [----- 1.5307 = 1.3267 + 1.00 * 0.2040 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [10/17] Loss: [----- 1.4250 = 1.3085 + 1.00 * 0.1165 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [11/17] Loss: [----- 1.3687 = 1.2423 + 1.00 * 0.1264 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [12/17] Loss: [----- 1.8291 = 1.2891 + 1.00 * 0.5400 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [13/17] Loss: [----- 2.0486 = 1.3212 + 1.00 * 0.7274 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [14/17] Loss: [----- 1.8457 = 1.3594 + 1.00 * 0.4863 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [15/17] Loss: [----- 2.6284 = 1.2614 + 1.00 * 1.3670 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [16/17] Loss: [----- 1.3366 = 1.3094 + 1.00 * 0.0272 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [17/17] Loss: [----- 1.7940 = 1.3056 + 1.00 * 0.4885 + 0.00 * 0.0000 -----]
Epoch [11/30], Iter [18/17] Loss: [----- 2.9152 = 1.2727 + 1.00 * 1.6425 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:norm\_layer.bias
---------
max: 0.000152061795234, mean: 8.49284406286e-05
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.25232233472e-08, mean: 4.20014112379e-09

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_62.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_63.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 36 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_65.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_66.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 31 \%
Epoch [12/30], Iter [1/17] Loss: [----- 1.8351 = 1.2849 + 1.00 * 0.5502 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [2/17] Loss: [----- 1.3241 = 1.2650 + 1.00 * 0.0591 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [3/17] Loss: [----- 3.3352 = 1.2400 + 1.00 * 2.0952 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [4/17] Loss: [----- 1.5956 = 1.2775 + 1.00 * 0.3180 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [5/17] Loss: [----- 2.2296 = 1.2546 + 1.00 * 0.9750 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [6/17] Loss: [----- 1.3295 = 1.3021 + 1.00 * 0.0273 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [7/17] Loss: [----- 1.4660 = 1.2758 + 1.00 * 0.1903 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [8/17] Loss: [----- 1.3476 = 1.3014 + 1.00 * 0.0462 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [9/17] Loss: [----- 1.3169 = 1.2934 + 1.00 * 0.0235 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [10/17] Loss: [----- 1.4544 = 1.2795 + 1.00 * 0.1749 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [11/17] Loss: [----- 3.9020 = 1.3354 + 1.00 * 2.5666 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [12/17] Loss: [----- 1.2411 = 1.2385 + 1.00 * 0.0025 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [13/17] Loss: [----- 1.3957 = 1.2559 + 1.00 * 0.1398 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [14/17] Loss: [----- 1.3245 = 1.2953 + 1.00 * 0.0292 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [15/17] Loss: [----- 1.5533 = 1.3216 + 1.00 * 0.2317 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [16/17] Loss: [----- 1.3675 = 1.3434 + 1.00 * 0.0241 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [17/17] Loss: [----- 7.7542 = 1.3431 + 1.00 * 6.4112 + 0.00 * 0.0000 -----]
Epoch [12/30], Iter [18/17] Loss: [----- 1.4889 = 1.2611 + 1.00 * 0.2278 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 8.41892173753e-08, mean: 3.12238235267e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_68.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_69.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 35 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_71.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_72.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 31 \%
Epoch [13/30], Iter [1/17] Loss: [----- 2.0100 = 1.3143 + 1.00 * 0.6957 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [2/17] Loss: [----- 1.3931 = 1.1905 + 1.00 * 0.2026 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [3/17] Loss: [----- 1.3970 = 1.2878 + 1.00 * 0.1092 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [4/17] Loss: [----- 2.0143 = 1.2905 + 1.00 * 0.7238 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [5/17] Loss: [----- 1.3449 = 1.2731 + 1.00 * 0.0718 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [6/17] Loss: [----- 5.0624 = 1.2029 + 1.00 * 3.8595 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [7/17] Loss: [----- 2.3007 = 1.2526 + 1.00 * 1.0481 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [8/17] Loss: [----- 1.4978 = 1.3259 + 1.00 * 0.1719 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [9/17] Loss: [----- 2.2696 = 1.2976 + 1.00 * 0.9720 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [10/17] Loss: [----- 5.1166 = 1.2885 + 1.00 * 3.8281 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [11/17] Loss: [----- 1.4466 = 1.2548 + 1.00 * 0.1918 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [12/17] Loss: [----- 1.8281 = 1.2901 + 1.00 * 0.5380 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [13/17] Loss: [----- 1.4969 = 1.3235 + 1.00 * 0.1734 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [14/17] Loss: [----- 2.9118 = 1.2587 + 1.00 * 1.6530 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [15/17] Loss: [----- 1.3057 = 1.2948 + 1.00 * 0.0109 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [16/17] Loss: [----- 2.3384 = 1.3056 + 1.00 * 1.0328 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [17/17] Loss: [----- 4.6594 = 1.2979 + 1.00 * 3.3615 + 0.00 * 0.0000 -----]
Epoch [13/30], Iter [18/17] Loss: [----- 2.0744 = 1.3277 + 1.00 * 0.7467 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 5.58644330795e-08, mean: 1.47259830996e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_74.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_75.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 34 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_77.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_78.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 26 \%
Epoch [14/30], Iter [1/17] Loss: [----- 2.0242 = 1.2838 + 1.00 * 0.7404 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [2/17] Loss: [----- 1.3458 = 1.2379 + 1.00 * 0.1079 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [3/17] Loss: [----- 1.2970 = 1.2285 + 1.00 * 0.0684 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [4/17] Loss: [----- 1.3732 = 1.3225 + 1.00 * 0.0507 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [5/17] Loss: [----- 1.8716 = 1.2431 + 1.00 * 0.6285 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [6/17] Loss: [----- 1.8973 = 1.2437 + 1.00 * 0.6536 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [7/17] Loss: [----- 2.3401 = 1.2969 + 1.00 * 1.0432 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [8/17] Loss: [----- 1.4042 = 1.3150 + 1.00 * 0.0891 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [9/17] Loss: [----- 1.7015 = 1.3086 + 1.00 * 0.3929 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [10/17] Loss: [----- 1.9742 = 1.3053 + 1.00 * 0.6689 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [11/17] Loss: [----- 1.5209 = 1.2359 + 1.00 * 0.2850 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [12/17] Loss: [----- 1.6694 = 1.3097 + 1.00 * 0.3598 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [13/17] Loss: [----- 1.5182 = 1.2973 + 1.00 * 0.2209 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [14/17] Loss: [----- 2.0988 = 1.3248 + 1.00 * 0.7740 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [15/17] Loss: [----- 2.0647 = 1.3014 + 1.00 * 0.7633 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [16/17] Loss: [----- 1.4077 = 1.2331 + 1.00 * 0.1746 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [17/17] Loss: [----- 1.3138 = 1.3075 + 1.00 * 0.0063 + 0.00 * 0.0000 -----]
Epoch [14/30], Iter [18/17] Loss: [----- 4.9665 = 1.2821 + 1.00 * 3.6845 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 3.6580662055e-08, mean: 7.86111087336e-09

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_80.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_81.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 38 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_83.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_84.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 33 \%
Epoch [15/30], Iter [1/17] Loss: [----- 1.2224 = 1.2171 + 1.00 * 0.0054 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [2/17] Loss: [----- 1.9744 = 1.2792 + 1.00 * 0.6952 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [3/17] Loss: [----- 1.3051 = 1.2493 + 1.00 * 0.0558 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [4/17] Loss: [----- 1.3060 = 1.2849 + 1.00 * 0.0211 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [5/17] Loss: [----- 1.5965 = 1.3234 + 1.00 * 0.2730 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [6/17] Loss: [----- 1.8157 = 1.3254 + 1.00 * 0.4902 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [7/17] Loss: [----- 3.3598 = 1.2973 + 1.00 * 2.0625 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [8/17] Loss: [----- 3.7069 = 1.2857 + 1.00 * 2.4213 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [9/17] Loss: [----- 2.2030 = 1.2826 + 1.00 * 0.9204 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [10/17] Loss: [----- 1.3244 = 1.3004 + 1.00 * 0.0240 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [11/17] Loss: [----- 1.6538 = 1.2855 + 1.00 * 0.3683 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [12/17] Loss: [----- 1.5162 = 1.2936 + 1.00 * 0.2226 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [13/17] Loss: [----- 2.7597 = 1.3233 + 1.00 * 1.4363 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [14/17] Loss: [----- 1.6517 = 1.2778 + 1.00 * 0.3739 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [15/17] Loss: [----- 4.2238 = 1.3849 + 1.00 * 2.8389 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [16/17] Loss: [----- 1.4887 = 1.2349 + 1.00 * 0.2538 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [17/17] Loss: [----- 1.3410 = 1.3014 + 1.00 * 0.0396 + 0.00 * 0.0000 -----]
Epoch [15/30], Iter [18/17] Loss: [----- 1.2342 = 1.2258 + 1.00 * 0.0084 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 5.96046447754e-07, mean: 2.57568899542e-07

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_86.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_87.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 38 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_89.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_90.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 34 \%
Epoch [16/30], Iter [1/17] Loss: [----- 1.2368 = 1.2098 + 1.00 * 0.0270 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [2/17] Loss: [----- 1.4903 = 1.2612 + 1.00 * 0.2290 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [3/17] Loss: [----- 3.6465 = 1.3113 + 1.00 * 2.3352 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [4/17] Loss: [----- 1.4108 = 1.2881 + 1.00 * 0.1227 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [5/17] Loss: [----- 1.7029 = 1.2087 + 1.00 * 0.4942 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [6/17] Loss: [----- 1.4351 = 1.2615 + 1.00 * 0.1736 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [7/17] Loss: [----- 1.2941 = 1.2577 + 1.00 * 0.0363 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [8/17] Loss: [----- 3.1499 = 1.2712 + 1.00 * 1.8787 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [9/17] Loss: [----- 2.8069 = 1.2560 + 1.00 * 1.5509 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [10/17] Loss: [----- 1.3816 = 1.2998 + 1.00 * 0.0818 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [11/17] Loss: [----- 1.4352 = 1.2876 + 1.00 * 0.1476 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [12/17] Loss: [----- 1.3015 = 1.2293 + 1.00 * 0.0721 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [13/17] Loss: [----- 1.7130 = 1.2767 + 1.00 * 0.4363 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [14/17] Loss: [----- 1.4902 = 1.2737 + 1.00 * 0.2165 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [15/17] Loss: [----- 1.3570 = 1.2707 + 1.00 * 0.0863 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [16/17] Loss: [----- 1.3072 = 1.2663 + 1.00 * 0.0409 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [17/17] Loss: [----- 2.1133 = 1.2334 + 1.00 * 0.8799 + 0.00 * 0.0000 -----]
Epoch [16/30], Iter [18/17] Loss: [----- 1.3917 = 1.2860 + 1.00 * 0.1057 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 2.96604639516e-07, mean: 1.13272697888e-07

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_92.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_93.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 39 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_95.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_96.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 32 \%
Epoch [17/30], Iter [1/17] Loss: [----- 1.3802 = 1.2878 + 1.00 * 0.0924 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [2/17] Loss: [----- 1.6432 = 1.2371 + 1.00 * 0.4061 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [3/17] Loss: [----- 1.2753 = 1.2329 + 1.00 * 0.0424 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [4/17] Loss: [----- 1.8123 = 1.2406 + 1.00 * 0.5716 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [5/17] Loss: [----- 1.7711 = 1.2384 + 1.00 * 0.5326 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [6/17] Loss: [----- 6.3603 = 1.2204 + 1.00 * 5.1399 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [7/17] Loss: [----- 1.3971 = 1.2662 + 1.00 * 0.1309 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [8/17] Loss: [----- 1.2756 = 1.2369 + 1.00 * 0.0386 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [9/17] Loss: [----- 1.8630 = 1.2349 + 1.00 * 0.6282 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [10/17] Loss: [----- 1.3231 = 1.2826 + 1.00 * 0.0405 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [11/17] Loss: [----- 1.3476 = 1.3023 + 1.00 * 0.0453 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [12/17] Loss: [----- 2.1469 = 1.1875 + 1.00 * 0.9595 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [13/17] Loss: [----- 1.3700 = 1.2683 + 1.00 * 0.1017 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [14/17] Loss: [----- 1.3103 = 1.2146 + 1.00 * 0.0957 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [15/17] Loss: [----- 2.9897 = 1.2744 + 1.00 * 1.7153 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [16/17] Loss: [----- 1.3815 = 1.2439 + 1.00 * 0.1376 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [17/17] Loss: [----- 2.1135 = 1.2545 + 1.00 * 0.8590 + 0.00 * 0.0000 -----]
Epoch [17/30], Iter [18/17] Loss: [----- 1.2271 = 1.1770 + 1.00 * 0.0500 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 2.08183791983e-07, mean: 5.76481156145e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_98.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_99.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 43 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_101.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_102.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 36 \%
Epoch [18/30], Iter [1/17] Loss: [----- 2.0923 = 1.2741 + 1.00 * 0.8182 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [2/17] Loss: [----- 1.7553 = 1.2236 + 1.00 * 0.5317 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [3/17] Loss: [----- 2.3965 = 1.1850 + 1.00 * 1.2116 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [4/17] Loss: [----- 1.7419 = 1.1952 + 1.00 * 0.5466 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [5/17] Loss: [----- 1.2614 = 1.2414 + 1.00 * 0.0200 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [6/17] Loss: [----- 1.2177 = 1.2016 + 1.00 * 0.0160 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [7/17] Loss: [----- 1.4132 = 1.2053 + 1.00 * 0.2080 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [8/17] Loss: [----- 1.3474 = 1.1973 + 1.00 * 0.1501 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [9/17] Loss: [----- 1.4323 = 1.1753 + 1.00 * 0.2570 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [10/17] Loss: [----- 2.2174 = 1.2997 + 1.00 * 0.9177 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [11/17] Loss: [----- 3.5052 = 1.2424 + 1.00 * 2.2627 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [12/17] Loss: [----- 2.0386 = 1.2924 + 1.00 * 0.7463 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [13/17] Loss: [----- 2.8091 = 1.2221 + 1.00 * 1.5870 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [14/17] Loss: [----- 1.3895 = 1.2554 + 1.00 * 0.1341 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [15/17] Loss: [----- 2.2338 = 1.2957 + 1.00 * 0.9381 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [16/17] Loss: [----- 2.5463 = 1.2924 + 1.00 * 1.2539 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [17/17] Loss: [----- 1.7508 = 1.2428 + 1.00 * 0.5080 + 0.00 * 0.0000 -----]
Epoch [18/30], Iter [18/17] Loss: [----- 1.3578 = 1.2269 + 1.00 * 0.1309 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.16794716121e-07, mean: 4.58027962225e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_104.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_105.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 39 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_107.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_108.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 34 \%
Epoch [19/30], Iter [1/17] Loss: [----- 1.4269 = 1.2415 + 1.00 * 0.1854 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [2/17] Loss: [----- 18.6143 = 1.2203 + 1.00 * 17.3940 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [3/17] Loss: [----- 5.2065 = 1.1995 + 1.00 * 4.0070 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [4/17] Loss: [----- 12.1470 = 1.2190 + 1.00 * 10.9279 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [5/17] Loss: [----- 5.5058 = 1.2419 + 1.00 * 4.2639 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [6/17] Loss: [----- 1.8959 = 1.1660 + 1.00 * 0.7299 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [7/17] Loss: [----- 10.7472 = 1.2538 + 1.00 * 9.4934 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [8/17] Loss: [----- 2.0688 = 1.2395 + 1.00 * 0.8293 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [9/17] Loss: [----- 1.3927 = 1.2007 + 1.00 * 0.1920 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [10/17] Loss: [----- 2.8679 = 1.1739 + 1.00 * 1.6940 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [11/17] Loss: [----- 1.6162 = 1.1907 + 1.00 * 0.4255 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [12/17] Loss: [----- 4.1258 = 1.1778 + 1.00 * 2.9479 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [13/17] Loss: [----- 1.1785 = 1.1782 + 1.00 * 0.0002 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [14/17] Loss: [----- 1.7104 = 1.2278 + 1.00 * 0.4827 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [15/17] Loss: [----- 1.3981 = 1.2901 + 1.00 * 0.1080 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [16/17] Loss: [----- 1.6512 = 1.1931 + 1.00 * 0.4581 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [17/17] Loss: [----- 1.3982 = 1.2359 + 1.00 * 0.1624 + 0.00 * 0.0000 -----]
Epoch [19/30], Iter [18/17] Loss: [----- 1.3926 = 1.2846 + 1.00 * 0.1081 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 2.26343459531e-07, mean: 9.33787305257e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_110.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_111.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 44 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_113.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_114.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 36 \%
Epoch [20/30], Iter [1/17] Loss: [----- 1.5116 = 1.2115 + 1.00 * 0.3001 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [2/17] Loss: [----- 1.3885 = 1.1512 + 1.00 * 0.2374 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [3/17] Loss: [----- 1.3031 = 1.2450 + 1.00 * 0.0581 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [4/17] Loss: [----- 1.2634 = 1.1984 + 1.00 * 0.0650 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [5/17] Loss: [----- 1.2089 = 1.1873 + 1.00 * 0.0216 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [6/17] Loss: [----- 2.7579 = 1.2503 + 1.00 * 1.5076 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [7/17] Loss: [----- 1.6852 = 1.1858 + 1.00 * 0.4994 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [8/17] Loss: [----- 2.1162 = 1.2081 + 1.00 * 0.9081 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [9/17] Loss: [----- 1.4129 = 1.2741 + 1.00 * 0.1388 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [10/17] Loss: [----- 2.5993 = 1.2492 + 1.00 * 1.3502 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [11/17] Loss: [----- 1.5317 = 1.1984 + 1.00 * 0.3334 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [12/17] Loss: [----- 1.3713 = 1.2591 + 1.00 * 0.1122 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [13/17] Loss: [----- 1.7888 = 1.2101 + 1.00 * 0.5787 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [14/17] Loss: [----- 1.3082 = 1.2578 + 1.00 * 0.0504 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [15/17] Loss: [----- 1.3321 = 1.1825 + 1.00 * 0.1496 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [16/17] Loss: [----- 2.6290 = 1.2022 + 1.00 * 1.4268 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [17/17] Loss: [----- 1.4531 = 1.2640 + 1.00 * 0.1891 + 0.00 * 0.0000 -----]
Epoch [20/30], Iter [18/17] Loss: [----- 1.4803 = 1.2167 + 1.00 * 0.2635 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.34002263508e-07, mean: 3.49536755095e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_116.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_117.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 37 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_119.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_120.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 32 \%
Epoch [21/30], Iter [1/17] Loss: [----- 1.6946 = 1.1680 + 1.00 * 0.5266 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [2/17] Loss: [----- 1.1745 = 1.1533 + 1.00 * 0.0212 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [3/17] Loss: [----- 1.2179 = 1.1604 + 1.00 * 0.0575 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [4/17] Loss: [----- 1.4090 = 1.1929 + 1.00 * 0.2161 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [5/17] Loss: [----- 1.4570 = 1.2615 + 1.00 * 0.1955 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [6/17] Loss: [----- 1.4132 = 1.2026 + 1.00 * 0.2106 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [7/17] Loss: [----- 3.6336 = 1.1336 + 1.00 * 2.5000 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [8/17] Loss: [----- 2.1131 = 1.2253 + 1.00 * 0.8878 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [9/17] Loss: [----- 1.1571 = 1.1226 + 1.00 * 0.0345 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [10/17] Loss: [----- 1.5245 = 1.2769 + 1.00 * 0.2476 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [11/17] Loss: [----- 1.4105 = 1.2329 + 1.00 * 0.1776 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [12/17] Loss: [----- 1.6852 = 1.2256 + 1.00 * 0.4595 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [13/17] Loss: [----- 1.2237 = 1.1643 + 1.00 * 0.0593 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [14/17] Loss: [----- 1.2745 = 1.2236 + 1.00 * 0.0509 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [15/17] Loss: [----- 1.3028 = 1.2222 + 1.00 * 0.0806 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [16/17] Loss: [----- 1.4097 = 1.1386 + 1.00 * 0.2711 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [17/17] Loss: [----- 2.7928 = 1.2290 + 1.00 * 1.5638 + 0.00 * 0.0000 -----]
Epoch [21/30], Iter [18/17] Loss: [----- 1.3531 = 1.1942 + 1.00 * 0.1589 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.79105086318e-07, mean: 4.21526635819e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_122.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_123.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 45 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_125.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_126.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 37 \%
Epoch [22/30], Iter [1/17] Loss: [----- 1.2326 = 1.1974 + 1.00 * 0.0352 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [2/17] Loss: [----- 1.3769 = 1.2435 + 1.00 * 0.1335 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [3/17] Loss: [----- 1.1669 = 1.0871 + 1.00 * 0.0798 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [4/17] Loss: [----- 1.1647 = 1.1449 + 1.00 * 0.0199 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [5/17] Loss: [----- 1.1591 = 1.1570 + 1.00 * 0.0020 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [6/17] Loss: [----- 1.3349 = 1.3177 + 1.00 * 0.0172 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [7/17] Loss: [----- 1.2273 = 1.2109 + 1.00 * 0.0164 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [8/17] Loss: [----- 1.3117 = 1.1903 + 1.00 * 0.1214 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [9/17] Loss: [----- 1.3076 = 1.2369 + 1.00 * 0.0708 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [10/17] Loss: [----- 1.4646 = 1.1856 + 1.00 * 0.2791 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [11/17] Loss: [----- 4.6591 = 1.1828 + 1.00 * 3.4763 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [12/17] Loss: [----- 1.7195 = 1.2756 + 1.00 * 0.4439 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [13/17] Loss: [----- 1.4544 = 1.3128 + 1.00 * 0.1416 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [14/17] Loss: [----- 1.4238 = 1.3556 + 1.00 * 0.0682 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [15/17] Loss: [----- 1.2484 = 1.2472 + 1.00 * 0.0012 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [16/17] Loss: [----- 1.2705 = 1.2629 + 1.00 * 0.0076 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [17/17] Loss: [----- 1.2467 = 1.2310 + 1.00 * 0.0157 + 0.00 * 0.0000 -----]
Epoch [22/30], Iter [18/17] Loss: [----- 1.3226 = 1.2763 + 1.00 * 0.0463 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 9.54215977345e-08, mean: 6.98422724099e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_128.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_129.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 38 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_131.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_132.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 35 \%
Epoch [23/30], Iter [1/17] Loss: [----- 1.2880 = 1.2357 + 1.00 * 0.0524 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [2/17] Loss: [----- 1.2498 = 1.2385 + 1.00 * 0.0113 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [3/17] Loss: [----- 1.1966 = 1.1717 + 1.00 * 0.0248 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [4/17] Loss: [----- 1.2538 = 1.1418 + 1.00 * 0.1120 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [5/17] Loss: [----- 3.7023 = 1.2601 + 1.00 * 2.4422 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [6/17] Loss: [----- 1.3246 = 1.2855 + 1.00 * 0.0391 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [7/17] Loss: [----- 2.8456 = 1.2643 + 1.00 * 1.5814 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [8/17] Loss: [----- 1.2921 = 1.1599 + 1.00 * 0.1322 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [9/17] Loss: [----- 1.3418 = 1.2310 + 1.00 * 0.1108 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [10/17] Loss: [----- 1.2490 = 1.1850 + 1.00 * 0.0640 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [11/17] Loss: [----- 1.6362 = 1.2260 + 1.00 * 0.4102 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [12/17] Loss: [----- 1.3788 = 1.2417 + 1.00 * 0.1371 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [13/17] Loss: [----- 2.3579 = 1.1995 + 1.00 * 1.1584 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [14/17] Loss: [----- 1.6195 = 1.2969 + 1.00 * 0.3226 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [15/17] Loss: [----- 1.3148 = 1.2108 + 1.00 * 0.1040 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [16/17] Loss: [----- 1.5677 = 1.2181 + 1.00 * 0.3496 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [17/17] Loss: [----- 1.3356 = 1.2313 + 1.00 * 0.1043 + 0.00 * 0.0000 -----]
Epoch [23/30], Iter [18/17] Loss: [----- 1.3036 = 1.1615 + 1.00 * 0.1422 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 2.25612254212e-07, mean: 6.19780564648e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_134.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_135.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 36 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_137.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_138.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 32 \%
Epoch [24/30], Iter [1/17] Loss: [----- 1.2006 = 1.1716 + 1.00 * 0.0289 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [2/17] Loss: [----- 2.5227 = 1.2104 + 1.00 * 1.3123 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [3/17] Loss: [----- 2.2105 = 1.1558 + 1.00 * 1.0548 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [4/17] Loss: [----- 1.2189 = 1.1785 + 1.00 * 0.0405 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [5/17] Loss: [----- 1.9571 = 1.1717 + 1.00 * 0.7854 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [6/17] Loss: [----- 2.4697 = 1.1935 + 1.00 * 1.2762 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [7/17] Loss: [----- 1.2059 = 1.1937 + 1.00 * 0.0122 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [8/17] Loss: [----- 1.2325 = 1.2296 + 1.00 * 0.0029 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [9/17] Loss: [----- 1.2355 = 1.2103 + 1.00 * 0.0252 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [10/17] Loss: [----- 1.3259 = 1.1034 + 1.00 * 0.2225 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [11/17] Loss: [----- 1.1648 = 1.1596 + 1.00 * 0.0053 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [12/17] Loss: [----- 1.2094 = 1.1278 + 1.00 * 0.0816 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [13/17] Loss: [----- 2.0346 = 1.2667 + 1.00 * 0.7680 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [14/17] Loss: [----- 1.2641 = 1.2095 + 1.00 * 0.0546 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [15/17] Loss: [----- 1.2213 = 1.1188 + 1.00 * 0.1026 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [16/17] Loss: [----- 1.5365 = 1.1838 + 1.00 * 0.3527 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [17/17] Loss: [----- 1.9541 = 1.1532 + 1.00 * 0.8009 + 0.00 * 0.0000 -----]
Epoch [24/30], Iter [18/17] Loss: [----- 1.3231 = 1.1315 + 1.00 * 0.1916 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.39356856721e-07, mean: 4.55780053699e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_140.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_141.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 37 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_143.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_144.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 33 \%
Epoch [25/30], Iter [1/17] Loss: [----- 1.2177 = 1.1498 + 1.00 * 0.0679 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [2/17] Loss: [----- 1.4594 = 1.1337 + 1.00 * 0.3257 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [3/17] Loss: [----- 1.8781 = 1.2055 + 1.00 * 0.6726 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [4/17] Loss: [----- 1.2277 = 1.1407 + 1.00 * 0.0870 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [5/17] Loss: [----- 1.3328 = 1.2094 + 1.00 * 0.1234 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [6/17] Loss: [----- 1.4879 = 1.2432 + 1.00 * 0.2446 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [7/17] Loss: [----- 1.3167 = 1.1974 + 1.00 * 0.1193 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [8/17] Loss: [----- 1.1508 = 1.1294 + 1.00 * 0.0214 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [9/17] Loss: [----- 1.3463 = 1.0919 + 1.00 * 0.2543 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [10/17] Loss: [----- 1.1658 = 1.1474 + 1.00 * 0.0184 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [11/17] Loss: [----- 1.1893 = 1.1299 + 1.00 * 0.0594 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [12/17] Loss: [----- 1.1908 = 1.1236 + 1.00 * 0.0672 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [13/17] Loss: [----- 1.6517 = 1.1667 + 1.00 * 0.4850 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [14/17] Loss: [----- 1.2688 = 1.1208 + 1.00 * 0.1480 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [15/17] Loss: [----- 1.1868 = 1.1344 + 1.00 * 0.0524 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [16/17] Loss: [----- 1.4619 = 1.1136 + 1.00 * 0.3484 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [17/17] Loss: [----- 1.3280 = 1.1905 + 1.00 * 0.1375 + 0.00 * 0.0000 -----]
Epoch [25/30], Iter [18/17] Loss: [----- 1.4128 = 1.0919 + 1.00 * 0.3209 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 2.57070666976e-07, mean: 4.4542655786e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_146.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_147.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 47 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_149.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_150.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 38 \%
Epoch [26/30], Iter [1/17] Loss: [----- 1.1138 = 1.1024 + 1.00 * 0.0114 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [2/17] Loss: [----- 1.3852 = 1.0996 + 1.00 * 0.2856 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [3/17] Loss: [----- 1.1554 = 1.1197 + 1.00 * 0.0357 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [4/17] Loss: [----- 1.2148 = 1.0737 + 1.00 * 0.1411 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [5/17] Loss: [----- 1.5357 = 1.0692 + 1.00 * 0.4665 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [6/17] Loss: [----- 3.9359 = 1.1004 + 1.00 * 2.8355 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [7/17] Loss: [----- 1.3735 = 1.1311 + 1.00 * 0.2425 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [8/17] Loss: [----- 1.1316 = 1.1146 + 1.00 * 0.0170 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [9/17] Loss: [----- 1.2589 = 1.2467 + 1.00 * 0.0122 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [10/17] Loss: [----- 1.1233 = 1.0702 + 1.00 * 0.0531 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [11/17] Loss: [----- 1.1725 = 1.1123 + 1.00 * 0.0602 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [12/17] Loss: [----- 1.2406 = 1.2034 + 1.00 * 0.0372 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [13/17] Loss: [----- 1.1222 = 1.1055 + 1.00 * 0.0167 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [14/17] Loss: [----- 1.1283 = 1.0932 + 1.00 * 0.0350 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [15/17] Loss: [----- 1.1528 = 1.1074 + 1.00 * 0.0454 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [16/17] Loss: [----- 1.3738 = 1.0816 + 1.00 * 0.2922 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [17/17] Loss: [----- 1.5851 = 1.1719 + 1.00 * 0.4132 + 0.00 * 0.0000 -----]
Epoch [26/30], Iter [18/17] Loss: [----- 2.3955 = 1.2193 + 1.00 * 1.1762 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 1.25349316704e-07, mean: 4.81536019947e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_152.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_153.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 42 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_155.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_156.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 32 \%
Epoch [27/30], Iter [1/17] Loss: [----- 1.9686 = 1.1955 + 1.00 * 0.7731 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [2/17] Loss: [----- 1.5579 = 1.1919 + 1.00 * 0.3660 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [3/17] Loss: [----- 2.0188 = 1.3077 + 1.00 * 0.7111 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [4/17] Loss: [----- 1.3438 = 1.2404 + 1.00 * 0.1035 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [5/17] Loss: [----- 2.6539 = 1.3062 + 1.00 * 1.3477 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [6/17] Loss: [----- 1.6003 = 1.1538 + 1.00 * 0.4465 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [7/17] Loss: [----- 1.1740 = 1.1349 + 1.00 * 0.0391 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [8/17] Loss: [----- 1.2933 = 1.1688 + 1.00 * 0.1245 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [9/17] Loss: [----- 2.4702 = 1.2217 + 1.00 * 1.2486 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [10/17] Loss: [----- 1.1967 = 1.1679 + 1.00 * 0.0287 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [11/17] Loss: [----- 1.9566 = 1.1298 + 1.00 * 0.8268 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [12/17] Loss: [----- 1.2035 = 1.1182 + 1.00 * 0.0853 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [13/17] Loss: [----- 1.0716 = 1.0675 + 1.00 * 0.0041 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [14/17] Loss: [----- 1.3112 = 1.0900 + 1.00 * 0.2212 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [15/17] Loss: [----- 1.5528 = 1.1520 + 1.00 * 0.4008 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [16/17] Loss: [----- 1.1504 = 1.0926 + 1.00 * 0.0577 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [17/17] Loss: [----- 1.7518 = 1.0905 + 1.00 * 0.6613 + 0.00 * 0.0000 -----]
Epoch [27/30], Iter [18/17] Loss: [----- 1.3688 = 1.2471 + 1.00 * 0.1217 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 7.5342393302e-07, mean: 2.3224599488e-07

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_158.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_159.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 34 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_161.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_162.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 31 \%
Epoch [28/30], Iter [1/17] Loss: [----- 1.1233 = 1.1054 + 1.00 * 0.0179 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [2/17] Loss: [----- 1.3149 = 1.1173 + 1.00 * 0.1976 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [3/17] Loss: [----- 1.3402 = 1.1621 + 1.00 * 0.1781 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [4/17] Loss: [----- 1.2423 = 1.1662 + 1.00 * 0.0761 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [5/17] Loss: [----- 1.2474 = 1.1347 + 1.00 * 0.1127 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [6/17] Loss: [----- 1.3442 = 1.1035 + 1.00 * 0.2407 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [7/17] Loss: [----- 1.0355 = 1.0066 + 1.00 * 0.0288 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [8/17] Loss: [----- 1.1353 = 1.1262 + 1.00 * 0.0091 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [9/17] Loss: [----- 1.0923 = 1.0766 + 1.00 * 0.0156 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [10/17] Loss: [----- 3.3449 = 1.1401 + 1.00 * 2.2048 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [11/17] Loss: [----- 1.1978 = 1.1541 + 1.00 * 0.0437 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [12/17] Loss: [----- 2.7782 = 1.0894 + 1.00 * 1.6888 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [13/17] Loss: [----- 0.9887 = 0.9661 + 1.00 * 0.0226 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [14/17] Loss: [----- 1.5517 = 1.0835 + 1.00 * 0.4682 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [15/17] Loss: [----- 1.4172 = 1.0983 + 1.00 * 0.3188 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [16/17] Loss: [----- 1.3329 = 1.2169 + 1.00 * 0.1159 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [17/17] Loss: [----- 1.5369 = 1.1564 + 1.00 * 0.3805 + 0.00 * 0.0000 -----]
Epoch [28/30], Iter [18/17] Loss: [----- 1.5500 = 1.1836 + 1.00 * 0.3664 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 9.16788067684e-08, mean: 5.9651540596e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_164.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_165.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 45 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_167.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_168.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 40 \%
Epoch [29/30], Iter [1/17] Loss: [----- 1.1826 = 1.1169 + 1.00 * 0.0657 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [2/17] Loss: [----- 2.2844 = 1.1602 + 1.00 * 1.1242 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [3/17] Loss: [----- 1.2232 = 1.1215 + 1.00 * 0.1017 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [4/17] Loss: [----- 1.3650 = 1.1739 + 1.00 * 0.1911 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [5/17] Loss: [----- 1.3370 = 1.1177 + 1.00 * 0.2193 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [6/17] Loss: [----- 1.5247 = 1.1857 + 1.00 * 0.3391 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [7/17] Loss: [----- 1.2456 = 1.1167 + 1.00 * 0.1289 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [8/17] Loss: [----- 1.2658 = 1.0808 + 1.00 * 0.1850 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [9/17] Loss: [----- 1.3624 = 1.0640 + 1.00 * 0.2985 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [10/17] Loss: [----- 1.3557 = 1.1114 + 1.00 * 0.2444 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [11/17] Loss: [----- 1.4440 = 1.1252 + 1.00 * 0.3188 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [12/17] Loss: [----- 1.0539 = 1.0485 + 1.00 * 0.0054 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [13/17] Loss: [----- 1.8001 = 1.0471 + 1.00 * 0.7530 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [14/17] Loss: [----- 1.0845 = 1.0556 + 1.00 * 0.0289 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [15/17] Loss: [----- 1.3038 = 1.1297 + 1.00 * 0.1741 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [16/17] Loss: [----- 1.1912 = 1.1082 + 1.00 * 0.0830 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [17/17] Loss: [----- 1.3685 = 1.0917 + 1.00 * 0.2768 + 0.00 * 0.0000 -----]
Epoch [29/30], Iter [18/17] Loss: [----- 1.6712 = 1.1740 + 1.00 * 0.4972 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 9.72191642745e-08, mean: 3.04333696022e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_170.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_171.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 49 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_173.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_174.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 45 \%
Epoch [30/30], Iter [1/17] Loss: [----- 1.2513 = 1.1942 + 1.00 * 0.0571 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [2/17] Loss: [----- 1.3579 = 1.1294 + 1.00 * 0.2285 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [3/17] Loss: [----- 1.1738 = 1.1472 + 1.00 * 0.0267 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [4/17] Loss: [----- 1.1527 = 1.0691 + 1.00 * 0.0836 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [5/17] Loss: [----- 2.5688 = 1.0324 + 1.00 * 1.5365 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [6/17] Loss: [----- 1.1532 = 1.1344 + 1.00 * 0.0188 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [7/17] Loss: [----- 1.2957 = 1.1085 + 1.00 * 0.1872 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [8/17] Loss: [----- 1.0307 = 0.9998 + 1.00 * 0.0309 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [9/17] Loss: [----- 1.6386 = 1.0086 + 1.00 * 0.6300 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [10/17] Loss: [----- 2.0722 = 1.0213 + 1.00 * 1.0509 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [11/17] Loss: [----- 1.0828 = 1.0656 + 1.00 * 0.0172 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [12/17] Loss: [----- 1.2103 = 1.0733 + 1.00 * 0.1370 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [13/17] Loss: [----- 2.1652 = 1.0470 + 1.00 * 1.1182 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [14/17] Loss: [----- 1.2847 = 1.0556 + 1.00 * 0.2290 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [15/17] Loss: [----- 1.2015 = 1.1665 + 1.00 * 0.0350 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [16/17] Loss: [----- 1.1688 = 1.0056 + 1.00 * 0.1632 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [17/17] Loss: [----- 1.0362 = 1.0262 + 1.00 * 0.0100 + 0.00 * 0.0000 -----]
Epoch [30/30], Iter [18/17] Loss: [----- 1.8258 = 1.0424 + 1.00 * 0.7834 + 0.00 * 0.0000 -----]
===========
 SMALL gradient:layer1.5.bias
---------
max: 8.17676379938e-08, mean: 2.91769683969e-08

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_176.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_177.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([1775, 4])
Train Accuracy: 57 \%

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_179.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_180.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
predicted set [0 1 2 3]
label set [0 1 2 3]
validation Accuracy: 46 \%

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}134}]:} \PY{k+kn}{import} \PY{n+nn}{pickle}
          \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{net}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mar16\PYZus{}2\PYZus{}53\PYZus{}params}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
