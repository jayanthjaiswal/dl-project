{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Human Activity Recognition\n",
    "\n",
    "Human activity recognition using smartwatch dataset and an LSTM RNN. Classifying the type of movement amongst six categories:\n",
    "- WALKING,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "Compared to a classical approach, using a Recurrent Neural Networks (RNN) with Long Short-Term Memory cells (LSTMs) require no or almost no feature engineering. Data can be fed directly into the neural network who acts like a black box, modeling the problem correctly. Other research on the activity recognition dataset used mostly use a big amount of feature engineering, which is rather a signal processing approach combined with classical data science techniques. The approach here is rather very simple in terms of how much did the data was preprocessed. \n",
    "\n",
    "## Details about input data\n",
    "\n",
    "I will be using an LSTM on the data to learn (as a smartwatch attached on the wrist) to recognise the type of activity that the user is doing. A similar dataset's description based on prior research goes like this:\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. \n",
    "\n",
    "That said, I will use the almost raw data. \n",
    "\n",
    "## What is an RNN?\n",
    "\n",
    "As explained in [this article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), an RNN takes many input vectors to process them and output other vectors. It can be roughly pictured like in the image below, imagining each rectangle has a vectorial depth and other special hidden quirks in the image below. **In our case, the \"many to one\" architecture is used**: we accept time series of feature vectors (one vector per time step) to convert them to a probability vector at the output for classification. Note that a \"one to one\" architecture would be a standard feedforward neural network. \n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" />\n",
    "\n",
    "An LSTM is an improved RNN. It is more complex, but easier to train, avoiding what is called the vanishing gradient problem. \n",
    "\n",
    "\n",
    "## Results \n",
    "\n",
    "Scroll on! Nice visuals awaits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Includes\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../project_datasets/A07T_slice.mat', '../project_datasets/A08T_slice.mat', '../project_datasets/A09T_slice.mat', '../project_datasets/A03T_slice.mat', '../project_datasets/A04T_slice.mat', '../project_datasets/A01T_slice.mat', '../project_datasets/A05T_slice.mat', '../project_datasets/A06T_slice.mat', '../project_datasets/A02T_slice.mat']\n",
      "[u'image', u'type']\n",
      "(288, 25, 1000)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "\n",
    "mat_names = glob.glob('../project_datasets/*.mat')\n",
    "# each test subject got a different file - 9 test subjects\n",
    "print(mat_names)\n",
    "\n",
    "matfile = h5py.File(mat_names[0], 'r')\n",
    "print(matfile.keys()) #image and type\n",
    "\n",
    "image_mat = matfile['image']\n",
    "image_shape = image_mat.shape # 288 (48x6) trials across 25 electrodes for 1000 time points (250Hz*4s)\n",
    "print image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "0\n",
      "288\n",
      "[0.0, 769.0, 770.0, 771.0, 772.0]\n"
     ]
    }
   ],
   "source": [
    "type_mat = matfile['type']\n",
    "type_shape = type_mat.shape\n",
    "print type_shape\n",
    "# plt.plot(type_mat[0,:288]) # gets the significant values of types\n",
    "# all the 0's occur after 288, and are meaningless I think\n",
    "# so the image_mat, which has shape (288, 25, 1000) should correspond\n",
    "# to the first 288 entries of type_mat, so\n",
    "# for a single subject, training data should be image_mat, with 288 samples, each sample has shape (25, 1000)\n",
    "# and our target label matrix should be type_mat[:288] (or 287?)\n",
    "\n",
    "nans = np.sum(np.isnan(image_mat[:,:]))\n",
    "print(nans) #No NaN in the data\n",
    "print len(image_mat[0:,:])\n",
    "count = 0\n",
    "# for i in range(len(image_mat[0:,:])):\n",
    "#  if np.sum(np.isnan(image_mat[i:,:])):\n",
    "#         pass\n",
    "\n",
    "type_set = list(set(type_mat[0,:]))\n",
    "print(type_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_channels = 22 #from project guidelines\n",
    "test_count = 50 #from project guideline, 238 for train-validation and 50 for test\n",
    "validation_count = 38 # 38 points in validation set and remaining 200 points in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting seed\n",
    "np.random.seed(seed=1337)\n",
    "test_picked = np.random.choice(image_shape[0], test_count, replace=False)\n",
    "train_val_picked = np.setdiff1d(np.arange(image_shape[0]), test_picked)\n",
    "val_picked = train_val_picked[:validation_count]\n",
    "train_picked = train_val_picked[validation_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_data_X = []\n",
    "training_data_X = []\n",
    "validation_data_X = []\n",
    "test_data_X = []\n",
    "\n",
    "trainval_data_Y = []\n",
    "training_data_Y = []\n",
    "validation_data_Y = []\n",
    "test_data_Y = []\n",
    "\n",
    "for i in range(len(mat_names)):\n",
    "    matfile = h5py.File(mat_names[i], 'r')\n",
    "    \n",
    "    trainval_data_X.append(matfile['image'][sorted(train_val_picked),:EEG_channels,:]) #(238, 22, 1000) x 9\n",
    "    training_data_X.append(matfile['image'][sorted(train_picked),:EEG_channels,:]) #(200, 22, 1000) x 9\n",
    "    validation_data_X.append(matfile['image'][sorted(val_picked),:EEG_channels,:]) #(38, 22, 1000) x 9\n",
    "    test_data_X.append(matfile['image'][sorted(test_picked),:EEG_channels,:]) #(50, 22, 1000) x 9\n",
    "    \n",
    "    trainval_data_Y.append(matfile['type'][0,sorted(train_val_picked)] - type_set[1]) #(238, ) x 9\n",
    "    training_data_Y.append(matfile['type'][0,sorted(train_picked)] - type_set[1]) #(200, ) x 9\n",
    "    validation_data_Y.append(matfile['type'][0,sorted(val_picked)] - type_set[1]) #(38, ) x 9\n",
    "    test_data_Y.append(matfile['type'][0,sorted(test_picked)] - type_set[1]) #(50, ) x 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "training_data_shape = training_data_X[0].shape\n",
    "print(training_data_shape) #(200, 22, 1000) while test data shape is (50, 22, 1000) and validation data is (38, 22,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before nans: (2142, 22, 1000)\n",
      "before nans: (2115, 22, 1000)\n",
      "before nans: (1800, 22, 1000)\n",
      "before nans: (1775, 22, 1000)\n",
      "before nans: (342, 22, 1000)\n",
      "before nans: (340, 22, 1000)\n",
      "before nans: (450, 22, 1000)\n",
      "before nans: (443, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "rnn_trainval_data_X = np.concatenate(trainval_data_X, axis=0) #(2142, 22, 1000)\n",
    "rnn_training_data_X = np.concatenate(training_data_X, axis=0) #(1800, 22, 1000)\n",
    "rnn_validation_data_X = np.concatenate(validation_data_X, axis=0) #(342, 22, 1000)\n",
    "rnn_test_data_X = np.concatenate(test_data_X, axis=0) #(450, 22, 1000)\n",
    "\n",
    "rnn_trainval_data_Y = np.concatenate(trainval_data_Y, axis=0) #(2142, )\n",
    "rnn_training_data_Y = np.concatenate(training_data_Y, axis=0) #(1800, )\n",
    "rnn_validation_data_Y = np.concatenate(validation_data_Y, axis=0) #(342, )\n",
    "rnn_test_data_Y = np.concatenate(test_data_Y, axis=0) #(450,)\n",
    "\n",
    "def remove_nan_rows_A(A, b, debug=True):\n",
    "    if (debug):\n",
    "        print('before nans: {}'.format(str(A.shape)))\n",
    "    if (np.isnan(A).any() or np.isnan(b).any()):\n",
    "        mask = ~np.isnan(np.sum(A,axis=(1,2))) & ~np.isnan(b[:])\n",
    "        A = A[mask, :, :]\n",
    "        b = b[mask]\n",
    "    \n",
    "    if (debug):\n",
    "        print('before nans: {}'.format(str(A.shape)))\n",
    "    assert A.shape[0] == b.shape[0]\n",
    "    return A, b\n",
    "\n",
    "rnn_trainval_data_X, rnn_trainval_data_Y = remove_nan_rows_A(rnn_trainval_data_X,\n",
    "                                                             rnn_trainval_data_Y)\n",
    "rnn_training_data_X, rnn_training_data_Y = remove_nan_rows_A(rnn_training_data_X, \n",
    "                                                             rnn_training_data_Y)\n",
    "rnn_validation_data_X, rnn_validation_data_Y = remove_nan_rows_A(rnn_validation_data_X,\n",
    "                                         rnn_validation_data_Y)\n",
    "rnn_test_data_X, rnn_test_data_Y = remove_nan_rows_A(rnn_test_data_X,\n",
    "                                   rnn_test_data_Y)\n",
    "\n",
    "new_axis = (0,2,1)\n",
    "rnn_trainval_data_X = np.transpose(rnn_trainval_data_X, new_axis)\n",
    "rnn_training_data_X = np.transpose(rnn_training_data_X, new_axis)\n",
    "rnn_validation_data_X = np.transpose(rnn_validation_data_X, new_axis)\n",
    "rnn_test_data_X = np.transpose(rnn_test_data_X, new_axis)\n",
    "\n",
    "# repeating the Y labels for the rnn\n",
    "N_trainval, T, E = rnn_trainval_data_X.shape\n",
    "N_training, _, _ = rnn_trainval_data_X.shape\n",
    "N_validation, _, _ = rnn_test_data_X.shape\n",
    "N_test, _, _ = rnn_test_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "training_data_shape = training_data_X[0].shape\n",
    "print(training_data_shape) #(200, 22, 1000) while test data shape is (50, 22, 1000) and validation data is (38, 22,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24211577 0.29200906 0.28058568 0.28058862 0.27744969 0.30503537\n",
      " 0.37432564 0.35469766 0.29599771 0.28391495 0.28070525 0.32476685\n",
      " 0.33706837 0.3755972  0.37135214 0.37447009 0.3089289  0.33019716\n",
      " 0.29176846 0.34339516 0.36118418 0.32799225]\n",
      "[11.27145177 10.2505745  10.92481975 11.49970902 11.21479773 10.89149331\n",
      "  8.71352886 10.06750202 10.70168511 11.33973592 10.97697775 10.88472781\n",
      " 10.10025118 10.33649046 10.64618123 11.09796571 10.98326707 11.08252466\n",
      " 10.81161869 11.24811363 11.16262363 11.35832584]\n"
     ]
    }
   ],
   "source": [
    "mean_list = np.mean(rnn_trainval_data_X.reshape(-1, rnn_trainval_data_X.shape[-1]), axis=0)\n",
    "std_list = np.sqrt((np.var(rnn_trainval_data_X.reshape(-1, rnn_trainval_data_X.shape[-1]), axis=0)))\n",
    "print(mean_list)\n",
    "print(std_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_trainval_data_X = (rnn_trainval_data_X - mean_list)/std_list\n",
    "rnn_training_data_X = (rnn_training_data_X - mean_list)/std_list\n",
    "rnn_validation_data_X = (rnn_validation_data_X - mean_list)/std_list\n",
    "rnn_test_data_X = (rnn_test_data_X - mean_list)/std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115, 1000, 22)\n",
      "(2115,)\n"
     ]
    }
   ],
   "source": [
    "print rnn_trainval_data_X.shape\n",
    "print rnn_trainval_data_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 1.0, 2.0, 3.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(rnn_trainval_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"LEFT\",\n",
    "    \"RIGHT\",\n",
    "    \"FOOT\",\n",
    "    \"TONGUE\"\n",
    "    \n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.019228</td>\n",
       "      <td>-0.082219</td>\n",
       "      <td>-0.075826</td>\n",
       "      <td>-0.047379</td>\n",
       "      <td>-0.050532</td>\n",
       "      <td>-0.060178</td>\n",
       "      <td>-0.063592</td>\n",
       "      <td>-0.079086</td>\n",
       "      <td>-0.051713</td>\n",
       "      <td>-0.044552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049065</td>\n",
       "      <td>-0.060679</td>\n",
       "      <td>-0.061354</td>\n",
       "      <td>-0.056502</td>\n",
       "      <td>-0.046323</td>\n",
       "      <td>-0.063935</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>-0.086984</td>\n",
       "      <td>-0.081453</td>\n",
       "      <td>-0.055814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.656299</td>\n",
       "      <td>0.594934</td>\n",
       "      <td>0.653857</td>\n",
       "      <td>0.670126</td>\n",
       "      <td>0.748175</td>\n",
       "      <td>0.684128</td>\n",
       "      <td>0.582003</td>\n",
       "      <td>0.595916</td>\n",
       "      <td>0.659181</td>\n",
       "      <td>0.691749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.646819</td>\n",
       "      <td>0.665252</td>\n",
       "      <td>0.665526</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.651515</td>\n",
       "      <td>0.680298</td>\n",
       "      <td>0.660292</td>\n",
       "      <td>0.651054</td>\n",
       "      <td>0.725231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.658983</td>\n",
       "      <td>-1.633771</td>\n",
       "      <td>-1.791123</td>\n",
       "      <td>-1.926622</td>\n",
       "      <td>-1.992703</td>\n",
       "      <td>-2.157500</td>\n",
       "      <td>-2.228408</td>\n",
       "      <td>-1.994662</td>\n",
       "      <td>-1.975914</td>\n",
       "      <td>-1.803387</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.368362</td>\n",
       "      <td>-2.147902</td>\n",
       "      <td>-2.208856</td>\n",
       "      <td>-1.837634</td>\n",
       "      <td>-2.104261</td>\n",
       "      <td>-2.179857</td>\n",
       "      <td>-2.172212</td>\n",
       "      <td>-2.166304</td>\n",
       "      <td>-2.175740</td>\n",
       "      <td>-2.100939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.472010</td>\n",
       "      <td>-0.485779</td>\n",
       "      <td>-0.526264</td>\n",
       "      <td>-0.492525</td>\n",
       "      <td>-0.566800</td>\n",
       "      <td>-0.494254</td>\n",
       "      <td>-0.452030</td>\n",
       "      <td>-0.482651</td>\n",
       "      <td>-0.497613</td>\n",
       "      <td>-0.529908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502304</td>\n",
       "      <td>-0.489827</td>\n",
       "      <td>-0.490086</td>\n",
       "      <td>-0.482515</td>\n",
       "      <td>-0.481587</td>\n",
       "      <td>-0.480294</td>\n",
       "      <td>-0.501194</td>\n",
       "      <td>-0.508040</td>\n",
       "      <td>-0.500401</td>\n",
       "      <td>-0.544743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.066967</td>\n",
       "      <td>-0.128520</td>\n",
       "      <td>-0.132951</td>\n",
       "      <td>-0.088090</td>\n",
       "      <td>-0.113995</td>\n",
       "      <td>-0.106462</td>\n",
       "      <td>-0.087789</td>\n",
       "      <td>-0.107983</td>\n",
       "      <td>-0.082411</td>\n",
       "      <td>-0.076708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069630</td>\n",
       "      <td>-0.090661</td>\n",
       "      <td>-0.096798</td>\n",
       "      <td>-0.068940</td>\n",
       "      <td>-0.061470</td>\n",
       "      <td>-0.093679</td>\n",
       "      <td>-0.106021</td>\n",
       "      <td>-0.113008</td>\n",
       "      <td>-0.106719</td>\n",
       "      <td>-0.101958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.390061</td>\n",
       "      <td>0.277565</td>\n",
       "      <td>0.307292</td>\n",
       "      <td>0.340759</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.339611</td>\n",
       "      <td>0.310075</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.347620</td>\n",
       "      <td>0.389409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359418</td>\n",
       "      <td>0.347477</td>\n",
       "      <td>0.336621</td>\n",
       "      <td>0.327036</td>\n",
       "      <td>0.345310</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.329798</td>\n",
       "      <td>0.286364</td>\n",
       "      <td>0.286964</td>\n",
       "      <td>0.393489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.006600</td>\n",
       "      <td>2.648573</td>\n",
       "      <td>2.950982</td>\n",
       "      <td>3.338457</td>\n",
       "      <td>3.754447</td>\n",
       "      <td>3.074328</td>\n",
       "      <td>1.839889</td>\n",
       "      <td>2.084250</td>\n",
       "      <td>2.691685</td>\n",
       "      <td>3.032174</td>\n",
       "      <td>...</td>\n",
       "      <td>2.644856</td>\n",
       "      <td>2.377555</td>\n",
       "      <td>2.643603</td>\n",
       "      <td>3.024075</td>\n",
       "      <td>2.692631</td>\n",
       "      <td>2.459520</td>\n",
       "      <td>2.529219</td>\n",
       "      <td>2.539346</td>\n",
       "      <td>2.535329</td>\n",
       "      <td>2.675121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.019228    -0.082219    -0.075826    -0.047379    -0.050532   \n",
       "std       0.656299     0.594934     0.653857     0.670126     0.748175   \n",
       "min      -1.658983    -1.633771    -1.791123    -1.926622    -1.992703   \n",
       "25%      -0.472010    -0.485779    -0.526264    -0.492525    -0.566800   \n",
       "50%      -0.066967    -0.128520    -0.132951    -0.088090    -0.113995   \n",
       "75%       0.390061     0.277565     0.307292     0.340759     0.368200   \n",
       "max       3.006600     2.648573     2.950982     3.338457     3.754447   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.060178    -0.063592    -0.079086    -0.051713    -0.044552   \n",
       "std       0.684128     0.582003     0.595916     0.659181     0.691749   \n",
       "min      -2.157500    -2.228408    -1.994662    -1.975914    -1.803387   \n",
       "25%      -0.494254    -0.452030    -0.482651    -0.497613    -0.529908   \n",
       "50%      -0.106462    -0.087789    -0.107983    -0.082411    -0.076708   \n",
       "75%       0.339611     0.310075     0.304273     0.347620     0.389409   \n",
       "max       3.074328     1.839889     2.084250     2.691685     3.032174   \n",
       "\n",
       "          ...                12           13           14           15  \\\n",
       "count     ...       1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      ...         -0.049065    -0.060679    -0.061354    -0.056502   \n",
       "std       ...          0.662745     0.646819     0.665252     0.665526   \n",
       "min       ...         -2.368362    -2.147902    -2.208856    -1.837634   \n",
       "25%       ...         -0.502304    -0.489827    -0.490086    -0.482515   \n",
       "50%       ...         -0.069630    -0.090661    -0.096798    -0.068940   \n",
       "75%       ...          0.359418     0.347477     0.336621     0.327036   \n",
       "max       ...          2.644856     2.377555     2.643603     3.024075   \n",
       "\n",
       "                16           17           18           19           20  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     -0.046323    -0.063935    -0.059752    -0.086984    -0.081453   \n",
       "std       0.664200     0.651515     0.680298     0.660292     0.651054   \n",
       "min      -2.104261    -2.179857    -2.172212    -2.166304    -2.175740   \n",
       "25%      -0.481587    -0.480294    -0.501194    -0.508040    -0.500401   \n",
       "50%      -0.061470    -0.093679    -0.106021    -0.113008    -0.106719   \n",
       "75%       0.345310     0.341400     0.329798     0.286364     0.286964   \n",
       "max       2.692631     2.459520     2.529219     2.539346     2.535329   \n",
       "\n",
       "                21  \n",
       "count  1000.000000  \n",
       "mean     -0.055814  \n",
       "std       0.725231  \n",
       "min      -2.100939  \n",
       "25%      -0.544743  \n",
       "50%      -0.101958  \n",
       "75%       0.393489  \n",
       "max       2.675121  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(rnn_trainval_data_X[3]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal Parameters:\n",
    "\n",
    "Here are some core parameter definitions for the training. \n",
    "\n",
    "The whole neural network's structure could be summarised by enumerating those parameters and the fact an LSTM is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "((443, 1000, 22), (443,), 0.0016580587469994378, 0.997292271181712)\n",
      "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "X_train = rnn_training_data_X\n",
    "X_test = rnn_test_data_X\n",
    "X_val = rnn_validation_data_X\n",
    "y_train = rnn_training_data_Y\n",
    "y_test = rnn_test_data_Y\n",
    "y_val = rnn_validation_data_Y\n",
    "\n",
    "training_data_count = len(X_train)  # 2115 training series \n",
    "test_data_count = len(X_test)  # 443 testing series\n",
    "n_steps = len(X_train[0])  # 1000 timesteps per series\n",
    "n_input = len(X_train[0][0])  # 22 input parameters per timestep\n",
    "\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = 4 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0025\n",
    "dropout = 1.0\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = training_data_count * 100  # Loop 10 times on the dataset\n",
    "batch_size = 100\n",
    "display_iter = 10000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"\n",
    "    define a class to store parameters,\n",
    "    the input should be feature mat of training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, X_test):\n",
    "        # Data shaping\n",
    "        self.train_count = len(X_train)  # 7352 training series\n",
    "        self.test_data_count = len(X_test)  # 2947 testing series\n",
    "        self.n_steps = len(X_train[0])  # 128 time_steps per series\n",
    "        self.n_classes = 4  # Final output classes\n",
    "\n",
    "        # Training\n",
    "        self.learning_rate = 0.0025\n",
    "        self.lambda_loss_amount = 0.005\n",
    "        self.training_epochs = 250\n",
    "        self.batch_size = 100\n",
    "        self.clip_gradients = 15.0\n",
    "        self.gradient_noise_scale = None\n",
    "        # Dropout is added on inputs and after each stacked layers (but not\n",
    "        # between residual layers).\n",
    "        self.keep_prob_for_dropout = 0.90  # **(1/3.0)\n",
    "\n",
    "        # Linear+relu structure\n",
    "        self.bias_mean = 0.3\n",
    "        # I would recommend between 0.1 and 1.0 or to change and use a xavier\n",
    "        # initializer\n",
    "        self.weights_stddev = 0.2\n",
    "\n",
    "        ########\n",
    "        # NOTE: I think that if any of the below parameters are changed,\n",
    "        # the best is to readjust every parameters in the \"Training\" section\n",
    "        # above to properly compare the architectures only once optimised.\n",
    "        ########\n",
    "\n",
    "        # LSTM structure\n",
    "        # Features count is of 9: three 3D sensors features over time\n",
    "        self.n_inputs = len(X_train[0][0])\n",
    "        self.n_hidden = 10  # nb of neurons inside the neural network\n",
    "        # Use bidir in every LSTM cell, or not:\n",
    "        self.use_bidirectionnal_cells = False\n",
    "\n",
    "        # High-level deep architecture\n",
    "        self.also_add_dropout_between_stacked_cells = False  # True\n",
    "        # NOTE: values of exactly 1 (int) for those 2 high-level parameters below totally disables them and result in only 1 starting LSTM.\n",
    "        # self.n_layers_in_highway = 1  # Number of residual connections to the LSTMs (highway-style), this is did for each stacked block (inside them).\n",
    "        # self.n_stacked_layers = 1  # Stack multiple blocks of residual\n",
    "        # layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    \"\"\"convert label from dense to one hot\n",
    "      argument:\n",
    "        label: ndarray dense label ,shape: [sample_num,1]\n",
    "      return:\n",
    "        one_hot_label: ndarray  one hot, shape: [sample_num,n_class]\n",
    "    \"\"\"\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "\n",
    "    y = y.reshape(len(y))\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[np.array(y, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "def batch_norm(input_tensor, config, i):\n",
    "    # Implementing batch normalisation: this is used out of the residual layers\n",
    "    # to normalise those output neurons by mean and standard deviation.\n",
    "\n",
    "    if config.n_layers_in_highway == 0:\n",
    "        # There is no residual layers, no need for batch_norm:\n",
    "        return input_tensor\n",
    "\n",
    "    with tf.variable_scope(\"batch_norm\") as scope:\n",
    "        if i != 0:\n",
    "            # Do not create extra variables for each time step\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        # Mean and variance normalisation simply crunched over all axes\n",
    "        axes = list(range(len(input_tensor.get_shape())))\n",
    "\n",
    "        mean, variance = tf.nn.moments(input_tensor, axes=axes, shift=None, name=None, keep_dims=False)\n",
    "        stdev = tf.sqrt(variance+0.001)\n",
    "\n",
    "        # Rescaling\n",
    "        bn = input_tensor - mean\n",
    "        bn /= stdev\n",
    "        # Learnable extra rescaling\n",
    "\n",
    "        # tf.get_variable(\"relu_fc_weights\", initializer=tf.random_normal(mean=0.0, stddev=0.0)\n",
    "        bn *= tf.get_variable(\"a_noreg\", initializer=tf.random_normal([1], mean=0.5, stddev=0.0))\n",
    "        bn += tf.get_variable(\"b_noreg\", initializer=tf.random_normal([1], mean=0.0, stddev=0.0))\n",
    "        # bn *= tf.Variable(0.5, name=(scope.name + \"/a_noreg\"))\n",
    "        # bn += tf.Variable(0.0, name=(scope.name + \"/b_noreg\"))\n",
    "\n",
    "    return bn\n",
    "\n",
    "def relu_fc(input_2D_tensor_list, features_len, new_features_len, config):\n",
    "    \"\"\"make a relu fully-connected layer, mainly change the shape of tensor\n",
    "       both input and output is a list of tensor\n",
    "        argument:\n",
    "            input_2D_tensor_list: list shape is [batch_size,feature_num]\n",
    "            features_len: int the initial features length of input_2D_tensor\n",
    "            new_feature_len: int the final features length of output_2D_tensor\n",
    "            config: Config used for weights initializers\n",
    "        return:\n",
    "            output_2D_tensor_list lit shape is [batch_size,new_feature_len]\n",
    "    \"\"\"\n",
    "\n",
    "    W = tf.get_variable(\n",
    "        \"relu_fc_weights\",\n",
    "        initializer=tf.random_normal(\n",
    "            [features_len, new_features_len],\n",
    "            mean=0.0,\n",
    "            stddev=float(config.weights_stddev)\n",
    "        )\n",
    "    )\n",
    "    b = tf.get_variable(\n",
    "        \"relu_fc_biases_noreg\",\n",
    "        initializer=tf.random_normal(\n",
    "            [new_features_len],\n",
    "            mean=float(config.bias_mean),\n",
    "            stddev=float(config.weights_stddev)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # intra-timestep multiplication:\n",
    "    output_2D_tensor_list = [\n",
    "        tf.nn.relu(tf.matmul(input_2D_tensor, W) + b)\n",
    "            for input_2D_tensor in input_2D_tensor_list\n",
    "    ]\n",
    "\n",
    "    return output_2D_tensor_list\n",
    "\n",
    "\n",
    "def single_LSTM_cell(input_hidden_tensor, n_outputs):\n",
    "    \"\"\" define the basic LSTM layer\n",
    "        argument:\n",
    "            input_hidden_tensor: list a list of tensor,\n",
    "                                 shape: time_steps*[batch_size,n_inputs]\n",
    "            n_outputs: int num of LSTM layer output\n",
    "        return:\n",
    "            outputs: list a time_steps list of tensor,\n",
    "                     shape: time_steps*[batch_size,n_outputs]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"lstm_cell\"):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_outputs, state_is_tuple=True, forget_bias=0.999)\n",
    "        outputs, _ = tf.nn.static_rnn(lstm_cell, input_hidden_tensor, dtype=tf.float32)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def bi_LSTM_cell(input_hidden_tensor, n_inputs, n_outputs, config):\n",
    "    \"\"\"build bi-LSTM, concatenating the two directions in an inner manner.\n",
    "        argument:\n",
    "            input_hidden_tensor: list a time_steps series of tensor, shape: [sample_num, n_inputs]\n",
    "            n_inputs: int units of input tensor\n",
    "            n_outputs: int units of output tensor, each bi-LSTM will have half those internal units\n",
    "            config: Config used for the relu_fc\n",
    "        return:\n",
    "            layer_hidden_outputs: list a time_steps series of tensor, shape: [sample_num, n_outputs]\n",
    "    \"\"\"\n",
    "    n_outputs = int(n_outputs/2)\n",
    "\n",
    "    print \"bidir:\"\n",
    "\n",
    "    with tf.variable_scope('pass_forward') as scope2:\n",
    "        hidden_forward = relu_fc(input_hidden_tensor, n_inputs, n_outputs, config)\n",
    "        forward = single_LSTM_cell(hidden_forward, n_outputs)\n",
    "\n",
    "    print (len(hidden_forward), str(hidden_forward[0].get_shape()))\n",
    "\n",
    "    # Backward pass is as simple as surrounding the cell with a double inversion:\n",
    "    with tf.variable_scope('pass_backward') as scope2:\n",
    "        hidden_backward = relu_fc(input_hidden_tensor, n_inputs, n_outputs, config)\n",
    "        backward = list(reversed(single_LSTM_cell(list(reversed(hidden_backward)), n_outputs)))\n",
    "\n",
    "    with tf.variable_scope('bidir_concat') as scope:\n",
    "        # Simply concatenating cells' outputs at each timesteps on the innermost\n",
    "        # dimension, like if the two cells acted as one cell\n",
    "        # with twice the n_hidden size:\n",
    "        layer_hidden_outputs = [\n",
    "            tf.concat(len(f.get_shape()) - 1, [f, b])\n",
    "                for f, b in zip(forward, backward)]\n",
    "\n",
    "    return layer_hidden_outputs\n",
    "\n",
    "\n",
    "def residual_bidirectional_LSTM_layers(input_hidden_tensor, n_input, n_output, layer_level, config, keep_prob_for_dropout):\n",
    "    \"\"\"This architecture is only enabled if \"config.n_layers_in_highway\" has a\n",
    "    value only greater than int(0). The arguments are same than for bi_LSTM_cell.\n",
    "    arguments:\n",
    "        input_hidden_tensor: list a time_steps series of tensor, shape: [sample_num, n_inputs]\n",
    "        n_inputs: int units of input tensor\n",
    "        n_outputs: int units of output tensor, each bi-LSTM will have half those internal units\n",
    "        config: Config used for determining if there are residual connections and if yes, their number and with some batch_norm.\n",
    "    return:\n",
    "        layer_hidden_outputs: list a time_steps series of tensor, shape: [sample_num, n_outputs]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('layer_{}'.format(layer_level)) as scope:\n",
    "\n",
    "        if config.use_bidirectionnal_cells:\n",
    "            get_lstm = lambda input_tensor: bi_LSTM_cell(input_tensor, n_input, n_output, config)\n",
    "        else:\n",
    "            get_lstm = lambda input_tensor: single_LSTM_cell(relu_fc(input_tensor, n_input, n_output, config), n_output)\n",
    "        def add_highway_redisual(layer, residual_minilayer):\n",
    "            return [a + b for a, b in zip(layer, residual_minilayer)]\n",
    "\n",
    "        hidden_LSTM_layer = get_lstm(input_hidden_tensor)\n",
    "        # Adding K new (residual bidir) connections to this first layer:\n",
    "        for i in range(config.n_layers_in_highway - 1):\n",
    "            with tf.variable_scope('LSTM_residual_{}'.format(i)) as scope2:\n",
    "                hidden_LSTM_layer = add_highway_redisual(\n",
    "                    hidden_LSTM_layer,\n",
    "                    get_lstm(input_hidden_tensor)\n",
    "                )\n",
    "\n",
    "        if config.also_add_dropout_between_stacked_cells:\n",
    "            hidden_LSTM_layer = [tf.nn.dropout(out, keep_prob_for_dropout) for out in hidden_LSTM_layer]\n",
    "\n",
    "        return [batch_norm(out, config, i) for i, out in enumerate(hidden_LSTM_layer)]\n",
    "\n",
    "\n",
    "def LSTM_network(feature_mat, config, keep_prob_for_dropout):\n",
    "    \"\"\"model a LSTM Network,\n",
    "      it stacks 2 LSTM layers, each layer has n_hidden=32 cells\n",
    "       and 1 output layer, it is a full connet layer\n",
    "      argument:\n",
    "        feature_mat: ndarray fature matrix, shape=[batch_size,time_steps,n_inputs]\n",
    "        config: class containing config of network\n",
    "      return:\n",
    "              : ndarray  output shape [batch_size, n_classes]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('LSTM_network') as scope:  # TensorFlow graph naming\n",
    "\n",
    "        feature_mat = tf.nn.dropout(feature_mat, keep_prob_for_dropout)\n",
    "\n",
    "        # Exchange dim 1 and dim 0\n",
    "        feature_mat = tf.transpose(feature_mat, [1, 0, 2])\n",
    "        print feature_mat.get_shape()\n",
    "        # New feature_mat's shape: [time_steps, batch_size, n_inputs]\n",
    "\n",
    "        # Temporarily crush the feature_mat's dimensions\n",
    "        feature_mat = tf.reshape(feature_mat, [-1, config.n_inputs])\n",
    "        print feature_mat.get_shape()\n",
    "        # New feature_mat's shape: [time_steps*batch_size, n_inputs]\n",
    "\n",
    "        # Split the series because the rnn cell needs time_steps features, each of shape:\n",
    "        hidden = tf.split(feature_mat, config.n_steps, 0)\n",
    "        print (len(hidden), str(hidden[0].get_shape()))\n",
    "        # New shape: a list of lenght \"time_step\" containing tensors of shape [batch_size, n_hidden]\n",
    "\n",
    "        # Stacking LSTM cells, at least one is stacked:\n",
    "        print \"\\nCreating hidden #1:\"\n",
    "        hidden = residual_bidirectional_LSTM_layers(hidden, config.n_inputs, config.n_hidden, 1, config, keep_prob_for_dropout)\n",
    "        print (len(hidden), str(hidden[0].get_shape()))\n",
    "\n",
    "        for stacked_hidden_index in range(config.n_stacked_layers - 1):\n",
    "            # If the config permits it, we stack more lstm cells:\n",
    "            print \"\\nCreating hidden #{}:\".format(stacked_hidden_index+2)\n",
    "            hidden = residual_bidirectional_LSTM_layers(hidden, config.n_hidden, config.n_hidden, stacked_hidden_index+2, config, keep_prob_for_dropout)\n",
    "            print (len(hidden), str(hidden[0].get_shape()))\n",
    "\n",
    "        print \"\"\n",
    "\n",
    "        # Final fully-connected activation logits\n",
    "        # Get the last output tensor of the inner loop output series, of shape [batch_size, n_classes]\n",
    "        last_hidden = tf.nn.dropout(hidden[-1], keep_prob_for_dropout)\n",
    "        last_logits = relu_fc(\n",
    "            [last_hidden],\n",
    "            config.n_hidden, config.n_classes, config\n",
    "        )[0]\n",
    "        return last_logits\n",
    "\n",
    "\n",
    "def run_with_config(Config, X_train, y_train, X_test, y_test):\n",
    "    tf.reset_default_graph()  # To enable to run multiple things in a loop\n",
    "\n",
    "    #-----------------------------------\n",
    "    # Define parameters for model\n",
    "    #-----------------------------------\n",
    "    config = Config(X_train, X_test)\n",
    "    print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "    print(\"features shape, labels shape, each features mean, each features standard deviation\")\n",
    "    print(X_test.shape, y_test.shape,\n",
    "          np.mean(X_test), np.std(X_test))\n",
    "    print(\"the dataset is therefore properly normalised, as expected.\")\n",
    "\n",
    "    #------------------------------------------------------\n",
    "    # Let's get serious and build the neural network\n",
    "    #------------------------------------------------------\n",
    "    #with tf.device(\"/cpu:0\"):  # Remove this line to use GPU. If you have a too small GPU, it crashes.\n",
    "    X = tf.placeholder(tf.float32, [\n",
    "                       None, config.n_steps, config.n_inputs], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [\n",
    "                       None, config.n_classes], name=\"Y\")\n",
    "\n",
    "    # is_train for dropout control:\n",
    "    is_train = tf.placeholder(tf.bool, name=\"is_train\")\n",
    "    keep_prob_for_dropout = tf.cond(is_train,\n",
    "        lambda: tf.constant(\n",
    "            config.keep_prob_for_dropout,\n",
    "            name=\"keep_prob_for_dropout\"\n",
    "        ),\n",
    "        lambda: tf.constant(\n",
    "            1.0,\n",
    "            name=\"keep_prob_for_dropout\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pred_y = LSTM_network(X, config, keep_prob_for_dropout)\n",
    "\n",
    "    # Loss, optimizer, evaluation\n",
    "\n",
    "    # Softmax loss with L2 and L1 layer-wise regularisation\n",
    "    print \"Unregularised variables:\"\n",
    "    for unreg in [tf_var.name for tf_var in tf.trainable_variables() if (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)]:\n",
    "        print unreg\n",
    "    l2 = config.lambda_loss_amount * sum(\n",
    "        tf.nn.l2_loss(tf_var)\n",
    "            for tf_var in tf.trainable_variables()\n",
    "            if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)\n",
    "    )\n",
    "    # first_weights = [w for w in tf.all_variables() if w.name == 'LSTM_network/layer_1/pass_forward/relu_fc_weights:0'][0]\n",
    "    # l1 = config.lambda_loss_amount * tf.reduce_mean(tf.abs(first_weights))\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=pred_y)) + l2  # + l1\n",
    "\n",
    "    # Gradient clipping Adam optimizer with gradient noise\n",
    "    optimize = tf.contrib.layers.optimize_loss(\n",
    "        loss,\n",
    "        global_step=tf.Variable(0),\n",
    "        learning_rate=config.learning_rate,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=config.learning_rate),\n",
    "        clip_gradients=config.clip_gradients,\n",
    "        gradient_noise_scale=config.gradient_noise_scale\n",
    "    )\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred_y, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "\n",
    "    #--------------------------------------------\n",
    "    # Hooray, now train the neural network\n",
    "    #--------------------------------------------\n",
    "    # Note that log_device_placement can be turned of for less console spam.\n",
    "\n",
    "    sessconfig = tf.ConfigProto(log_device_placement=False)\n",
    "    with tf.Session(config=sessconfig) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        best_accuracy = (0.0, \"iter: -1\")\n",
    "        best_f1_score = (0.0, \"iter: -1\")\n",
    "\n",
    "        # Start training for each batch and loop epochs\n",
    "\n",
    "        worst_batches = []\n",
    "\n",
    "        for i in range(config.training_epochs):\n",
    "\n",
    "            # Loop batches for an epoch:\n",
    "            shuffled_X, shuffled_y = shuffle(X_train, y_train, random_state=i*42)\n",
    "            for start, end in zip(range(0, config.train_count, config.batch_size),\n",
    "                                  range(config.batch_size, config.train_count + 1, config.batch_size)):\n",
    "\n",
    "                _, train_acc, train_loss, train_pred = sess.run(\n",
    "                    [optimize, accuracy, loss, pred_y],\n",
    "                    feed_dict={\n",
    "                        X: shuffled_X[start:end],\n",
    "                        Y: shuffled_y[start:end],\n",
    "                        is_train: True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                worst_batches.append(\n",
    "                    (train_loss, shuffled_X[start:end], shuffled_y[start:end])\n",
    "                )\n",
    "                worst_batches = list(sorted(worst_batches))[-5:]  # Keep 5 poorest\n",
    "\n",
    "            # Train F1 score is not on boosting\n",
    "            train_f1_score = metrics.f1_score(\n",
    "                shuffled_y[start:end].argmax(1), train_pred.argmax(1), average=\"weighted\"\n",
    "            )\n",
    "\n",
    "            # Retrain on top worst batches of this epoch (boosting):\n",
    "            # a.k.a. \"focus on the hardest exercises while training\":\n",
    "            for _, x_, y_ in worst_batches:\n",
    "\n",
    "                _, train_acc, train_loss, train_pred = sess.run(\n",
    "                    [optimize, accuracy, loss, pred_y],\n",
    "                    feed_dict={\n",
    "                        X: x_,\n",
    "                        Y: y_,\n",
    "                        is_train: True\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Test completely at the end of every epoch:\n",
    "            # Calculate accuracy and F1 score\n",
    "            pred_out, accuracy_out, loss_out = sess.run(\n",
    "                [pred_y, accuracy, loss],\n",
    "                feed_dict={\n",
    "                    X: X_test,\n",
    "                    Y: y_test,\n",
    "                    is_train: False\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # \"y_test.argmax(1)\": could be optimised by being computed once...\n",
    "            f1_score_out = metrics.f1_score(\n",
    "                y_test.argmax(1), pred_out.argmax(1), average=\"weighted\"\n",
    "            )\n",
    "\n",
    "            print (\n",
    "                \"iter: {}, \".format(i) + \\\n",
    "                \"train loss: {}, \".format(train_loss) + \\\n",
    "                \"train accuracy: {}, \".format(train_acc) + \\\n",
    "                \"train F1-score: {}, \".format(train_f1_score) + \\\n",
    "                \"test loss: {}, \".format(loss_out) + \\\n",
    "                \"test accuracy: {}, \".format(accuracy_out) + \\\n",
    "                \"test F1-score: {}\".format(f1_score_out)\n",
    "            )\n",
    "\n",
    "            best_accuracy = max(best_accuracy, (accuracy_out, \"iter: {}\".format(i)))\n",
    "            best_f1_score = max(best_f1_score, (f1_score_out, \"iter: {}\".format(i)))\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"final test accuracy: {}\".format(accuracy_out))\n",
    "        print(\"best epoch's test accuracy: {}\".format(best_accuracy))\n",
    "        print(\"final F1 score: {}\".format(f1_score_out))\n",
    "        print(\"best epoch's F1 score: {}\".format(best_f1_score))\n",
    "        print(\"\")\n",
    "\n",
    "    # returning both final and bests accuracies and f1 scores.\n",
    "    return accuracy_out, best_accuracy, f1_score_out, best_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(np.asarray([[3], [0], [3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get serious and build the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.0025\n",
      "lambda_loss_amount: 0.005\n",
      "\n",
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "features shape, labels shape, each features mean, each features standard deviation\n",
      "((443, 1000, 22), (443, 4), 0.0016580587469994378, 0.997292271181712)\n",
      "the dataset is therefore properly normalised, as expected.\n",
      "(1000, ?, 22)\n",
      "(?, 22)\n",
      "(1000, '(?, 22)')\n",
      "\n",
      "Creating hidden #1:\n",
      "(1000, '(?, 10)')\n",
      "\n",
      "Creating hidden #2:\n",
      "(1000, '(?, 10)')\n",
      "\n",
      "Creating hidden #3:\n",
      "(1000, '(?, 10)')\n",
      "\n",
      "Unregularised variables:\n",
      "LSTM_network/layer_1/relu_fc_biases_noreg:0\n",
      "LSTM_network/layer_2/relu_fc_biases_noreg:0\n",
      "LSTM_network/layer_3/relu_fc_biases_noreg:0\n",
      "LSTM_network/relu_fc_biases_noreg:0\n",
      "iter: 0, train loss: 1.54455471039, train accuracy: 0.259999990463, train F1-score: 0.202718616182, test loss: 1.54643905163, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 1, train loss: 1.48272764683, train accuracy: 0.280000001192, train F1-score: 0.127151515152, test loss: 1.49025642872, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 2, train loss: 1.44924807549, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.45233654976, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 3, train loss: 1.42180585861, train accuracy: 0.25, train F1-score: 0.199110941605, test loss: 1.42932701111, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 4, train loss: 1.41743671894, train accuracy: 0.230000004172, train F1-score: 0.123464566929, test loss: 1.41839289665, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 5, train loss: 1.4068801403, train accuracy: 0.25, train F1-score: 0.105766666667, test loss: 1.41094100475, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 6, train loss: 1.40429329872, train accuracy: 0.25, train F1-score: 0.0589022556391, test loss: 1.40521979332, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 7, train loss: 1.40209305286, train accuracy: 0.300000011921, train F1-score: 0.14548553719, test loss: 1.40010118484, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 8, train loss: 1.39648234844, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39958143234, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 9, train loss: 1.39758217335, train accuracy: 0.25, train F1-score: 0.0904918032787, test loss: 1.39855980873, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 10, train loss: 1.39534199238, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39811265469, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 11, train loss: 1.38966166973, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39622426033, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 12, train loss: 1.39009201527, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39628624916, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 13, train loss: 1.39138460159, train accuracy: 0.25, train F1-score: 0.0666666666667, test loss: 1.39572298527, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 14, train loss: 1.38915860653, train accuracy: 0.25, train F1-score: 0.201926729986, test loss: 1.39500248432, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 15, train loss: 1.38941752911, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39453220367, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 16, train loss: 1.38963496685, train accuracy: 0.25, train F1-score: 0.0867213114754, test loss: 1.39301490784, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 17, train loss: 1.39164638519, train accuracy: 0.25, train F1-score: 0.172537313433, test loss: 1.39375090599, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 18, train loss: 1.38972580433, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.3940435648, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 19, train loss: 1.39090311527, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39352750778, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 20, train loss: 1.39021325111, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39318501949, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 21, train loss: 1.39139771461, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39401364326, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 22, train loss: 1.38835227489, train accuracy: 0.25, train F1-score: 0.0666666666667, test loss: 1.39224207401, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 23, train loss: 1.38997161388, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39308464527, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 24, train loss: 1.38426363468, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39447653294, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 25, train loss: 1.38613116741, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39367580414, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 26, train loss: 1.38709533215, train accuracy: 0.25, train F1-score: 0.0606722689076, test loss: 1.39286184311, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 27, train loss: 1.38960969448, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39312458038, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 28, train loss: 1.38751327991, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.3927744627, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 29, train loss: 1.38837814331, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39293980598, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 30, train loss: 1.38722670078, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.3924305439, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 31, train loss: 1.38657081127, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39261746407, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 32, train loss: 1.3860887289, train accuracy: 0.25, train F1-score: 0.0666666666667, test loss: 1.39260041714, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 33, train loss: 1.38656628132, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39255928993, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 34, train loss: 1.38531816006, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39327168465, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 35, train loss: 1.38571178913, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.39308476448, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 36, train loss: 1.38851702213, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39245736599, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 37, train loss: 1.38862466812, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39195752144, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 38, train loss: 1.38636648655, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39237415791, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 39, train loss: 1.38742852211, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39184296131, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 40, train loss: 1.38861989975, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39180982113, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 41, train loss: 1.38670909405, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.3917517662, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 42, train loss: 1.38564300537, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39237308502, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 43, train loss: 1.38887023926, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39172005653, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 44, train loss: 1.38643968105, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39120364189, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 45, train loss: 1.38727521896, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39164364338, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 46, train loss: 1.38671064377, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39157688618, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 47, train loss: 1.38759434223, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39219391346, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 48, train loss: 1.38774633408, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39231169224, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 49, train loss: 1.3865737915, train accuracy: 0.25, train F1-score: 0.0606722689076, test loss: 1.39218580723, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 50, train loss: 1.38776433468, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39152598381, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 51, train loss: 1.3878070116, train accuracy: 0.25, train F1-score: 0.181481481481, test loss: 1.39152026176, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 52, train loss: 1.38712215424, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39248812199, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 53, train loss: 1.3867483139, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39208912849, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 54, train loss: 1.38689863682, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39190161228, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 55, train loss: 1.38675963879, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39283514023, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 56, train loss: 1.38698351383, train accuracy: 0.25, train F1-score: 0.0549152542373, test loss: 1.39185667038, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 57, train loss: 1.3869048357, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39182293415, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 58, train loss: 1.38723647594, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39231228828, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 59, train loss: 1.38625848293, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39226448536, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 60, train loss: 1.38638162613, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39201045036, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 61, train loss: 1.3875361681, train accuracy: 0.25, train F1-score: 0.0391304347826, test loss: 1.39216887951, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 62, train loss: 1.38674974442, train accuracy: 0.25, train F1-score: 0.181481481481, test loss: 1.39216053486, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 63, train loss: 1.38700950146, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39243614674, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 64, train loss: 1.3869073391, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39203333855, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 65, train loss: 1.38648188114, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39167892933, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 66, train loss: 1.3867405653, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39158129692, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 67, train loss: 1.38665294647, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39131879807, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 68, train loss: 1.38728237152, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39187049866, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 69, train loss: 1.38674259186, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39208972454, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 70, train loss: 1.38673532009, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39192712307, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 71, train loss: 1.38642001152, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39232969284, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 72, train loss: 1.38706147671, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39164566994, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 73, train loss: 1.38678276539, train accuracy: 0.25, train F1-score: 0.172537313433, test loss: 1.39158296585, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 74, train loss: 1.38686323166, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39207863808, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 75, train loss: 1.38665056229, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39191067219, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 76, train loss: 1.38674414158, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39199030399, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 77, train loss: 1.38662159443, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39171075821, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 78, train loss: 1.38639175892, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39205479622, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 79, train loss: 1.38691329956, train accuracy: 0.25, train F1-score: 0.0666666666667, test loss: 1.39199459553, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 80, train loss: 1.38684356213, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39179170132, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 81, train loss: 1.38672161102, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39186275005, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 82, train loss: 1.38652646542, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39192509651, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 83, train loss: 1.38663911819, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39202046394, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 84, train loss: 1.38669276237, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39205884933, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 85, train loss: 1.38667619228, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39182162285, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 86, train loss: 1.38664078712, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39173853397, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 87, train loss: 1.38669383526, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39186882973, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 88, train loss: 1.38671553135, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.3919506073, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 89, train loss: 1.38658988476, train accuracy: 0.25, train F1-score: 0.172537313433, test loss: 1.39157533646, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 90, train loss: 1.38666558266, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39189183712, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 91, train loss: 1.38670945168, train accuracy: 0.25, train F1-score: 0.0606722689076, test loss: 1.39204204082, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 92, train loss: 1.38666248322, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39167749882, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 93, train loss: 1.38665521145, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39173161983, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 94, train loss: 1.38673400879, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39221918583, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 95, train loss: 1.38675773144, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39211285114, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 96, train loss: 1.38672208786, train accuracy: 0.25, train F1-score: 0.172537313433, test loss: 1.39206647873, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 97, train loss: 1.38674151897, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39220881462, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 98, train loss: 1.38678300381, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39223396778, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 99, train loss: 1.38667225838, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39205932617, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 100, train loss: 1.38667190075, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39187431335, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 101, train loss: 1.38670969009, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39209938049, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 102, train loss: 1.38676857948, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.39219677448, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 103, train loss: 1.38684499264, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39226591587, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 104, train loss: 1.38684785366, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39238941669, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 105, train loss: 1.38675749302, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39232039452, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 106, train loss: 1.38675904274, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39240813255, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 107, train loss: 1.3867777586, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39237868786, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 108, train loss: 1.38673520088, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39218366146, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 109, train loss: 1.38666033745, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.3919301033, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 110, train loss: 1.38665056229, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39175713062, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 111, train loss: 1.38654375076, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39134657383, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 112, train loss: 1.38653075695, train accuracy: 0.25, train F1-score: 0.218848920863, test loss: 1.39124774933, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 113, train loss: 1.38664543629, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39166605473, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 114, train loss: 1.38665294647, train accuracy: 0.25, train F1-score: 0.172537313433, test loss: 1.39172589779, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 115, train loss: 1.38677370548, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.39218306541, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 116, train loss: 1.386901021, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.3927295208, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 117, train loss: 1.38684463501, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39263617992, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 118, train loss: 1.38678848743, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39250576496, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 119, train loss: 1.38684260845, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39276885986, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 120, train loss: 1.38689732552, train accuracy: 0.25, train F1-score: 0.0666666666667, test loss: 1.39290499687, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 121, train loss: 1.38686811924, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39276087284, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 122, train loss: 1.38678765297, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.3923265934, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 123, train loss: 1.38672709465, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39212858677, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 124, train loss: 1.3866186142, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39198660851, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 125, train loss: 1.3866622448, train accuracy: 0.25, train F1-score: 0.181481481481, test loss: 1.39231300354, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 126, train loss: 1.38673841953, train accuracy: 0.25, train F1-score: 0.0549152542373, test loss: 1.39237713814, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 127, train loss: 1.38658964634, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39204466343, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 128, train loss: 1.38658499718, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39190793037, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 129, train loss: 1.38663816452, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39213502407, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 130, train loss: 1.38665878773, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39224696159, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 131, train loss: 1.38676059246, train accuracy: 0.25, train F1-score: 0.0606722689076, test loss: 1.39244675636, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 132, train loss: 1.38661444187, train accuracy: 0.25, train F1-score: 0.172537313433, test loss: 1.39177310467, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 133, train loss: 1.38662469387, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39194762707, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 134, train loss: 1.38665938377, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39199054241, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 135, train loss: 1.38675320148, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39225935936, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 136, train loss: 1.38678014278, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39242649078, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 137, train loss: 1.38668012619, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39214730263, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 138, train loss: 1.38675177097, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39230310917, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 139, train loss: 1.38673758507, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39230072498, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 140, train loss: 1.38675570488, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39230692387, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 141, train loss: 1.38669645786, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39205467701, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 142, train loss: 1.38665866852, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39203321934, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 143, train loss: 1.38672566414, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39213836193, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 144, train loss: 1.38673329353, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39215934277, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 145, train loss: 1.38669061661, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39193940163, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 146, train loss: 1.38668119907, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39204478264, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 147, train loss: 1.38667678833, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39191544056, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 148, train loss: 1.38667833805, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39182066917, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 149, train loss: 1.38665997982, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39173793793, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 150, train loss: 1.38666307926, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.3917632103, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 151, train loss: 1.38663160801, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39172184467, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 152, train loss: 1.38658034801, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39141845703, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 153, train loss: 1.38655161858, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39129018784, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 154, train loss: 1.38652050495, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.3913141489, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 155, train loss: 1.38660037518, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39142310619, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 156, train loss: 1.38671314716, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39206516743, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 157, train loss: 1.38664472103, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.39179730415, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 158, train loss: 1.38663339615, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39180660248, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 159, train loss: 1.38664078712, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39165842533, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 160, train loss: 1.38670659065, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39176702499, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 161, train loss: 1.38663601875, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39178276062, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 162, train loss: 1.38664960861, train accuracy: 0.25, train F1-score: 0.0606722689076, test loss: 1.392105937, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 163, train loss: 1.38668477535, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39199542999, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 164, train loss: 1.38668763638, train accuracy: 0.25, train F1-score: 0.0494017094017, test loss: 1.39182114601, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 165, train loss: 1.38671696186, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39185011387, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 166, train loss: 1.38666200638, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39190518856, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 167, train loss: 1.38670015335, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.3920776844, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 168, train loss: 1.38670349121, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.3922971487, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 169, train loss: 1.38678967953, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39237535, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 170, train loss: 1.38676130772, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39225947857, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 171, train loss: 1.38675737381, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.3924125433, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 172, train loss: 1.38674724102, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39227151871, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 173, train loss: 1.38666522503, train accuracy: 0.25, train F1-score: 0.199854014599, test loss: 1.39199769497, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 174, train loss: 1.38665735722, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39191389084, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 175, train loss: 1.38662779331, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39178705215, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 176, train loss: 1.38661789894, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39187026024, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 177, train loss: 1.38659322262, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.39189374447, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 178, train loss: 1.38664507866, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39218592644, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 179, train loss: 1.38657367229, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39193165302, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 180, train loss: 1.38662648201, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.392131567, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 181, train loss: 1.38657987118, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.3919557333, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 182, train loss: 1.38659787178, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.39203643799, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 183, train loss: 1.38666951656, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39205610752, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 184, train loss: 1.38665986061, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39203095436, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 185, train loss: 1.38671886921, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39229524136, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 186, train loss: 1.38680315018, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39257228374, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 187, train loss: 1.38676559925, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39222836494, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 188, train loss: 1.38683307171, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.39250195026, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 189, train loss: 1.38676822186, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39239740372, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 190, train loss: 1.38675904274, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39211475849, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 191, train loss: 1.38669681549, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39204013348, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 192, train loss: 1.38679361343, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39232051373, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 193, train loss: 1.38672578335, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39232695103, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 194, train loss: 1.38674640656, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39255678654, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 195, train loss: 1.38673365116, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39261424541, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 196, train loss: 1.38665640354, train accuracy: 0.25, train F1-score: 0.181481481481, test loss: 1.39230036736, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 197, train loss: 1.38672721386, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39241290092, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 198, train loss: 1.38668751717, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39208650589, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 199, train loss: 1.38671159744, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.3921598196, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 200, train loss: 1.38673198223, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39206302166, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 201, train loss: 1.38679158688, train accuracy: 0.25, train F1-score: 0.0793442622951, test loss: 1.39229977131, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 202, train loss: 1.3867430687, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.3922137022, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 203, train loss: 1.38671302795, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39213812351, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 204, train loss: 1.38661110401, train accuracy: 0.25, train F1-score: 0.199854014599, test loss: 1.39205682278, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 205, train loss: 1.38673758507, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39247691631, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 206, train loss: 1.3867661953, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39237630367, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 207, train loss: 1.38677704334, train accuracy: 0.25, train F1-score: 0.163759398496, test loss: 1.39214384556, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 208, train loss: 1.3867316246, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39209449291, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 209, train loss: 1.3867123127, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39200747013, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 210, train loss: 1.38660502434, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39165651798, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 211, train loss: 1.38659358025, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39176511765, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 212, train loss: 1.38672268391, train accuracy: 0.25, train F1-score: 0.0441379310345, test loss: 1.39221096039, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 213, train loss: 1.38668560982, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39205646515, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 214, train loss: 1.38664889336, train accuracy: 0.25, train F1-score: 0.107301587302, test loss: 1.39200437069, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 215, train loss: 1.38674747944, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39225757122, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 216, train loss: 1.38670611382, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39217376709, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 217, train loss: 1.38666391373, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39224994183, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 218, train loss: 1.38668394089, train accuracy: 0.25, train F1-score: 0.155151515152, test loss: 1.3923125267, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 219, train loss: 1.38666164875, train accuracy: 0.25, train F1-score: 0.190588235294, test loss: 1.39215457439, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 220, train loss: 1.38662934303, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39186131954, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 221, train loss: 1.38665246964, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39192771912, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 222, train loss: 1.38661539555, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.3916670084, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 223, train loss: 1.38664340973, train accuracy: 0.25, train F1-score: 0.0860162601626, test loss: 1.39177620411, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 224, train loss: 1.38669657707, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39205813408, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 225, train loss: 1.3865814209, train accuracy: 0.25, train F1-score: 0.172537313433, test loss: 1.39191353321, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 226, train loss: 1.3866494894, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.392111063, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 227, train loss: 1.38677978516, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39241242409, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 228, train loss: 1.38684093952, train accuracy: 0.25, train F1-score: 0.0494017094017, test loss: 1.39258515835, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 229, train loss: 1.38682246208, train accuracy: 0.25, train F1-score: 0.130387596899, test loss: 1.39243817329, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 230, train loss: 1.3868625164, train accuracy: 0.25, train F1-score: 0.0441379310345, test loss: 1.39270675182, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 231, train loss: 1.38675522804, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.3923561573, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 232, train loss: 1.38673710823, train accuracy: 0.25, train F1-score: 0.138461538462, test loss: 1.39214289188, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 233, train loss: 1.38665938377, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39187848568, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 234, train loss: 1.38672637939, train accuracy: 0.25, train F1-score: 0.0606722689076, test loss: 1.39193427563, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 235, train loss: 1.38674426079, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39196419716, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 236, train loss: 1.38668322563, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39181792736, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 237, train loss: 1.38671505451, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.392167449, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 238, train loss: 1.38677716255, train accuracy: 0.25, train F1-score: 0.0441379310345, test loss: 1.39247763157, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 239, train loss: 1.38663578033, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39195418358, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 240, train loss: 1.38671052456, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39214026928, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 241, train loss: 1.38675141335, train accuracy: 0.25, train F1-score: 0.114803149606, test loss: 1.39232218266, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 242, train loss: 1.38683092594, train accuracy: 0.25, train F1-score: 0.0666666666667, test loss: 1.39259290695, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 243, train loss: 1.3867919445, train accuracy: 0.25, train F1-score: 0.1225, test loss: 1.39240980148, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 244, train loss: 1.38683843613, train accuracy: 0.25, train F1-score: 0.0666666666667, test loss: 1.39235949516, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 245, train loss: 1.3867701292, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39220809937, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 246, train loss: 1.38685464859, train accuracy: 0.25, train F1-score: 0.0728925619835, test loss: 1.39263224602, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 247, train loss: 1.38686406612, train accuracy: 0.25, train F1-score: 0.0929032258065, test loss: 1.39266896248, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 248, train loss: 1.38680577278, train accuracy: 0.25, train F1-score: 0.1, test loss: 1.39217853546, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "iter: 249, train loss: 1.38673603535, train accuracy: 0.25, train F1-score: 0.146717557252, test loss: 1.39181220531, test accuracy: 0.203160271049, test F1-score: 0.0686094723423\n",
      "\n",
      "final test accuracy: 0.203160271049\n",
      "best epoch's test accuracy: (0.20316027, 'iter: 99')\n",
      "final F1 score: 0.0686094723423\n",
      "best epoch's F1 score: (0.06860947234233586, 'iter: 99')\n",
      "\n",
      "(0.20316027, (0.20316027, 'iter: 99'), 0.06860947234233586, (0.06860947234233586, 'iter: 99'))\n",
      "________________________________________________________\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "n_layers_in_highway = 0\n",
    "n_stacked_layers = 3\n",
    "trial_name = \"{}x{}\".format(n_layers_in_highway, n_stacked_layers)\n",
    "\n",
    "for learning_rate in [0.0025]:  # [0.01, 0.007, 0.001, 0.0007, 0.0001]:\n",
    "    for lambda_loss_amount in [0.005]:\n",
    "        for clip_gradients in [15.0]:\n",
    "            print \"learning_rate: {}\".format(learning_rate)\n",
    "            print \"lambda_loss_amount: {}\".format(lambda_loss_amount)\n",
    "            print \"\"\n",
    "\n",
    "            class EditedConfig(Config):\n",
    "                def __init__(self, X, Y):\n",
    "                    super(EditedConfig, self).__init__(X, Y)\n",
    "\n",
    "                    # Edit only some parameters:\n",
    "                    self.learning_rate = learning_rate\n",
    "                    self.lambda_loss_amount = lambda_loss_amount\n",
    "                    self.clip_gradients = clip_gradients\n",
    "                    # Architecture params:\n",
    "                    self.n_layers_in_highway = n_layers_in_highway\n",
    "                    self.n_stacked_layers = n_stacked_layers\n",
    "\n",
    "            # # Useful catch upon looping (e.g.: not enough memory)\n",
    "            # try:\n",
    "            #     accuracy_out, best_accuracy = run_with_config(EditedConfig)\n",
    "            # except:\n",
    "            #     accuracy_out, best_accuracy = -1, -1\n",
    "            accuracy_out, best_accuracy, f1_score_out, best_f1_score = (\n",
    "                run_with_config(EditedConfig, X_train, one_hot(y_train.astype(int)), X_test, one_hot(y_test.astype(int)))\n",
    "            )\n",
    "            print (accuracy_out, best_accuracy, f1_score_out, best_f1_score)\n",
    "\n",
    "            with open('{}_result.txt'.format(trial_name), 'a') as f:\n",
    "                f.write(str(learning_rate) + ' \\t' + str(lambda_loss_amount) + ' \\t' + str(clip_gradients) + ' \\t' + str(\n",
    "                    accuracy_out) + ' \\t' + str(best_accuracy) + ' \\t' + str(f1_score_out) + ' \\t' + str(best_f1_score) + '\\n\\n')\n",
    "\n",
    "            print \"________________________________________________________\"\n",
    "        print \"\"\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Outstandingly, **the final accuracy is of 20.3%**! \n",
    "\n",
    "## References\n",
    "\n",
    "The code is based on the following repository: \n",
    "> Guillaume Chevalier, LSTMs for Human Activity Recognition, 2016\n",
    "> https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
