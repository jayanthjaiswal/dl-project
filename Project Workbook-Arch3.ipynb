{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "\n",
    "mat_names = glob.glob('./project_datasets/*.mat')\n",
    "# each test subject got a different file - 9 test subjects\n",
    "print(mat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matfile = h5py.File(mat_names[0], 'r')\n",
    "print(matfile.keys()) #image and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mat = matfile['image']\n",
    "image_shape = image_mat.shape # 288 (48x6) trials across 25 electrodes for 1000 time points (250Hz*4s)\n",
    "print image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mat = matfile['type']\n",
    "type_shape = type_mat.shape\n",
    "print type_shape\n",
    "# plt.plot(type_mat[0,:288]) # gets the significant values of types\n",
    "# all the 0's occur after 288, and are meaningless I think\n",
    "# so the image_mat, which has shape (288, 25, 1000) should correspond\n",
    "# to the first 288 entries of type_mat, so\n",
    "# for a single subject, training data should be image_mat, with 288 samples, each sample has shape (25, 1000)\n",
    "# and our target label matrix should be type_mat[:288] (or 287?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = np.sum(np.isnan(image_mat[:,:]))\n",
    "print(nans) #No NaN in the data\n",
    "print len(image_mat[0:,:])\n",
    "count = 0\n",
    "# for i in range(len(image_mat[0:,:])):\n",
    "#  if np.sum(np.isnan(image_mat[i:,:])):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_set = list(set(type_mat[0,:]))\n",
    "print(type_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(image_shape[1]):\n",
    "    plt.subplot(image_shape[1]**0.5,image_shape[1]**0.5,i+1)\n",
    "    plt.plot(matfile['image'][0,i,:])\n",
    "    plt.title(\"Electrode \" + str(i))\n",
    "    plt.rcParams[\"figure.figsize\"] = [20,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(matfile['type'][0,:image_shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_channels = 22 #from project guidelines\n",
    "test_count = 50 #from project guideline, 238 for train-validation and 50 for test\n",
    "validation_count = 38 # 38 points in validation set and remaining 200 points in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting seed\n",
    "np.random.seed(seed=1337)\n",
    "test_picked = np.random.choice(image_shape[0], test_count, replace=False)\n",
    "train_val_picked = np.setdiff1d(np.arange(image_shape[0]), test_picked)\n",
    "val_picked = train_val_picked[:validation_count]\n",
    "train_picked = train_val_picked[validation_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_data_X = []\n",
    "training_data_X = []\n",
    "validation_data_X = []\n",
    "test_data_X = []\n",
    "\n",
    "trainval_data_Y = []\n",
    "training_data_Y = []\n",
    "validation_data_Y = []\n",
    "test_data_Y = []\n",
    "\n",
    "for i in range(len(mat_names)):\n",
    "    matfile = h5py.File(mat_names[i], 'r')\n",
    "    \n",
    "    trainval_data_X.append(matfile['image'][sorted(train_val_picked),:EEG_channels,:]) #(238, 22, 1000) x 9\n",
    "    training_data_X.append(matfile['image'][sorted(train_picked),:EEG_channels,:]) #(200, 22, 1000) x 9\n",
    "    validation_data_X.append(matfile['image'][sorted(val_picked),:EEG_channels,:]) #(38, 22, 1000) x 9\n",
    "    test_data_X.append(matfile['image'][sorted(test_picked),:EEG_channels,:]) #(50, 22, 1000) x 9\n",
    "    \n",
    "    trainval_data_Y.append(matfile['type'][0,sorted(train_val_picked)] - type_set[1]) #(238, ) x 9\n",
    "    training_data_Y.append(matfile['type'][0,sorted(train_picked)] - type_set[1]) #(200, ) x 9\n",
    "    validation_data_Y.append(matfile['type'][0,sorted(val_picked)] - type_set[1]) #(38, ) x 9\n",
    "    test_data_Y.append(matfile['type'][0,sorted(test_picked)] - type_set[1]) #(50, ) x 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mat_names)):\n",
    "    assert(set(training_data_Y[i]) == set(validation_data_Y[i])) #check whether we have data for all labels \n",
    "    assert(set(training_data_Y[i]) == set(test_data_Y[i])) #check whether we have data for all labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_shape = training_data_X[0].shape\n",
    "print(training_data_shape) #(200, 22, 1000) while test data shape is (50, 22, 1000) and validation data is (38, 22,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data_Y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def remove_nan_rows_A(A, b):\n",
    "    if (np.isnan(A).any() or np.isnan(b).any()):\n",
    "        mask = ~np.isnan(np.sum(A,axis=(1,2))) & ~np.isnan(b[:])\n",
    "        A = A[mask, :, :]\n",
    "        b = b[mask]\n",
    "        \n",
    "    assert A.shape[0] == b.shape[0]\n",
    "    return A, b\n",
    "\n",
    "\n",
    "cnn_trainval_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), trainval_data_X) #(2142, 22, 1000) \n",
    "cnn_training_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), training_data_X) #(1800, 22, 1000)\n",
    "cnn_validation_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), validation_data_X) #(342, 22, 1000)\n",
    "cnn_test_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), test_data_X) #(450, 22, 1000)\n",
    "\n",
    "cnn_trainval_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), trainval_data_Y) #(2142, )\n",
    "cnn_training_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), training_data_Y) #(1800, )\n",
    "cnn_validation_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), validation_data_Y) #(342, )\n",
    "cnn_test_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), test_data_Y) #(450,)\n",
    "\n",
    "\n",
    "cnn_training_data_X, cnn_training_data_Y = remove_nan_rows_A(cnn_training_data_X, cnn_training_data_Y) #(1775,22,1000)\n",
    "cnn_validation_data_X, cnn_validation_data_Y = remove_nan_rows_A(cnn_validation_data_X, cnn_validation_data_Y) #(340,22,1000)\n",
    "\n",
    "cnn_test_data_X, cnn_test_data_Y = remove_nan_rows_A(cnn_test_data_X, cnn_test_data_Y) #(443,22,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cnn_training_data_X.shape\n",
    "print cnn_validation_data_X.shape\n",
    "print cnn_test_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_trainval_data_X = np.transpose(rnn_trainval_data_X, (0,2,1))\n",
    "# mean_list = np.mean(cnn_trainval_data_X.reshape(-1, cnn_trainval_data_X.shape[-1]), axis=0)\n",
    "# std_list = np.sqrt((np.var(cnn_trainval_data_X.reshape(-1, cnn_trainval_data_X.shape[-1]), axis=0)))\n",
    "# cnn_training_data_X = (cnn_training_data_X - mean_list)/std_list\n",
    "# cnn_validation_data_X = np.transpose(rnn_trainval_data_X, (0,2,1))\n",
    "\n",
    "# print cnn_training_data_X.shape()\n",
    "# print cnn_validation_data_X.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_training_X = np.reshape(cnn_training_data_X, (1775, 22000))\n",
    "# svm_training_Y = cnn_training_data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = svm.SVC()\n",
    "# clf.fit(svm_training_X, svm_training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_test_X = np.reshape(cnn_validation_data_X, (340, 22000))\n",
    "# svm_test_Y = cnn_validation_data_Y\n",
    "# predicted = clf.predict(svm_test_X)\n",
    "\n",
    "# print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import gc\n",
    "import resource\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get rss size, see stat(5) under statm. This is in pages...\n",
    "def memory_usage():\n",
    "    return int(open('/proc/self/statm').read().split()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "batch_size = 10\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.BatchNorm1d(22),\n",
    "#             nn.Conv1d(22, 64, kernel_size=40), #32x1000\n",
    "#             nn.BatchNorm1d(64), \n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(2)) #32 x 500\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv1d(64, 32, kernel_size=40), #8x250\n",
    "#             nn.BatchNorm1d(32),\n",
    "#             nn.ReLU()) #8x250\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv1d(32, 16, kernel_size=40), #8x250\n",
    "#             nn.BatchNorm1d(16),\n",
    "#             nn.ReLU()) #8x250\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv1d(16, 8, kernel_size=40), #8x250\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.ReLU()) #8x250\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(2904, 500), #100\n",
    "#             nn.BatchNorm1d(500),\n",
    "#             nn.ReLU())\n",
    "# #         self.fc2 = nn.Sequential(\n",
    "# #             nn.Linear(5000, 1000), #100\n",
    "# #             nn.BatchNorm1d(1000),\n",
    "# #             nn.ReLU())\n",
    "# #         self.fc3 = nn.Sequential(\n",
    "# #             nn.Linear(1000, 500), #100\n",
    "# #             nn.BatchNorm1d(500),\n",
    "# #             nn.ReLU())\n",
    "# #         self.fc4 = nn.Sequential(\n",
    "# #             nn.Linear(1000, 500), #100\n",
    "# #             nn.BatchNorm1d(500),\n",
    "# #             nn.ReLU())\n",
    "#         self.fc5 = nn.Linear(500, 4)\n",
    "        \n",
    "#     #basic forward - go through two conv layers + fc layer\n",
    "   \n",
    "#     def forward(self, x):\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.fc1(out)\n",
    "# #         out = self.fc2(out)\n",
    "# #         out = self.fc3(out)\n",
    "# #         out = self.fc4(out)\n",
    "#         out = self.fc5(out)\n",
    "#         return out\n",
    "\n",
    "#59% with this arch and 10 epochs - uping epochs\n",
    "# 20 epochs - mem problem\n",
    "#back to 10 epochs, changing first layer kernel from 15 to 25\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(22),\n",
    "            nn.Conv1d(22, 44, kernel_size=15, groups=22), #32x1000\n",
    "            nn.BatchNorm1d(44), \n",
    "            nn.ELU(),\n",
    "            nn.AvgPool1d(kernel_size=75, stride=15)) #32 x 500\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(44, 30, kernel_size=3), #8x250\n",
    "            nn.BatchNorm1d(30),\n",
    "            nn.ELU()) #8x250\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(15904, 5000), #100\n",
    "#             nn.BatchNorm1d(5000),\n",
    "#             nn.ReLU())\n",
    "#         self.fc2 = nn.Sequential(\n",
    "#             nn.Linear(5000, 1000), #100\n",
    "#             nn.BatchNorm1d(1000),\n",
    "#             nn.ReLU())\n",
    "#         self.fc3 = nn.Sequential(\n",
    "#             nn.Linear(1000, 500), #100\n",
    "#             nn.BatchNorm1d(500),\n",
    "#             nn.ReLU())\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(1770, 10), #100\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU())\n",
    "        self.fc5 = nn.Linear(1770, 4)\n",
    "        \n",
    "    #basic forward - go through two conv layers + fc layer\n",
    "   \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "#         out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "\n",
    "cnn = CNN()\n",
    "\n",
    "for i in cnn.parameters():\n",
    "    print i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x_train = torch.Tensor(cnn_training_data_X)\n",
    "tensor_y_train = torch.LongTensor(cnn_training_data_Y)\n",
    "\n",
    "print tensor_x_train.shape\n",
    "print tensor_y_train.shape\n",
    "\n",
    "tensor_x_val = torch.Tensor(cnn_validation_data_X)\n",
    "tensor_y_val = torch.LongTensor(cnn_validation_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(tensor_x_train,tensor_y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(tensor_x_val,tensor_y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(train_loader)\n",
    "print len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(1337)\n",
    "cnn = CNN()\n",
    "\n",
    "# Loss and Optimizer\n",
    "\n",
    "#Can also try nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate, weight_decay=0.03)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "loss_history = []\n",
    "          \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "    #     for i in range(cnn_training_data_X.shape[0]):\n",
    "        images = Variable(images, requires_grad=True) #unsqueeze used to make a 4d tensor because \n",
    "    #     print images.shape\n",
    "        labels = Variable(labels, volatile=True)\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_history.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del outputs\n",
    "        del images, labels\n",
    "        gc.collect()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(cnn_training_data_X.shape)//batch_size, loss.data[0]))\n",
    "            gc.collect()\n",
    "            max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "            print(\"{:.2f} MB\".format(max_mem_used /(1024*1024)))\n",
    " #           print (\"mem usage\",memory_usage())\n",
    "        del loss\n",
    "    \n",
    "    images = Variable(torch.Tensor(cnn_training_data_X), volatile=True)\n",
    "    labels = torch.LongTensor(cnn_training_data_Y)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = labels.size(0) #1800\n",
    "#     print (predicted == labels).sum().int()\n",
    "#     print type((predicted == labels).sum().int())\n",
    "    correct = (predicted == labels).sum()\n",
    "    del outputs\n",
    "    del images, labels\n",
    "    print(correct)\n",
    "    print('Train Accuracy: %d %%' % (100.0 * int(correct) / float(total)))\n",
    "    train_acc.append((100.0 * int(correct) / float(total)))\n",
    "\n",
    "    cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    images = Variable(torch.Tensor(cnn_validation_data_X), volatile=True)\n",
    "    labels = torch.LongTensor(cnn_validation_data_Y)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = labels.size(0)\n",
    "    correct = (predicted == labels).sum()\n",
    "    del outputs\n",
    "    del images, labels\n",
    "    print('validation Accuracy: %d %%' % (100.0 * int(correct) / float(total)))\n",
    "    val_acc.append((100.0 * int(correct) / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "images = Variable(torch.Tensor(cnn_test_data_X))\n",
    "labels = torch.LongTensor(cnn_test_data_Y)\n",
    "outputs = cnn(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total += labels.size(0)\n",
    "correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100.0 * int(correct) / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot train/val accuracies\n",
    "print train_acc\n",
    "plt.title(\"Train/Test Acc\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel('acc')\n",
    "plt.plot(train_acc, color='red')\n",
    "plt.plot(val_acc, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guppy import hpy; h=hpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.heap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.iso(1,[],{})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
