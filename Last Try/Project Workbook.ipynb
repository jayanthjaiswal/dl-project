{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../project_datasets/A07T_slice.mat', '../project_datasets/A02T_slice.mat', '../project_datasets/A05T_slice.mat', '../project_datasets/A08T_slice.mat', '../project_datasets/A03T_slice.mat', '../project_datasets/A06T_slice.mat', '../project_datasets/A01T_slice.mat', '../project_datasets/A04T_slice.mat', '../project_datasets/A09T_slice.mat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "\n",
    "mat_names = glob.glob('../project_datasets/*.mat')\n",
    "# each test subject got a different file - 9 test subjects\n",
    "print(mat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'image', u'type']\n",
      "(288, 25, 1000)\n"
     ]
    }
   ],
   "source": [
    "matfile = h5py.File(mat_names[0], 'r')\n",
    "print(matfile.keys()) #image and type\n",
    "image_mat = matfile['image']\n",
    "image_shape = image_mat.shape # 288 (48x6) trials across 25 electrodes for 1000 time points (250Hz*4s)\n",
    "print(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "0\n",
      "288\n",
      "[0.0, 769.0, 770.0, 771.0, 772.0]\n"
     ]
    }
   ],
   "source": [
    "type_mat = matfile['type']\n",
    "type_shape = type_mat.shape\n",
    "print(type_shape)\n",
    "# plt.plot(type_mat[0,:288]) # gets the significant values of types\n",
    "# all the 0's occur after 288, and are meaningless I think\n",
    "# so the image_mat, which has shape (288, 25, 1000) should correspond\n",
    "# to the first 288 entries of type_mat, so\n",
    "# for a single subject, training data should be image_mat, with 288 samples, each sample has shape (25, 1000)\n",
    "# and our target label matrix should be type_mat[:288] (or 287?)\n",
    "nans = np.sum(np.isnan(image_mat[:,:]))\n",
    "print(nans) #No NaN in the data\n",
    "print(len(image_mat[0:,:]))\n",
    "count = 0\n",
    "# for i in range(len(image_mat[0:,:])):\n",
    "#  if np.sum(np.isnan(image_mat[i:,:])):\n",
    "#         pass\n",
    "type_set = list(set(type_mat[0,:]))\n",
    "print(type_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_channels = 22 #from project guidelines\n",
    "test_count = 50 #from project guideline, 238 for train-validation and 50 for test\n",
    "validation_count = 38 # 38 points in validation set and remaining 200 points in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting seed\n",
    "np.random.seed(seed=101)\n",
    "test_picked = np.random.choice(image_shape[0], test_count, replace=False)\n",
    "train_val_picked = np.setdiff1d(np.arange(image_shape[0]), test_picked)\n",
    "val_picked = train_val_picked[:validation_count]\n",
    "train_picked = train_val_picked[validation_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_data_X = []\n",
    "training_data_X = []\n",
    "validation_data_X = []\n",
    "test_data_X = []\n",
    "\n",
    "trainval_data_Y = []\n",
    "training_data_Y = []\n",
    "validation_data_Y = []\n",
    "test_data_Y = []\n",
    "\n",
    "for i in range(len(mat_names)):\n",
    "    matfile = h5py.File(mat_names[i], 'r')\n",
    "    \n",
    "    trainval_data_X.append(matfile['image'][sorted(train_val_picked),:EEG_channels,:]) #(238, 22, 1000) x 9\n",
    "    training_data_X.append(matfile['image'][sorted(train_picked),:EEG_channels,:]) #(200, 22, 1000) x 9\n",
    "    validation_data_X.append(matfile['image'][sorted(val_picked),:EEG_channels,:]) #(38, 22, 1000) x 9\n",
    "    test_data_X.append(matfile['image'][sorted(test_picked),:EEG_channels,:]) #(50, 22, 1000) x 9\n",
    "    \n",
    "    trainval_data_Y.append(matfile['type'][0,sorted(train_val_picked)] - type_set[1]) #(238, ) x 9\n",
    "    training_data_Y.append(matfile['type'][0,sorted(train_picked)] - type_set[1]) #(200, ) x 9\n",
    "    validation_data_Y.append(matfile['type'][0,sorted(val_picked)] - type_set[1]) #(38, ) x 9\n",
    "    test_data_Y.append(matfile['type'][0,sorted(test_picked)] - type_set[1]) #(50, ) x 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 22, 1000)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "training_data_shape = training_data_X[0].shape\n",
    "print(training_data_shape) #(200, 22, 1000) while test data shape is (50, 22, 1000) and validation data is (38, 22,1000)\n",
    "print(training_data_Y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def remove_nan_rows_A(A, b):\n",
    "    if (np.isnan(A).any() or np.isnan(b).any()):\n",
    "        mask = ~np.isnan(np.sum(A,axis=(1,2))) & ~np.isnan(b[:])\n",
    "        A = A[mask, :, :]\n",
    "        b = b[mask]\n",
    "        \n",
    "    assert A.shape[0] == b.shape[0]\n",
    "    return A, b\n",
    "\n",
    "\n",
    "cnn_trainval_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), trainval_data_X) #(2142, 22, 1000) \n",
    "cnn_training_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), training_data_X) #(1800, 22, 1000)\n",
    "cnn_validation_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), validation_data_X) #(342, 22, 1000)\n",
    "cnn_test_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), test_data_X) #(450, 22, 1000)\n",
    "\n",
    "cnn_trainval_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), trainval_data_Y) #(2142, )\n",
    "cnn_training_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), training_data_Y) #(1800, )\n",
    "cnn_validation_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), validation_data_Y) #(342, )\n",
    "cnn_test_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), test_data_Y) #(450,)\n",
    "\n",
    "cnn_trainval_data_X, cnn_trainval_data_Y = remove_nan_rows_A(cnn_trainval_data_X, cnn_trainval_data_Y) #(1775,22,1000)\n",
    "cnn_training_data_X, cnn_training_data_Y = remove_nan_rows_A(cnn_training_data_X, cnn_training_data_Y) #(1775,22,1000)\n",
    "cnn_validation_data_X, cnn_validation_data_Y = remove_nan_rows_A(cnn_validation_data_X, cnn_validation_data_Y) #(340,22,1000)\n",
    "cnn_test_data_X, cnn_test_data_Y = remove_nan_rows_A(cnn_test_data_X, cnn_test_data_Y) #(443,22,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1774, 22, 1000)\n",
      "(339, 22, 1000)\n",
      "(445, 22, 1000)\n",
      "(1774, 22, 1000)\n",
      "(339, 22, 1000)\n",
      "(445, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(cnn_training_data_X.shape)\n",
    "print(cnn_validation_data_X.shape)\n",
    "print(cnn_test_data_X.shape)\n",
    "\n",
    "cnn_trainval_data_X = np.transpose(cnn_trainval_data_X, (0,2,1))\n",
    "cnn_training_data_X = np.transpose(cnn_training_data_X, (0,2,1))\n",
    "cnn_validation_data_X = np.transpose(cnn_validation_data_X, (0,2,1))\n",
    "cnn_test_data_X = np.transpose(cnn_test_data_X, (0,2,1))\n",
    "\n",
    "mean_list = np.mean(cnn_trainval_data_X.reshape(-1, cnn_trainval_data_X.shape[-1]), axis=0)\n",
    "std_list = np.sqrt((np.var(cnn_trainval_data_X.reshape(-1, cnn_trainval_data_X.shape[-1]), axis=0)))\n",
    "\n",
    "cnn_trainval_data_X = (cnn_trainval_data_X - mean_list)/std_list\n",
    "cnn_training_data_X = (cnn_training_data_X - mean_list)/std_list\n",
    "cnn_validation_data_X = (cnn_validation_data_X - mean_list)/std_list\n",
    "cnn_test_data_X = (cnn_test_data_X - mean_list)/std_list\n",
    "\n",
    "cnn_trainval_data_X = np.transpose(cnn_trainval_data_X, (0,2,1))\n",
    "cnn_training_data_X = np.transpose(cnn_training_data_X, (0,2,1))\n",
    "cnn_validation_data_X = np.transpose(cnn_validation_data_X, (0,2,1))\n",
    "cnn_test_data_X = np.transpose(cnn_test_data_X, (0,2,1))\n",
    "\n",
    "print(cnn_training_data_X.shape)\n",
    "print(cnn_validation_data_X.shape)\n",
    "print(cnn_test_data_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import activations\n",
    "from keras import utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the squashing function.\n",
    "# we use 0.5 in stead of 1 in hinton's paper.\n",
    "# if 1, the norm of vector will be zoomed out.\n",
    "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
    "# and be zoomed out while original norm is greater than 0.5.\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "# define our own softmax function instead of K.softmax\n",
    "# because K.softmax can not specify axis.\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 4\n",
    "epochs = 50\n",
    "(x_train, y_train), (x_test, y_test) = (cnn_training_data_X, cnn_training_data_Y), (cnn_validation_data_X, cnn_validation_data_Y)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1774, 22, 1000)\n",
      "[22, 1000]\n"
     ]
    }
   ],
   "source": [
    "xshape = x_train.shape\n",
    "print(xshape)\n",
    "permutedshape = [xshape[1],xshape[2]]\n",
    "print(permutedshape)\n",
    "permutedshape = tuple(permutedshape+[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1774, 22, 1000, 1)\n",
      "(339, 22, 1000, 1)\n",
      "(1774, 4)\n",
      "(339, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(x_train, tuple(list(x_train.shape)+[1]))\n",
    "x_test = np.reshape(x_test, tuple(list(x_test.shape)+[1]))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_36 (Conv2D)           (None, 22, 1000, 25)      2525      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 22, 1000, 25)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 1, 1000, 25)       13751     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1, 1000, 25)       0         \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 1000, 25)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 333, 25)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 333, 50)           12550     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 333, 50)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 111, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 111, 100)          50100     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 111, 100)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 37, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 37, 200)           200200    \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 37, 200)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 12, 200)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 9604      \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 288,730\n",
      "Trainable params: 288,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A common Conv2D model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(25, 10, padding = 'same', input_shape=(22,1000,1))) #22x1000x25\n",
    "model.add(Activation('linear'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(1, 25, padding = 'same',data_format='channels_first' )) #1x1000x25\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Reshape((1000, 25))) #1000x25\n",
    "model.add(MaxPooling1D(pool_size=3)) # 333x25\n",
    "\n",
    "model.add(Conv1D(50, 10, padding = 'same')) #333x50\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling1D(pool_size=3)) #111x50\n",
    "\n",
    "model.add(Conv1D(100, 10, padding = 'same')) #111x100\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling1D(pool_size=3)) #37x100\n",
    "\n",
    "model.add(Conv1D(200, 10, padding = 'same')) #37x200\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling1D(pool_size=3)) #11x200\n",
    "\n",
    "# #model.add(Dense(1000))\n",
    "# #model.add(Activation('relu'))\n",
    "\n",
    "# #model.add(Dense(100))\n",
    "# #model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.Adam(lr=3e-4)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1774 samples, validate on 339 samples\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36404494382\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(cnn_test_data_X)\n",
    "# round predictions\n",
    "rounded = [np.argmax(x) for x in predictions]\n",
    "print(1.0*np.sum(rounded==cnn_test_data_Y)/len(cnn_test_data_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
