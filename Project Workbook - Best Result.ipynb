{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project_datasets/A01T_slice.mat', 'project_datasets/A02T_slice.mat', 'project_datasets/A03T_slice.mat', 'project_datasets/A04T_slice.mat', 'project_datasets/A05T_slice.mat', 'project_datasets/A06T_slice.mat', 'project_datasets/A07T_slice.mat', 'project_datasets/A08T_slice.mat', 'project_datasets/A09T_slice.mat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "\n",
    "mat_names = glob.glob('project_datasets/*.mat')\n",
    "# each test subject got a different file - 9 test subjects\n",
    "print(mat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'image', u'type']\n",
      "(288, 25, 1000)\n"
     ]
    }
   ],
   "source": [
    "matfile = h5py.File(mat_names[0], 'r')\n",
    "print(matfile.keys()) #image and type\n",
    "image_mat = matfile['image']\n",
    "image_shape = image_mat.shape # 288 (48x6) trials across 25 electrodes for 1000 time points (250Hz*4s)\n",
    "print(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "48\n",
      "288\n",
      "[0.0, 769.0, 770.0, 771.0, 772.0]\n"
     ]
    }
   ],
   "source": [
    "type_mat = matfile['type']\n",
    "type_shape = type_mat.shape\n",
    "print(type_shape)\n",
    "# plt.plot(type_mat[0,:288]) # gets the significant values of types\n",
    "# all the 0's occur after 288, and are meaningless I think\n",
    "# so the image_mat, which has shape (288, 25, 1000) should correspond\n",
    "# to the first 288 entries of type_mat, so\n",
    "# for a single subject, training data should be image_mat, with 288 samples, each sample has shape (25, 1000)\n",
    "# and our target label matrix should be type_mat[:288] (or 287?)\n",
    "nans = np.sum(np.isnan(image_mat[:,:]))\n",
    "print(nans) #No NaN in the data\n",
    "print(len(image_mat[0:,:]))\n",
    "count = 0\n",
    "# for i in range(len(image_mat[0:,:])):\n",
    "#  if np.sum(np.isnan(image_mat[i:,:])):\n",
    "#         pass\n",
    "type_set = list(set(type_mat[0,:]))\n",
    "print(type_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_channels = 22 #from project guidelines\n",
    "test_count = 50 #from project guideline, 238 for train-validation and 50 for test\n",
    "validation_count = 38 # 38 points in validation set and remaining 200 points in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting seed\n",
    "np.random.seed(seed=101)\n",
    "test_picked = np.random.choice(image_shape[0], test_count, replace=False)\n",
    "train_val_picked = np.setdiff1d(np.arange(image_shape[0]), test_picked)\n",
    "val_picked = train_val_picked[:validation_count]\n",
    "train_picked = train_val_picked[validation_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train, val, test sets\n",
    "trainval_data_X = []\n",
    "training_data_X = []\n",
    "validation_data_X = []\n",
    "test_data_X = []\n",
    "\n",
    "trainval_data_Y = []\n",
    "training_data_Y = []\n",
    "validation_data_Y = []\n",
    "test_data_Y = []\n",
    "\n",
    "for i in range(len(mat_names)):\n",
    "    matfile = h5py.File(mat_names[i], 'r')\n",
    "    \n",
    "    trainval_data_X.append(matfile['image'][sorted(train_val_picked),:EEG_channels,:]) #(238, 22, 1000) x 9\n",
    "    training_data_X.append(matfile['image'][sorted(train_picked),:EEG_channels,:]) #(200, 22, 1000) x 9\n",
    "    validation_data_X.append(matfile['image'][sorted(val_picked),:EEG_channels,:]) #(38, 22, 1000) x 9\n",
    "    test_data_X.append(matfile['image'][sorted(test_picked),:EEG_channels,:]) #(50, 22, 1000) x 9\n",
    "    \n",
    "    trainval_data_Y.append(matfile['type'][0,sorted(train_val_picked)] - type_set[1]) #(238, ) x 9\n",
    "    training_data_Y.append(matfile['type'][0,sorted(train_picked)] - type_set[1]) #(200, ) x 9\n",
    "    validation_data_Y.append(matfile['type'][0,sorted(val_picked)] - type_set[1]) #(38, ) x 9\n",
    "    test_data_Y.append(matfile['type'][0,sorted(test_picked)] - type_set[1]) #(50, ) x 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 22, 1000)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "training_data_shape = training_data_X[0].shape\n",
    "print(training_data_shape) #(200, 22, 1000) while test data shape is (50, 22, 1000) and validation data is (38, 22,1000)\n",
    "print(training_data_Y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def remove_nan_rows_A(A, b):\n",
    "    if (np.isnan(A).any() or np.isnan(b).any()):\n",
    "        mask = ~np.isnan(np.sum(A,axis=(1,2))) & ~np.isnan(b[:])\n",
    "        A = A[mask, :, :]\n",
    "        b = b[mask]\n",
    "        \n",
    "    assert A.shape[0] == b.shape[0]\n",
    "    return A, b\n",
    "\n",
    "\n",
    "cnn_trainval_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), trainval_data_X) #(2142, 22, 1000) \n",
    "cnn_training_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), training_data_X) #(1800, 22, 1000)\n",
    "cnn_validation_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), validation_data_X) #(342, 22, 1000)\n",
    "cnn_test_data_X = reduce((lambda x, y: np.concatenate((x, y), axis=0)), test_data_X) #(450, 22, 1000)\n",
    "\n",
    "cnn_trainval_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), trainval_data_Y) #(2142, )\n",
    "cnn_training_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), training_data_Y) #(1800, )\n",
    "cnn_validation_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), validation_data_Y) #(342, )\n",
    "cnn_test_data_Y = reduce((lambda x, y: np.concatenate((x, y), axis=0)), test_data_Y) #(450,)\n",
    "\n",
    "cnn_trainval_data_X, cnn_trainval_data_Y = remove_nan_rows_A(cnn_trainval_data_X, cnn_trainval_data_Y) #(1775,22,1000)\n",
    "cnn_training_data_X, cnn_training_data_Y = remove_nan_rows_A(cnn_training_data_X, cnn_training_data_Y) #(1775,22,1000)\n",
    "cnn_validation_data_X, cnn_validation_data_Y = remove_nan_rows_A(cnn_validation_data_X, cnn_validation_data_Y) #(340,22,1000)\n",
    "cnn_test_data_X, cnn_test_data_Y = remove_nan_rows_A(cnn_test_data_X, cnn_test_data_Y) #(443,22,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1774, 22, 1000)\n",
      "(339, 22, 1000)\n",
      "(445, 22, 1000)\n",
      "(1774, 1000, 22)\n",
      "(339, 1000, 22)\n",
      "(445, 1000, 22)\n"
     ]
    }
   ],
   "source": [
    "print(cnn_training_data_X.shape)\n",
    "print(cnn_validation_data_X.shape)\n",
    "print(cnn_test_data_X.shape)\n",
    "\n",
    "cnn_trainval_data_X = np.transpose(cnn_trainval_data_X, (0,2,1))\n",
    "cnn_training_data_X = np.transpose(cnn_training_data_X, (0,2,1))\n",
    "cnn_validation_data_X = np.transpose(cnn_validation_data_X, (0,2,1))\n",
    "cnn_test_data_X = np.transpose(cnn_test_data_X, (0,2,1))\n",
    "\n",
    "mean_list = np.mean(cnn_trainval_data_X.reshape(-1, cnn_trainval_data_X.shape[-1]), axis=0)\n",
    "std_list = np.sqrt((np.var(cnn_trainval_data_X.reshape(-1, cnn_trainval_data_X.shape[-1]), axis=0)))\n",
    "\n",
    "cnn_trainval_data_X = (cnn_trainval_data_X - mean_list)/std_list\n",
    "cnn_training_data_X = (cnn_training_data_X - mean_list)/std_list\n",
    "cnn_validation_data_X = (cnn_validation_data_X - mean_list)/std_list\n",
    "cnn_test_data_X = (cnn_test_data_X - mean_list)/std_list\n",
    "\n",
    "#cnn_trainval_data_X = np.transpose(cnn_trainval_data_X, (0,2,1))\n",
    "#cnn_training_data_X = np.transpose(cnn_training_data_X, (0,2,1))\n",
    "#cnn_validation_data_X = np.transpose(cnn_validation_data_X, (0,2,1))\n",
    "#cnn_test_data_X = np.transpose(cnn_test_data_X, (0,2,1))\n",
    "\n",
    "print(cnn_training_data_X.shape)\n",
    "print(cnn_validation_data_X.shape)\n",
    "print(cnn_test_data_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import activations\n",
    "from keras import utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the squashing function.\n",
    "# we use 0.5 in stead of 1 in hinton's paper.\n",
    "# if 1, the norm of vector will be zoomed out.\n",
    "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
    "# and be zoomed out while original norm is greater than 0.5.\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "# define our own softmax function instead of K.softmax\n",
    "# because K.softmax can not specify axis.\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200 #try 200\n",
    "num_classes = 4\n",
    "epochs = 100\n",
    "(x_train, y_train), (x_test, y_test) = (cnn_training_data_X, cnn_training_data_Y), (cnn_validation_data_X, cnn_validation_data_Y)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_37 (Conv1D)           (None, 1000, 128)         19840     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1000, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 500, 32)           20512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 250, 8)            776       \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 250, 8)            32        \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 250, 8)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 125, 8)            0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 125, 8)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 4004      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 45,804\n",
      "Trainable params: 45,468\n",
      "Non-trainable params: 336\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A common Conv2D model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 7, padding = 'same',input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv1D(32, 5,padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv1D(8, 3,padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(5000))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dense(1000))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dense(100))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model.add(Dense(4, input_dim=4,\n",
    "#                 kernel_regularizer=regularizers.l2(0.001),\n",
    "#                 activity_regularizer=regularizers.l1(0.001)))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.Adam(lr=3e-3) #try 3e-5\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1774 samples, validate on 339 samples\n",
      "Epoch 1/100\n",
      "1774/1774 [==============================] - 39s 22ms/step - loss: 0.4347 - acc: 0.8382 - val_loss: 0.9838 - val_acc: 0.6313\n",
      "Epoch 2/100\n",
      "1774/1774 [==============================] - 38s 21ms/step - loss: 0.4536 - acc: 0.8286 - val_loss: 1.0037 - val_acc: 0.6372\n",
      "Epoch 3/100\n",
      "1774/1774 [==============================] - 39s 22ms/step - loss: 0.4510 - acc: 0.8264 - val_loss: 1.0431 - val_acc: 0.6254\n",
      "Epoch 4/100\n",
      "1774/1774 [==============================] - 38s 22ms/step - loss: 0.4237 - acc: 0.8388 - val_loss: 1.0171 - val_acc: 0.6283\n",
      "Epoch 5/100\n",
      "1774/1774 [==============================] - 42s 24ms/step - loss: 0.4318 - acc: 0.8320 - val_loss: 1.0667 - val_acc: 0.6283\n",
      "Epoch 6/100\n",
      "1774/1774 [==============================] - 40s 22ms/step - loss: 0.4484 - acc: 0.8264 - val_loss: 1.0556 - val_acc: 0.6431\n",
      "Epoch 7/100\n",
      "1774/1774 [==============================] - 39s 22ms/step - loss: 0.4643 - acc: 0.8202 - val_loss: 1.0806 - val_acc: 0.6283\n",
      "Epoch 8/100\n",
      "1774/1774 [==============================] - 36s 21ms/step - loss: 0.4360 - acc: 0.8275 - val_loss: 1.0602 - val_acc: 0.6047\n",
      "Epoch 9/100\n",
      "1774/1774 [==============================] - 41s 23ms/step - loss: 0.4400 - acc: 0.8224 - val_loss: 1.0634 - val_acc: 0.6254\n",
      "Epoch 10/100\n",
      "1774/1774 [==============================] - 44s 25ms/step - loss: 0.4486 - acc: 0.8258 - val_loss: 1.0291 - val_acc: 0.6401\n",
      "Epoch 11/100\n",
      "1774/1774 [==============================] - 43s 24ms/step - loss: 0.4273 - acc: 0.8337 - val_loss: 1.0771 - val_acc: 0.6224\n",
      "Epoch 12/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.4240 - acc: 0.8315 - val_loss: 1.0519 - val_acc: 0.6195\n",
      "Epoch 13/100\n",
      "1774/1774 [==============================] - 38s 21ms/step - loss: 0.4035 - acc: 0.8529 - val_loss: 1.1229 - val_acc: 0.6136\n",
      "Epoch 14/100\n",
      "1774/1774 [==============================] - 39s 22ms/step - loss: 0.4333 - acc: 0.8388 - val_loss: 1.0675 - val_acc: 0.6254\n",
      "Epoch 15/100\n",
      "1774/1774 [==============================] - 38s 21ms/step - loss: 0.4303 - acc: 0.8303 - val_loss: 1.0721 - val_acc: 0.6224\n",
      "Epoch 16/100\n",
      "1774/1774 [==============================] - 38s 22ms/step - loss: 0.4255 - acc: 0.8326 - val_loss: 1.0775 - val_acc: 0.6195\n",
      "Epoch 17/100\n",
      "1774/1774 [==============================] - 38s 22ms/step - loss: 0.4082 - acc: 0.8365 - val_loss: 1.0180 - val_acc: 0.6372\n",
      "Epoch 18/100\n",
      "1774/1774 [==============================] - 38s 21ms/step - loss: 0.4105 - acc: 0.8461 - val_loss: 1.0820 - val_acc: 0.6460\n",
      "Epoch 19/100\n",
      "1774/1774 [==============================] - 37s 21ms/step - loss: 0.4465 - acc: 0.8224 - val_loss: 1.0438 - val_acc: 0.6431\n",
      "Epoch 20/100\n",
      "1774/1774 [==============================] - 39s 22ms/step - loss: 0.3935 - acc: 0.8512 - val_loss: 1.0762 - val_acc: 0.6254\n",
      "Epoch 21/100\n",
      "1774/1774 [==============================] - 38s 21ms/step - loss: 0.4271 - acc: 0.8371 - val_loss: 1.0577 - val_acc: 0.6106\n",
      "Epoch 22/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.4490 - acc: 0.8354 - val_loss: 1.0867 - val_acc: 0.6077\n",
      "Epoch 23/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.4160 - acc: 0.8410 - val_loss: 1.0245 - val_acc: 0.6431\n",
      "Epoch 24/100\n",
      "1774/1774 [==============================] - 37s 21ms/step - loss: 0.4285 - acc: 0.8360 - val_loss: 1.0724 - val_acc: 0.6195\n",
      "Epoch 25/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.4066 - acc: 0.8393 - val_loss: 1.0034 - val_acc: 0.6490\n",
      "Epoch 26/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3880 - acc: 0.8546 - val_loss: 1.0496 - val_acc: 0.6195\n",
      "Epoch 27/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.4115 - acc: 0.8315 - val_loss: 1.0256 - val_acc: 0.6313\n",
      "Epoch 28/100\n",
      "1774/1774 [==============================] - 37s 21ms/step - loss: 0.4231 - acc: 0.8365 - val_loss: 1.1058 - val_acc: 0.6313\n",
      "Epoch 29/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3835 - acc: 0.8523 - val_loss: 0.9919 - val_acc: 0.6224\n",
      "Epoch 30/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3944 - acc: 0.8534 - val_loss: 1.0188 - val_acc: 0.6283\n",
      "Epoch 31/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.4075 - acc: 0.8467 - val_loss: 1.0164 - val_acc: 0.6401\n",
      "Epoch 32/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3744 - acc: 0.8619 - val_loss: 1.0734 - val_acc: 0.6313\n",
      "Epoch 33/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.4131 - acc: 0.8416 - val_loss: 1.0307 - val_acc: 0.6401\n",
      "Epoch 34/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3891 - acc: 0.8467 - val_loss: 1.0484 - val_acc: 0.6136\n",
      "Epoch 35/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3933 - acc: 0.8568 - val_loss: 1.0621 - val_acc: 0.6165\n",
      "Epoch 36/100\n",
      "1774/1774 [==============================] - 38s 21ms/step - loss: 0.3896 - acc: 0.8472 - val_loss: 1.0584 - val_acc: 0.6460\n",
      "Epoch 37/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3950 - acc: 0.8416 - val_loss: 1.0776 - val_acc: 0.6165\n",
      "Epoch 38/100\n",
      "1774/1774 [==============================] - 37s 21ms/step - loss: 0.3937 - acc: 0.8444 - val_loss: 1.0215 - val_acc: 0.6165\n",
      "Epoch 39/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3953 - acc: 0.8517 - val_loss: 1.1055 - val_acc: 0.6342\n",
      "Epoch 40/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3928 - acc: 0.8472 - val_loss: 1.0744 - val_acc: 0.6313\n",
      "Epoch 41/100\n",
      "1774/1774 [==============================] - 43s 24ms/step - loss: 0.4163 - acc: 0.8388 - val_loss: 1.0512 - val_acc: 0.6401\n",
      "Epoch 42/100\n",
      "1774/1774 [==============================] - 46s 26ms/step - loss: 0.4026 - acc: 0.8450 - val_loss: 1.0604 - val_acc: 0.6224\n",
      "Epoch 43/100\n",
      "1774/1774 [==============================] - 44s 25ms/step - loss: 0.3828 - acc: 0.8636 - val_loss: 1.0382 - val_acc: 0.6195\n",
      "Epoch 44/100\n",
      "1774/1774 [==============================] - 45s 25ms/step - loss: 0.4057 - acc: 0.8467 - val_loss: 1.0953 - val_acc: 0.6372\n",
      "Epoch 45/100\n",
      "1774/1774 [==============================] - 45s 25ms/step - loss: 0.3716 - acc: 0.8591 - val_loss: 1.0602 - val_acc: 0.6372\n",
      "Epoch 46/100\n",
      "1774/1774 [==============================] - 41s 23ms/step - loss: 0.4059 - acc: 0.8506 - val_loss: 1.0250 - val_acc: 0.6401\n",
      "Epoch 47/100\n",
      "1774/1774 [==============================] - 35s 19ms/step - loss: 0.4010 - acc: 0.8422 - val_loss: 1.0519 - val_acc: 0.6195\n",
      "Epoch 48/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3970 - acc: 0.8484 - val_loss: 1.0199 - val_acc: 0.6401\n",
      "Epoch 49/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3777 - acc: 0.8551 - val_loss: 1.0998 - val_acc: 0.6313\n",
      "Epoch 50/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3925 - acc: 0.8444 - val_loss: 1.0410 - val_acc: 0.6431\n",
      "Epoch 51/100\n",
      "1774/1774 [==============================] - 35s 19ms/step - loss: 0.3925 - acc: 0.8512 - val_loss: 1.0114 - val_acc: 0.6460\n",
      "Epoch 52/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3648 - acc: 0.8563 - val_loss: 1.0611 - val_acc: 0.6283\n",
      "Epoch 53/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3722 - acc: 0.8591 - val_loss: 1.0110 - val_acc: 0.6460\n",
      "Epoch 54/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3921 - acc: 0.8636 - val_loss: 1.0544 - val_acc: 0.6283\n",
      "Epoch 55/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3800 - acc: 0.8517 - val_loss: 1.0336 - val_acc: 0.6549\n",
      "Epoch 56/100\n",
      "1774/1774 [==============================] - 39s 22ms/step - loss: 0.3705 - acc: 0.8596 - val_loss: 1.0032 - val_acc: 0.6519\n",
      "Epoch 57/100\n",
      "1774/1774 [==============================] - 46s 26ms/step - loss: 0.4113 - acc: 0.8382 - val_loss: 1.0675 - val_acc: 0.6401\n",
      "Epoch 58/100\n",
      "1774/1774 [==============================] - 45s 25ms/step - loss: 0.3559 - acc: 0.8670 - val_loss: 1.0069 - val_acc: 0.6401\n",
      "Epoch 59/100\n",
      "1774/1774 [==============================] - 45s 25ms/step - loss: 0.3808 - acc: 0.8495 - val_loss: 1.0175 - val_acc: 0.6431\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1774/1774 [==============================] - 46s 26ms/step - loss: 0.3700 - acc: 0.8636 - val_loss: 1.0837 - val_acc: 0.6460\n",
      "Epoch 61/100\n",
      "1774/1774 [==============================] - 44s 25ms/step - loss: 0.3675 - acc: 0.8613 - val_loss: 1.0467 - val_acc: 0.6165\n",
      "Epoch 62/100\n",
      "1774/1774 [==============================] - 46s 26ms/step - loss: 0.3779 - acc: 0.8484 - val_loss: 1.0417 - val_acc: 0.6313\n",
      "Epoch 63/100\n",
      "1774/1774 [==============================] - 41s 23ms/step - loss: 0.3664 - acc: 0.8506 - val_loss: 1.0757 - val_acc: 0.6372\n",
      "Epoch 64/100\n",
      "1774/1774 [==============================] - 42s 24ms/step - loss: 0.3846 - acc: 0.8574 - val_loss: 1.0553 - val_acc: 0.6460\n",
      "Epoch 65/100\n",
      "1774/1774 [==============================] - 41s 23ms/step - loss: 0.3824 - acc: 0.8568 - val_loss: 1.0369 - val_acc: 0.6254\n",
      "Epoch 66/100\n",
      "1774/1774 [==============================] - 42s 24ms/step - loss: 0.3542 - acc: 0.8625 - val_loss: 1.0349 - val_acc: 0.6401\n",
      "Epoch 67/100\n",
      "1774/1774 [==============================] - 42s 24ms/step - loss: 0.3507 - acc: 0.8670 - val_loss: 1.0859 - val_acc: 0.6313\n",
      "Epoch 68/100\n",
      "1774/1774 [==============================] - 45s 25ms/step - loss: 0.3500 - acc: 0.8653 - val_loss: 1.1116 - val_acc: 0.6401\n",
      "Epoch 69/100\n",
      "1774/1774 [==============================] - 43s 24ms/step - loss: 0.3775 - acc: 0.8602 - val_loss: 1.0377 - val_acc: 0.6431\n",
      "Epoch 70/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3798 - acc: 0.8501 - val_loss: 1.0569 - val_acc: 0.6313\n",
      "Epoch 71/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3830 - acc: 0.8472 - val_loss: 1.1259 - val_acc: 0.6254\n",
      "Epoch 72/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3523 - acc: 0.8596 - val_loss: 1.0797 - val_acc: 0.6254\n",
      "Epoch 73/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3947 - acc: 0.8523 - val_loss: 1.1223 - val_acc: 0.6372\n",
      "Epoch 74/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3548 - acc: 0.8630 - val_loss: 1.2166 - val_acc: 0.6018\n",
      "Epoch 75/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.4056 - acc: 0.8478 - val_loss: 1.0459 - val_acc: 0.6519\n",
      "Epoch 76/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3630 - acc: 0.8568 - val_loss: 1.0358 - val_acc: 0.6401\n",
      "Epoch 77/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3876 - acc: 0.8501 - val_loss: 1.0704 - val_acc: 0.6401\n",
      "Epoch 78/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3663 - acc: 0.8591 - val_loss: 1.0916 - val_acc: 0.6490\n",
      "Epoch 79/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3759 - acc: 0.8574 - val_loss: 1.0462 - val_acc: 0.6372\n",
      "Epoch 80/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3835 - acc: 0.8602 - val_loss: 1.0354 - val_acc: 0.6460\n",
      "Epoch 81/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3923 - acc: 0.8478 - val_loss: 1.0459 - val_acc: 0.6519\n",
      "Epoch 82/100\n",
      "1774/1774 [==============================] - 36s 20ms/step - loss: 0.3513 - acc: 0.8715 - val_loss: 1.0229 - val_acc: 0.6490\n",
      "Epoch 83/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3828 - acc: 0.8647 - val_loss: 1.0082 - val_acc: 0.6608\n",
      "Epoch 84/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3572 - acc: 0.8675 - val_loss: 1.1375 - val_acc: 0.6372\n",
      "Epoch 85/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3665 - acc: 0.8619 - val_loss: 1.0556 - val_acc: 0.6372\n",
      "Epoch 86/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3758 - acc: 0.8523 - val_loss: 1.0730 - val_acc: 0.6490\n",
      "Epoch 87/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3478 - acc: 0.8715 - val_loss: 1.0311 - val_acc: 0.6372\n",
      "Epoch 88/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3554 - acc: 0.8653 - val_loss: 1.0571 - val_acc: 0.6313\n",
      "Epoch 89/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3921 - acc: 0.8540 - val_loss: 1.1454 - val_acc: 0.6342\n",
      "Epoch 90/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3668 - acc: 0.8596 - val_loss: 1.0537 - val_acc: 0.6460\n",
      "Epoch 91/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3731 - acc: 0.8517 - val_loss: 1.1190 - val_acc: 0.6077\n",
      "Epoch 92/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3781 - acc: 0.8551 - val_loss: 1.0663 - val_acc: 0.6313\n",
      "Epoch 93/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3682 - acc: 0.8591 - val_loss: 1.0152 - val_acc: 0.6608\n",
      "Epoch 94/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3438 - acc: 0.8625 - val_loss: 1.0757 - val_acc: 0.6401\n",
      "Epoch 95/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3550 - acc: 0.8608 - val_loss: 1.1019 - val_acc: 0.6549\n",
      "Epoch 96/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3692 - acc: 0.8568 - val_loss: 1.0374 - val_acc: 0.6431\n",
      "Epoch 97/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3550 - acc: 0.8715 - val_loss: 1.0272 - val_acc: 0.6460\n",
      "Epoch 98/100\n",
      "1774/1774 [==============================] - 35s 20ms/step - loss: 0.3548 - acc: 0.8585 - val_loss: 1.0345 - val_acc: 0.6372\n",
      "Epoch 99/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3595 - acc: 0.8596 - val_loss: 1.1261 - val_acc: 0.6136\n",
      "Epoch 100/100\n",
      "1774/1774 [==============================] - 34s 19ms/step - loss: 0.3814 - acc: 0.8472 - val_loss: 1.1266 - val_acc: 0.6313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15b20efd0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694382022472\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(cnn_test_data_X)\n",
    "# round predictions\n",
    "rounded = [np.argmax(x) for x in predictions]\n",
    "print(1.0*np.sum(rounded==cnn_test_data_Y)/len(cnn_test_data_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
